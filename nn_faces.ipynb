{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_faces.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV-JhzUIrLC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import Adamax, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7jcbwUunPlu",
        "colab_type": "text"
      },
      "source": [
        "## functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Qp5eF7nOj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting result\n",
        "def result(hist):\n",
        "    plt.plot(hist.history['acc'], label='correct predictions @train')\n",
        "    plt.plot(hist.history['val_acc'], label='correct predictions @test')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('correct predictions share')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        f\"accuracy: {hist.history['acc'][-1]},validation accuracy: {hist.history['val_acc'][-1]}\"\n",
        "    )\n",
        "\n",
        "# checkpointer function\n",
        "def check(model_num):\n",
        "    filepath = os.path.join(\n",
        "        'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/'+model_num+'_weights-improvement-{epoch:02d}-{val_acc:.5f}-{acc:.5f}.hdf5'\n",
        "    )\n",
        "\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath,\n",
        "        monitor='val_acc',\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    )\n",
        "    callback_list = [checkpoint]\n",
        "\n",
        "    return callback_list\n",
        "\n",
        "# plotting image\n",
        "def my_img(array):\n",
        "    plt.imshow(Image.fromarray(array).convert('RGBA'))\n",
        "    plt.show()\n",
        "\n",
        "def oliv_traced(df_x):\n",
        "    olivetti_labels = []\n",
        "    for i in range(int(x_oliv.shape[0])):\n",
        "        print(i)\n",
        "        img(df_x[i].reshape(64,64)*100)\n",
        "        olivetti_labels.append(float(input('happy or not: ')))\n",
        "        olivetti_labels = np.array(olivetti_labels)\n",
        "\n",
        "    return olivetti_labels\n",
        "\n",
        "# predicting for test dataset \n",
        "def pred(model, x_test):\n",
        "    y_pred = model.predict(x_test)\n",
        "    pred = []\n",
        "    for i in range(y_pred.shape[0]):\n",
        "        pred.append(np.argmax(y_pred[i]))\n",
        "    pred = np.array(pred)\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndIWCQAMtByv",
        "colab_type": "code",
        "outputId": "45c2e1d3-1c67-40b8-ac4a-ee16bd2952ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8K5ufJ5tF1g",
        "colab_type": "code",
        "outputId": "62d33f50-0a21-4c48-f631-861cd6be51d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!ls 'drive/My Drive/Colab Notebooks/neural_network/nn_diploma'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 00_weights-improvement-01-0.45000-0.51250.hdf5\n",
            " 01_weights-improvement-01-0.45000-0.51250.hdf5\n",
            " img\n",
            " nn_faces.ipynb\n",
            " olivetti_X.csv\n",
            " olivetti_y.csv\n",
            " savings\n",
            " test_happy.h5\n",
            " train_happy.h5\n",
            "'Задание для курсовой работы.docx'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ZWMUOPY4L-",
        "colab_type": "text"
      },
      "source": [
        "## data upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioWrdvECY7G7",
        "colab_type": "text"
      },
      "source": [
        "##### olivetti_x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu3AVJOdtKdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_oliv = np.loadtxt(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/olivetti_X.csv', \n",
        "    delimiter=','\n",
        ") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPOvYck60ibk",
        "colab_type": "code",
        "outputId": "3d742ff0-b33f-4600-cb24-350a5a7be78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_oliv.ndim)\n",
        "print(x_oliv.shape)\n",
        "x_oliv[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "(400, 4096)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.30991736, 0.36776859, 0.41735536, ..., 0.15289256, 0.16115703,\n",
              "       0.1570248 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmU3LIElZ-1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_oliv = x_oliv.reshape(400, 64, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItWQqNBEcWta",
        "colab_type": "text"
      },
      "source": [
        "##### olivetti_y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaZsH8O4hgX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_oliv = np.loadtxt(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/olivetti_y.csv', \n",
        "    delimiter=','\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo95DkzChjEu",
        "colab_type": "code",
        "outputId": "8dc87093-e358-45be-a030-30b7e644580e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(y_oliv.ndim)\n",
        "print(y_oliv.shape)\n",
        "print(set(y_oliv))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "(400,)\n",
            "{0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snk-WqzOLf6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# manual target labelling\n",
        "\n",
        "#oliv_traced(x_oliv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbzM4dWLNSN1",
        "colab_type": "code",
        "outputId": "b9dcc021-18a2-4d27-f2a8-d65e333dbf41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "olivetti_labels = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
        "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
        "       0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
        "       1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
        "       1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
        "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
        "       1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
        "       0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
        "       1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
        "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
        "       1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
        "       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
        "       0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
        "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
        "       1., 1., 1., 0., 1., 1., 1., 0., 1.])\n",
        "\n",
        "olivetti_labels"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
              "       0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
              "       1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "       1., 1., 1., 0., 1., 1., 1., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx8HX4AZk4rV",
        "colab_type": "text"
      },
      "source": [
        "##### train_happy.h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBuNRA9Bk5G8",
        "colab_type": "code",
        "outputId": "4c21fee2-723f-4871-ec35-73b6489d94a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f_train = h5py.File('drive/My Drive/Colab Notebooks/neural_network/nn_diploma/train_happy.h5', 'r')\n",
        "list(f_train.keys())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list_classes', 'train_set_x', 'train_set_y']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SamYi852lAKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_happy = f_train['train_set_x']\n",
        "y_train_happy = f_train['train_set_y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k09neKpGlGfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_happy = np.array(x_train_happy)\n",
        "y_train_happy = np.array(y_train_happy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxzodhhvmEki",
        "colab_type": "code",
        "outputId": "4b09fcd2-70eb-47ea-fe5d-43337f2a0895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x_train_happy.shape)\n",
        "print(y_train_happy.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600, 64, 64, 3)\n",
            "(600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyZZ3IWls06t",
        "colab_type": "code",
        "outputId": "3419eae9-78b3-4514-ab99-3f2cb73b912f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_happy[0].shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wBZ52fKE7ic",
        "colab_type": "code",
        "outputId": "5890e8d0-f7ea-4a98-ad10-6b47fadd29f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# unique values in y_train\n",
        "set(y_train_happy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "791sMADznfv1",
        "colab_type": "code",
        "outputId": "e1d4b0a8-1942-4f4c-accb-15e976354ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "my_img(x_train_happy[255])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4jsccwif+A7HHMInvsMxh/j/3fIX8YzBDW0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rULjbzYPtHye",
        "colab_type": "code",
        "outputId": "0655bbdb-52e2-4fe3-b1e7-8e7ec17706dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_happy[10]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIdNDA6ucgLX",
        "colab_type": "text"
      },
      "source": [
        "##### test_happy.h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncebEm6gRqPY",
        "colab_type": "code",
        "outputId": "1c670b1f-1bfd-4da8-b17d-712236410789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f_test = h5py.File('drive/My Drive/Colab Notebooks/neural_network/nn_diploma/test_happy.h5', 'r')\n",
        "list(f_test.keys())\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['list_classes', 'test_set_x', 'test_set_y']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMrpeTfGjz_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_happy = f_test['test_set_x']\n",
        "y_test_happy = f_test['test_set_y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R29mrBoakBva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_happy = np.array(x_test_happy)\n",
        "y_test_happy = np.array(y_test_happy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TaOGVGVkgyF",
        "colab_type": "code",
        "outputId": "afc428cd-2dcf-472a-f7bc-6bc0c8a939ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x_test_happy.shape)\n",
        "print(y_test_happy.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 64, 64, 3)\n",
            "(150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQWxkT7_N0sv",
        "colab_type": "text"
      },
      "source": [
        "## preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFKKA2T2vguL",
        "colab_type": "text"
      },
      "source": [
        "##### labeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtG1hbeMwfj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting targets to categorical values / one-hot encoding\n",
        "y_train_happy_cat = to_categorical(y_train_happy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk4pfyQ__pB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting uint8 to floats + some standardization\n",
        "x_train_happy_stnd = x_train_happy / 256.\n",
        "x_test_happy_stnd = x_test_happy / 256."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pLaoBUAmusp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scaling features \n",
        "v_min = x_train_happy.min(axis=(0, 1), keepdims=True)\n",
        "v_max = x_train_happy.max(axis=(0, 1), keepdims=True)\n",
        "x_train_happy_scld = (x_train_happy - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = x_test_happy.min(axis=(0, 1), keepdims=True)\n",
        "v_max = x_test_happy.max(axis=(0, 1), keepdims=True)\n",
        "x_test_happy_scld = (x_test_happy - v_min)/(v_max - v_min)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnS8q7L5vmfh",
        "colab_type": "text"
      },
      "source": [
        "##### unlabeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-AaEEWgPlL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_oliv = x_oliv.reshape((x_oliv.shape[0], 64, 64, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PsaWlSPtz1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x_oliv_train, x_oliv_test, olivetti_labels_cat_train, olivetti_labels_cat_test = train_test_split(x_oliv, olivetti_labels_cat, test_size=0.2)  \n",
        "x_oliv_train, x_oliv_test, olivetti_labels_train, olivetti_labels_test = train_test_split(x_oliv, olivetti_labels, test_size=0.2, random_state=42)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEgQz1KjC1pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting targets to categorical values / one-hot encoding\n",
        "olivetti_labels_train_cat = to_categorical(olivetti_labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2ceP-BSDVOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scaling features \n",
        "v_min = x_oliv_train.min(axis=(0, 1), keepdims=True)\n",
        "v_max = x_oliv_train.max(axis=(0, 1), keepdims=True)\n",
        "x_oliv_train_scld = (x_oliv_train - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = x_oliv_test.min(axis=(0, 1), keepdims=True)\n",
        "v_max = x_oliv_test.max(axis=(0, 1), keepdims=True)\n",
        "x_oliv_test_scld = (x_oliv_test - v_min)/(v_max - v_min)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5imqRvSkj3qV",
        "colab_type": "text"
      },
      "source": [
        "## build, fit & predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62GaqWKxAQdS",
        "colab_type": "text"
      },
      "source": [
        "### standard convolutional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aehSgYmGJxu",
        "colab_type": "text"
      },
      "source": [
        "##### labeled dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Ljj7B4j9h5",
        "colab_type": "code",
        "outputId": "608b24a3-0596-4edb-8e7e-e855d23f073c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 00 conv2D based on \n",
        "# categorical values and metrics \n",
        "# unscaled x train\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 3)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_happy,\n",
        "    y_train_happy_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=100,\n",
        "    callbacks=check('00'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 480 samples, validate on 120 samples\n",
            "Epoch 1/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 2.5817 - acc: 0.5500\n",
            "Epoch 00001: val_acc improved from -inf to 0.55000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-01-0.55000-0.53333.hdf5\n",
            "480/480 [==============================] - 6s 13ms/sample - loss: 2.4133 - acc: 0.5333 - val_loss: 1.4567 - val_acc: 0.5500\n",
            "Epoch 2/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.7603 - acc: 0.4775\n",
            "Epoch 00002: val_acc did not improve from 0.55000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.7470 - acc: 0.4917 - val_loss: 0.6913 - val_acc: 0.4333\n",
            "Epoch 3/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6824 - acc: 0.5375\n",
            "Epoch 00003: val_acc did not improve from 0.55000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6837 - acc: 0.5417 - val_loss: 0.6831 - val_acc: 0.5500\n",
            "Epoch 4/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6770 - acc: 0.6325\n",
            "Epoch 00004: val_acc did not improve from 0.55000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6788 - acc: 0.6208 - val_loss: 0.6899 - val_acc: 0.5500\n",
            "Epoch 5/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6739 - acc: 0.5775\n",
            "Epoch 00005: val_acc improved from 0.55000 to 0.58333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-05-0.58333-0.59167.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6699 - acc: 0.5917 - val_loss: 0.6906 - val_acc: 0.5833\n",
            "Epoch 6/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6410 - acc: 0.6650\n",
            "Epoch 00006: val_acc improved from 0.58333 to 0.61667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-06-0.61667-0.65833.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6386 - acc: 0.6583 - val_loss: 0.6949 - val_acc: 0.6167\n",
            "Epoch 7/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6028 - acc: 0.6600\n",
            "Epoch 00007: val_acc did not improve from 0.61667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5981 - acc: 0.6604 - val_loss: 0.8151 - val_acc: 0.5833\n",
            "Epoch 8/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5444 - acc: 0.7150\n",
            "Epoch 00008: val_acc improved from 0.61667 to 0.68333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-08-0.68333-0.71042.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5419 - acc: 0.7104 - val_loss: 1.0795 - val_acc: 0.6833\n",
            "Epoch 9/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4727 - acc: 0.7650\n",
            "Epoch 00009: val_acc improved from 0.68333 to 0.69167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-09-0.69167-0.74583.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4819 - acc: 0.7458 - val_loss: 1.3317 - val_acc: 0.6917\n",
            "Epoch 10/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4211 - acc: 0.8125\n",
            "Epoch 00010: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4211 - acc: 0.8042 - val_loss: 0.9730 - val_acc: 0.6500\n",
            "Epoch 11/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3800 - acc: 0.8300\n",
            "Epoch 00011: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4039 - acc: 0.8104 - val_loss: 1.2622 - val_acc: 0.5667\n",
            "Epoch 12/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4053 - acc: 0.7925\n",
            "Epoch 00012: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4038 - acc: 0.7937 - val_loss: 1.2809 - val_acc: 0.4917\n",
            "Epoch 13/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3410 - acc: 0.8450\n",
            "Epoch 00013: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3337 - acc: 0.8458 - val_loss: 2.0648 - val_acc: 0.5167\n",
            "Epoch 14/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3172 - acc: 0.8550\n",
            "Epoch 00014: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3220 - acc: 0.8542 - val_loss: 1.4696 - val_acc: 0.5333\n",
            "Epoch 15/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3188 - acc: 0.8575\n",
            "Epoch 00015: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3076 - acc: 0.8604 - val_loss: 1.4507 - val_acc: 0.5083\n",
            "Epoch 16/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2535 - acc: 0.9000\n",
            "Epoch 00016: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2516 - acc: 0.8979 - val_loss: 2.4670 - val_acc: 0.5000\n",
            "Epoch 17/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2080 - acc: 0.9200\n",
            "Epoch 00017: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2343 - acc: 0.9104 - val_loss: 1.5800 - val_acc: 0.5417\n",
            "Epoch 18/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2401 - acc: 0.9000\n",
            "Epoch 00018: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2302 - acc: 0.9104 - val_loss: 1.7431 - val_acc: 0.5083\n",
            "Epoch 19/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2299 - acc: 0.9075\n",
            "Epoch 00019: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2456 - acc: 0.9042 - val_loss: 1.9321 - val_acc: 0.5000\n",
            "Epoch 20/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2060 - acc: 0.9300\n",
            "Epoch 00020: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2215 - acc: 0.9167 - val_loss: 1.6514 - val_acc: 0.5417\n",
            "Epoch 21/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1847 - acc: 0.9225\n",
            "Epoch 00021: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1920 - acc: 0.9146 - val_loss: 1.5800 - val_acc: 0.6250\n",
            "Epoch 22/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1749 - acc: 0.9325\n",
            "Epoch 00022: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1933 - acc: 0.9250 - val_loss: 1.0917 - val_acc: 0.6583\n",
            "Epoch 23/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1768 - acc: 0.9425\n",
            "Epoch 00023: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1809 - acc: 0.9396 - val_loss: 0.8563 - val_acc: 0.6500\n",
            "Epoch 24/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1838 - acc: 0.9175\n",
            "Epoch 00024: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1935 - acc: 0.9125 - val_loss: 1.3619 - val_acc: 0.5167\n",
            "Epoch 25/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1818 - acc: 0.9225\n",
            "Epoch 00025: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1847 - acc: 0.9208 - val_loss: 0.5569 - val_acc: 0.6500\n",
            "Epoch 26/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1469 - acc: 0.9350\n",
            "Epoch 00026: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1439 - acc: 0.9396 - val_loss: 0.6882 - val_acc: 0.6750\n",
            "Epoch 27/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1233 - acc: 0.9475\n",
            "Epoch 00027: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1385 - acc: 0.9458 - val_loss: 1.2276 - val_acc: 0.6333\n",
            "Epoch 28/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1377 - acc: 0.9475\n",
            "Epoch 00028: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1291 - acc: 0.9500 - val_loss: 0.8911 - val_acc: 0.6917\n",
            "Epoch 29/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1531 - acc: 0.9375\n",
            "Epoch 00029: val_acc did not improve from 0.69167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1539 - acc: 0.9375 - val_loss: 0.7147 - val_acc: 0.6667\n",
            "Epoch 30/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1126 - acc: 0.9600\n",
            "Epoch 00030: val_acc improved from 0.69167 to 0.77500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-30-0.77500-0.96042.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1046 - acc: 0.9604 - val_loss: 0.5095 - val_acc: 0.7750\n",
            "Epoch 31/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1226 - acc: 0.9525\n",
            "Epoch 00031: val_acc improved from 0.77500 to 0.79167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-31-0.79167-0.95208.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1187 - acc: 0.9521 - val_loss: 0.6322 - val_acc: 0.7917\n",
            "Epoch 32/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1268 - acc: 0.9475\n",
            "Epoch 00032: val_acc improved from 0.79167 to 0.84167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-32-0.84167-0.95208.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1339 - acc: 0.9521 - val_loss: 0.4066 - val_acc: 0.8417\n",
            "Epoch 33/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0976 - acc: 0.9675\n",
            "Epoch 00033: val_acc did not improve from 0.84167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0987 - acc: 0.9646 - val_loss: 0.4629 - val_acc: 0.7750\n",
            "Epoch 34/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1186 - acc: 0.9575\n",
            "Epoch 00034: val_acc did not improve from 0.84167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1228 - acc: 0.9542 - val_loss: 0.4610 - val_acc: 0.7583\n",
            "Epoch 35/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1126 - acc: 0.9550\n",
            "Epoch 00035: val_acc improved from 0.84167 to 0.86667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-35-0.86667-0.96042.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1005 - acc: 0.9604 - val_loss: 0.2995 - val_acc: 0.8667\n",
            "Epoch 36/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0953 - acc: 0.9675\n",
            "Epoch 00036: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1018 - acc: 0.9646 - val_loss: 0.3871 - val_acc: 0.8083\n",
            "Epoch 37/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1206 - acc: 0.9525\n",
            "Epoch 00037: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1164 - acc: 0.9542 - val_loss: 0.4818 - val_acc: 0.7833\n",
            "Epoch 38/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0881 - acc: 0.9625\n",
            "Epoch 00038: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1079 - acc: 0.9521 - val_loss: 0.4375 - val_acc: 0.7750\n",
            "Epoch 39/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0811 - acc: 0.9775\n",
            "Epoch 00039: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0942 - acc: 0.9708 - val_loss: 0.3139 - val_acc: 0.8583\n",
            "Epoch 40/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0858 - acc: 0.9700\n",
            "Epoch 00040: val_acc improved from 0.86667 to 0.92500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-40-0.92500-0.97083.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0794 - acc: 0.9708 - val_loss: 0.1878 - val_acc: 0.9250\n",
            "Epoch 41/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0981 - acc: 0.9700\n",
            "Epoch 00041: val_acc improved from 0.92500 to 0.94167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-41-0.94167-0.97083.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0948 - acc: 0.9708 - val_loss: 0.1521 - val_acc: 0.9417\n",
            "Epoch 42/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0666 - acc: 0.9700\n",
            "Epoch 00042: val_acc improved from 0.94167 to 0.97500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-42-0.97500-0.97292.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0632 - acc: 0.9729 - val_loss: 0.1217 - val_acc: 0.9750\n",
            "Epoch 43/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0720 - acc: 0.9750\n",
            "Epoch 00043: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0795 - acc: 0.9771 - val_loss: 0.0996 - val_acc: 0.9750\n",
            "Epoch 44/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0913 - acc: 0.9650\n",
            "Epoch 00044: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0857 - acc: 0.9667 - val_loss: 0.1330 - val_acc: 0.9583\n",
            "Epoch 45/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0739 - acc: 0.9800\n",
            "Epoch 00045: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0688 - acc: 0.9812 - val_loss: 0.1401 - val_acc: 0.9500\n",
            "Epoch 46/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0823 - acc: 0.9775\n",
            "Epoch 00046: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0750 - acc: 0.9812 - val_loss: 0.1544 - val_acc: 0.9583\n",
            "Epoch 47/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1007 - acc: 0.9600\n",
            "Epoch 00047: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1022 - acc: 0.9604 - val_loss: 0.1629 - val_acc: 0.9500\n",
            "Epoch 48/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0669 - acc: 0.9775\n",
            "Epoch 00048: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0797 - acc: 0.9729 - val_loss: 0.1201 - val_acc: 0.9500\n",
            "Epoch 49/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0758 - acc: 0.9725\n",
            "Epoch 00049: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0731 - acc: 0.9750 - val_loss: 0.1866 - val_acc: 0.9250\n",
            "Epoch 50/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0654 - acc: 0.9850\n",
            "Epoch 00050: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0594 - acc: 0.9854 - val_loss: 0.1425 - val_acc: 0.9417\n",
            "Epoch 51/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0810 - acc: 0.9800\n",
            "Epoch 00051: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0725 - acc: 0.9812 - val_loss: 0.1105 - val_acc: 0.9667\n",
            "Epoch 52/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0716 - acc: 0.9725\n",
            "Epoch 00052: val_acc improved from 0.97500 to 0.98333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/00_weights-improvement-52-0.98333-0.97292.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0807 - acc: 0.9729 - val_loss: 0.0730 - val_acc: 0.9833\n",
            "Epoch 53/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0591 - acc: 0.9750\n",
            "Epoch 00053: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0642 - acc: 0.9750 - val_loss: 0.0840 - val_acc: 0.9833\n",
            "Epoch 54/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0636 - acc: 0.9750\n",
            "Epoch 00054: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0634 - acc: 0.9729 - val_loss: 0.1344 - val_acc: 0.9583\n",
            "Epoch 55/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0397 - acc: 0.9875\n",
            "Epoch 00055: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0350 - acc: 0.9896 - val_loss: 0.2325 - val_acc: 0.9083\n",
            "Epoch 56/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0393 - acc: 0.9850\n",
            "Epoch 00056: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0543 - acc: 0.9771 - val_loss: 0.1959 - val_acc: 0.9500\n",
            "Epoch 57/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0507 - acc: 0.9775\n",
            "Epoch 00057: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0468 - acc: 0.9792 - val_loss: 0.0855 - val_acc: 0.9750\n",
            "Epoch 58/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0623 - acc: 0.9825\n",
            "Epoch 00058: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0635 - acc: 0.9833 - val_loss: 0.1393 - val_acc: 0.9583\n",
            "Epoch 59/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0425 - acc: 0.9875\n",
            "Epoch 00059: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0516 - acc: 0.9833 - val_loss: 0.0962 - val_acc: 0.9750\n",
            "Epoch 60/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0697 - acc: 0.9725\n",
            "Epoch 00060: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0619 - acc: 0.9750 - val_loss: 0.1658 - val_acc: 0.9500\n",
            "Epoch 61/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0463 - acc: 0.9825\n",
            "Epoch 00061: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0503 - acc: 0.9833 - val_loss: 0.1711 - val_acc: 0.9500\n",
            "Epoch 62/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0587 - acc: 0.9775\n",
            "Epoch 00062: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0598 - acc: 0.9792 - val_loss: 0.1766 - val_acc: 0.9500\n",
            "Epoch 63/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0524 - acc: 0.9775\n",
            "Epoch 00063: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0459 - acc: 0.9812 - val_loss: 0.0990 - val_acc: 0.9750\n",
            "Epoch 64/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0638 - acc: 0.9725\n",
            "Epoch 00064: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0577 - acc: 0.9771 - val_loss: 0.1321 - val_acc: 0.9250\n",
            "Epoch 65/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0509 - acc: 0.9800\n",
            "Epoch 00065: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0514 - acc: 0.9792 - val_loss: 0.1142 - val_acc: 0.9750\n",
            "Epoch 66/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0668 - acc: 0.9725\n",
            "Epoch 00066: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0712 - acc: 0.9729 - val_loss: 0.0700 - val_acc: 0.9750\n",
            "Epoch 67/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0509 - acc: 0.9825\n",
            "Epoch 00067: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0583 - acc: 0.9812 - val_loss: 0.0649 - val_acc: 0.9833\n",
            "Epoch 68/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0428 - acc: 0.9775\n",
            "Epoch 00068: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0492 - acc: 0.9792 - val_loss: 0.1136 - val_acc: 0.9833\n",
            "Epoch 69/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0399 - acc: 0.9825\n",
            "Epoch 00069: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0476 - acc: 0.9792 - val_loss: 0.1282 - val_acc: 0.9750\n",
            "Epoch 70/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0926 - acc: 0.9625\n",
            "Epoch 00070: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0892 - acc: 0.9646 - val_loss: 0.0772 - val_acc: 0.9583\n",
            "Epoch 71/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1112 - acc: 0.9650\n",
            "Epoch 00071: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1209 - acc: 0.9604 - val_loss: 0.1410 - val_acc: 0.9583\n",
            "Epoch 72/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0830 - acc: 0.9700\n",
            "Epoch 00072: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0740 - acc: 0.9729 - val_loss: 0.1264 - val_acc: 0.9667\n",
            "Epoch 73/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0541 - acc: 0.9850\n",
            "Epoch 00073: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0492 - acc: 0.9875 - val_loss: 0.1930 - val_acc: 0.9333\n",
            "Epoch 74/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0873 - acc: 0.9675\n",
            "Epoch 00074: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0861 - acc: 0.9688 - val_loss: 0.0842 - val_acc: 0.9750\n",
            "Epoch 75/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0505 - acc: 0.9900\n",
            "Epoch 00075: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0494 - acc: 0.9875 - val_loss: 0.0974 - val_acc: 0.9750\n",
            "Epoch 76/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0371 - acc: 0.9850\n",
            "Epoch 00076: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0436 - acc: 0.9833 - val_loss: 0.1010 - val_acc: 0.9750\n",
            "Epoch 77/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0688 - acc: 0.9725\n",
            "Epoch 00077: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0679 - acc: 0.9708 - val_loss: 0.0863 - val_acc: 0.9667\n",
            "Epoch 78/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0376 - acc: 0.9850\n",
            "Epoch 00078: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0421 - acc: 0.9854 - val_loss: 0.0785 - val_acc: 0.9833\n",
            "Epoch 79/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0537 - acc: 0.9825\n",
            "Epoch 00079: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0485 - acc: 0.9833 - val_loss: 0.0742 - val_acc: 0.9750\n",
            "Epoch 80/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0420 - acc: 0.9825\n",
            "Epoch 00080: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0656 - acc: 0.9771 - val_loss: 0.1011 - val_acc: 0.9667\n",
            "Epoch 81/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0337 - acc: 0.9850\n",
            "Epoch 00081: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0449 - acc: 0.9812 - val_loss: 0.0639 - val_acc: 0.9667\n",
            "Epoch 82/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0756 - acc: 0.9825\n",
            "Epoch 00082: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0736 - acc: 0.9792 - val_loss: 0.0805 - val_acc: 0.9750\n",
            "Epoch 83/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0512 - acc: 0.9775\n",
            "Epoch 00083: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0503 - acc: 0.9771 - val_loss: 0.0895 - val_acc: 0.9833\n",
            "Epoch 84/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0245 - acc: 0.9900\n",
            "Epoch 00084: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0381 - acc: 0.9875 - val_loss: 0.0736 - val_acc: 0.9750\n",
            "Epoch 85/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0666 - acc: 0.9800\n",
            "Epoch 00085: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0602 - acc: 0.9812 - val_loss: 0.0815 - val_acc: 0.9750\n",
            "Epoch 86/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0501 - acc: 0.9825\n",
            "Epoch 00086: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0537 - acc: 0.9812 - val_loss: 0.0887 - val_acc: 0.9833\n",
            "Epoch 87/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0417 - acc: 0.9900\n",
            "Epoch 00087: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0423 - acc: 0.9896 - val_loss: 0.0683 - val_acc: 0.9750\n",
            "Epoch 88/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0426 - acc: 0.9800\n",
            "Epoch 00088: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0448 - acc: 0.9771 - val_loss: 0.0819 - val_acc: 0.9750\n",
            "Epoch 89/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0301 - acc: 0.9900\n",
            "Epoch 00089: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0459 - acc: 0.9812 - val_loss: 0.0780 - val_acc: 0.9750\n",
            "Epoch 90/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0271 - acc: 0.9950\n",
            "Epoch 00090: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0361 - acc: 0.9917 - val_loss: 0.1030 - val_acc: 0.9833\n",
            "Epoch 91/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0395 - acc: 0.9825\n",
            "Epoch 00091: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0364 - acc: 0.9854 - val_loss: 0.1041 - val_acc: 0.9750\n",
            "Epoch 92/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0803 - acc: 0.9800\n",
            "Epoch 00092: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0724 - acc: 0.9792 - val_loss: 0.0739 - val_acc: 0.9833\n",
            "Epoch 93/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0391 - acc: 0.9925\n",
            "Epoch 00093: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0441 - acc: 0.9875 - val_loss: 0.1064 - val_acc: 0.9750\n",
            "Epoch 94/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0529 - acc: 0.9750\n",
            "Epoch 00094: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0475 - acc: 0.9792 - val_loss: 0.1070 - val_acc: 0.9750\n",
            "Epoch 95/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0546 - acc: 0.9850\n",
            "Epoch 00095: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0537 - acc: 0.9833 - val_loss: 0.1253 - val_acc: 0.9667\n",
            "Epoch 96/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1011 - acc: 0.9625\n",
            "Epoch 00096: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0902 - acc: 0.9667 - val_loss: 0.1096 - val_acc: 0.9500\n",
            "Epoch 97/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0581 - acc: 0.9750\n",
            "Epoch 00097: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0553 - acc: 0.9771 - val_loss: 0.0830 - val_acc: 0.9833\n",
            "Epoch 98/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0563 - acc: 0.9875\n",
            "Epoch 00098: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0602 - acc: 0.9854 - val_loss: 0.0681 - val_acc: 0.9833\n",
            "Epoch 99/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0570 - acc: 0.9750\n",
            "Epoch 00099: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0512 - acc: 0.9792 - val_loss: 0.0725 - val_acc: 0.9833\n",
            "Epoch 100/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0713 - acc: 0.9700\n",
            "Epoch 00100: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0628 - acc: 0.9750 - val_loss: 0.0792 - val_acc: 0.9833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/T8ss0DV6KQjxQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9750000238418579,validation accuracy: 0.9833333492279053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK9zaeGS4IDZ",
        "colab_type": "code",
        "outputId": "3c73d117-27eb-4838-a545-e78c48a3cc11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_test_happy)\n",
        "accuracy_score(y_test_happy, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9466666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLpX_zl9zdqD",
        "colab_type": "code",
        "outputId": "e0ec4482-f451-45c5-fd81-c5f4af89fda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 01 \n",
        "# standardized x train\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 3)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_happy_stnd,\n",
        "    y_train_happy_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=100,\n",
        "    callbacks=check('01'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 480 samples, validate on 120 samples\n",
            "Epoch 1/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 3.0415 - acc: 0.5025\n",
            "Epoch 00001: val_acc improved from -inf to 0.45000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-01-0.45000-0.50417.hdf5\n",
            "480/480 [==============================] - 7s 14ms/sample - loss: 2.6596 - acc: 0.5042 - val_loss: 0.6961 - val_acc: 0.4500\n",
            "Epoch 2/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.8206 - acc: 0.5225\n",
            "Epoch 00002: val_acc did not improve from 0.45000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.7970 - acc: 0.5250 - val_loss: 0.6955 - val_acc: 0.4500\n",
            "Epoch 3/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6847 - acc: 0.5425\n",
            "Epoch 00003: val_acc did not improve from 0.45000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6863 - acc: 0.5396 - val_loss: 0.6945 - val_acc: 0.4500\n",
            "Epoch 4/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6844 - acc: 0.5800\n",
            "Epoch 00004: val_acc did not improve from 0.45000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6839 - acc: 0.5813 - val_loss: 0.6936 - val_acc: 0.4417\n",
            "Epoch 5/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6859 - acc: 0.5650\n",
            "Epoch 00005: val_acc improved from 0.45000 to 0.49167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-05-0.49167-0.57708.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6853 - acc: 0.5771 - val_loss: 0.6932 - val_acc: 0.4917\n",
            "Epoch 6/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6756 - acc: 0.6075\n",
            "Epoch 00006: val_acc improved from 0.49167 to 0.50000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-06-0.50000-0.62708.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6723 - acc: 0.6271 - val_loss: 0.6887 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6421 - acc: 0.7150\n",
            "Epoch 00007: val_acc improved from 0.50000 to 0.54167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-07-0.54167-0.70208.hdf5\n",
            "480/480 [==============================] - 1s 3ms/sample - loss: 0.6425 - acc: 0.7021 - val_loss: 0.6861 - val_acc: 0.5417\n",
            "Epoch 8/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6218 - acc: 0.6625\n",
            "Epoch 00008: val_acc improved from 0.54167 to 0.63333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-08-0.63333-0.66875.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6113 - acc: 0.6687 - val_loss: 0.6682 - val_acc: 0.6333\n",
            "Epoch 9/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5610 - acc: 0.6825\n",
            "Epoch 00009: val_acc improved from 0.63333 to 0.65833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-09-0.65833-0.68125.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5599 - acc: 0.6812 - val_loss: 0.6480 - val_acc: 0.6583\n",
            "Epoch 10/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5295 - acc: 0.7125\n",
            "Epoch 00010: val_acc did not improve from 0.65833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5155 - acc: 0.7250 - val_loss: 0.6549 - val_acc: 0.5750\n",
            "Epoch 11/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4661 - acc: 0.7225\n",
            "Epoch 00011: val_acc did not improve from 0.65833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4791 - acc: 0.7208 - val_loss: 0.6392 - val_acc: 0.5917\n",
            "Epoch 12/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4189 - acc: 0.7825\n",
            "Epoch 00012: val_acc did not improve from 0.65833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4213 - acc: 0.7812 - val_loss: 0.6779 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3766 - acc: 0.8225\n",
            "Epoch 00013: val_acc did not improve from 0.65833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3824 - acc: 0.8229 - val_loss: 0.6594 - val_acc: 0.5667\n",
            "Epoch 14/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3647 - acc: 0.8275\n",
            "Epoch 00014: val_acc did not improve from 0.65833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3681 - acc: 0.8229 - val_loss: 0.6178 - val_acc: 0.6083\n",
            "Epoch 15/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3296 - acc: 0.8575\n",
            "Epoch 00015: val_acc improved from 0.65833 to 0.66667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-15-0.66667-0.85208.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3353 - acc: 0.8521 - val_loss: 0.5778 - val_acc: 0.6667\n",
            "Epoch 16/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3174 - acc: 0.8575\n",
            "Epoch 00016: val_acc did not improve from 0.66667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3113 - acc: 0.8687 - val_loss: 0.6486 - val_acc: 0.5333\n",
            "Epoch 17/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2752 - acc: 0.8625\n",
            "Epoch 00017: val_acc did not improve from 0.66667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2874 - acc: 0.8583 - val_loss: 0.6161 - val_acc: 0.6250\n",
            "Epoch 18/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2442 - acc: 0.8875\n",
            "Epoch 00018: val_acc improved from 0.66667 to 0.69167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-18-0.69167-0.87708.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2653 - acc: 0.8771 - val_loss: 0.5874 - val_acc: 0.6917\n",
            "Epoch 19/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2632 - acc: 0.8900\n",
            "Epoch 00019: val_acc improved from 0.69167 to 0.72500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-19-0.72500-0.88333.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2689 - acc: 0.8833 - val_loss: 0.5906 - val_acc: 0.7250\n",
            "Epoch 20/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2516 - acc: 0.9075\n",
            "Epoch 00020: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2547 - acc: 0.8979 - val_loss: 0.5979 - val_acc: 0.6250\n",
            "Epoch 21/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1897 - acc: 0.9250\n",
            "Epoch 00021: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1990 - acc: 0.9104 - val_loss: 0.6148 - val_acc: 0.6750\n",
            "Epoch 22/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2061 - acc: 0.9200\n",
            "Epoch 00022: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2022 - acc: 0.9167 - val_loss: 0.6119 - val_acc: 0.7167\n",
            "Epoch 23/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1799 - acc: 0.9250\n",
            "Epoch 00023: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1793 - acc: 0.9229 - val_loss: 0.6429 - val_acc: 0.7250\n",
            "Epoch 24/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1823 - acc: 0.9200\n",
            "Epoch 00024: val_acc improved from 0.72500 to 0.73333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-24-0.73333-0.91875.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1803 - acc: 0.9187 - val_loss: 0.6373 - val_acc: 0.7333\n",
            "Epoch 25/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1826 - acc: 0.9300\n",
            "Epoch 00025: val_acc did not improve from 0.73333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1938 - acc: 0.9229 - val_loss: 0.5424 - val_acc: 0.6667\n",
            "Epoch 26/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1950 - acc: 0.9425\n",
            "Epoch 00026: val_acc did not improve from 0.73333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1901 - acc: 0.9417 - val_loss: 0.5528 - val_acc: 0.6917\n",
            "Epoch 27/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1851 - acc: 0.9350\n",
            "Epoch 00027: val_acc did not improve from 0.73333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1807 - acc: 0.9333 - val_loss: 0.5651 - val_acc: 0.7000\n",
            "Epoch 28/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1790 - acc: 0.9375\n",
            "Epoch 00028: val_acc improved from 0.73333 to 0.74167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-28-0.74167-0.93333.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1746 - acc: 0.9333 - val_loss: 0.5396 - val_acc: 0.7417\n",
            "Epoch 29/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1657 - acc: 0.9325\n",
            "Epoch 00029: val_acc improved from 0.74167 to 0.77500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-29-0.77500-0.93750.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1551 - acc: 0.9375 - val_loss: 0.4384 - val_acc: 0.7750\n",
            "Epoch 30/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1225 - acc: 0.9650\n",
            "Epoch 00030: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1375 - acc: 0.9563 - val_loss: 0.4686 - val_acc: 0.7583\n",
            "Epoch 31/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1609 - acc: 0.9400\n",
            "Epoch 00031: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1502 - acc: 0.9458 - val_loss: 0.4780 - val_acc: 0.7417\n",
            "Epoch 32/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1440 - acc: 0.9475\n",
            "Epoch 00032: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1358 - acc: 0.9500 - val_loss: 0.5534 - val_acc: 0.7417\n",
            "Epoch 33/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1451 - acc: 0.9450\n",
            "Epoch 00033: val_acc improved from 0.77500 to 0.83333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-33-0.83333-0.94375.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1388 - acc: 0.9438 - val_loss: 0.3538 - val_acc: 0.8333\n",
            "Epoch 34/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1107 - acc: 0.9550\n",
            "Epoch 00034: val_acc improved from 0.83333 to 0.89167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-34-0.89167-0.94583.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1332 - acc: 0.9458 - val_loss: 0.2667 - val_acc: 0.8917\n",
            "Epoch 35/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1048 - acc: 0.9625\n",
            "Epoch 00035: val_acc did not improve from 0.89167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1090 - acc: 0.9604 - val_loss: 0.3080 - val_acc: 0.8833\n",
            "Epoch 36/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1277 - acc: 0.9625\n",
            "Epoch 00036: val_acc did not improve from 0.89167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1135 - acc: 0.9688 - val_loss: 0.3767 - val_acc: 0.7833\n",
            "Epoch 37/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1239 - acc: 0.9600\n",
            "Epoch 00037: val_acc did not improve from 0.89167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1206 - acc: 0.9563 - val_loss: 0.3252 - val_acc: 0.8250\n",
            "Epoch 38/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0791 - acc: 0.9725\n",
            "Epoch 00038: val_acc did not improve from 0.89167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0820 - acc: 0.9708 - val_loss: 0.2483 - val_acc: 0.8917\n",
            "Epoch 39/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1338 - acc: 0.9525\n",
            "Epoch 00039: val_acc did not improve from 0.89167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1210 - acc: 0.9563 - val_loss: 0.2780 - val_acc: 0.8750\n",
            "Epoch 40/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1116 - acc: 0.9575\n",
            "Epoch 00040: val_acc improved from 0.89167 to 0.91667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-40-0.91667-0.95833.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1059 - acc: 0.9583 - val_loss: 0.2341 - val_acc: 0.9167\n",
            "Epoch 41/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0806 - acc: 0.9675\n",
            "Epoch 00041: val_acc improved from 0.91667 to 0.94167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-41-0.94167-0.96458.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0808 - acc: 0.9646 - val_loss: 0.1692 - val_acc: 0.9417\n",
            "Epoch 42/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0800 - acc: 0.9725\n",
            "Epoch 00042: val_acc did not improve from 0.94167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0826 - acc: 0.9729 - val_loss: 0.1759 - val_acc: 0.9417\n",
            "Epoch 43/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1188 - acc: 0.9675\n",
            "Epoch 00043: val_acc did not improve from 0.94167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1135 - acc: 0.9688 - val_loss: 0.3163 - val_acc: 0.8667\n",
            "Epoch 44/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1219 - acc: 0.9625\n",
            "Epoch 00044: val_acc did not improve from 0.94167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1308 - acc: 0.9583 - val_loss: 0.2185 - val_acc: 0.9167\n",
            "Epoch 45/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1372 - acc: 0.9625\n",
            "Epoch 00045: val_acc improved from 0.94167 to 0.95833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-45-0.95833-0.96250.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1415 - acc: 0.9625 - val_loss: 0.1815 - val_acc: 0.9583\n",
            "Epoch 46/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1024 - acc: 0.9600\n",
            "Epoch 00046: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0962 - acc: 0.9646 - val_loss: 0.2008 - val_acc: 0.9333\n",
            "Epoch 47/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0794 - acc: 0.9750\n",
            "Epoch 00047: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0975 - acc: 0.9688 - val_loss: 0.1615 - val_acc: 0.9417\n",
            "Epoch 48/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0990 - acc: 0.9725\n",
            "Epoch 00048: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0977 - acc: 0.9708 - val_loss: 0.1434 - val_acc: 0.9250\n",
            "Epoch 49/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0869 - acc: 0.9700\n",
            "Epoch 00049: val_acc improved from 0.95833 to 0.96667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-49-0.96667-0.96875.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0989 - acc: 0.9688 - val_loss: 0.1051 - val_acc: 0.9667\n",
            "Epoch 50/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1030 - acc: 0.9525\n",
            "Epoch 00050: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0996 - acc: 0.9542 - val_loss: 0.1170 - val_acc: 0.9667\n",
            "Epoch 51/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0798 - acc: 0.9675\n",
            "Epoch 00051: val_acc improved from 0.96667 to 0.97500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-51-0.97500-0.97292.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0747 - acc: 0.9729 - val_loss: 0.1344 - val_acc: 0.9750\n",
            "Epoch 52/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0725 - acc: 0.9725\n",
            "Epoch 00052: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0769 - acc: 0.9708 - val_loss: 0.1178 - val_acc: 0.9583\n",
            "Epoch 53/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0741 - acc: 0.9750\n",
            "Epoch 00053: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0663 - acc: 0.9792 - val_loss: 0.1250 - val_acc: 0.9667\n",
            "Epoch 54/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0656 - acc: 0.9725\n",
            "Epoch 00054: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0692 - acc: 0.9708 - val_loss: 0.1089 - val_acc: 0.9750\n",
            "Epoch 55/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0817 - acc: 0.9625\n",
            "Epoch 00055: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0822 - acc: 0.9646 - val_loss: 0.1261 - val_acc: 0.9667\n",
            "Epoch 56/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0631 - acc: 0.9750\n",
            "Epoch 00056: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0634 - acc: 0.9729 - val_loss: 0.1109 - val_acc: 0.9750\n",
            "Epoch 57/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1072 - acc: 0.9650\n",
            "Epoch 00057: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0966 - acc: 0.9688 - val_loss: 0.1366 - val_acc: 0.9417\n",
            "Epoch 58/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0704 - acc: 0.9700\n",
            "Epoch 00058: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0704 - acc: 0.9729 - val_loss: 0.1133 - val_acc: 0.9667\n",
            "Epoch 59/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0622 - acc: 0.9725\n",
            "Epoch 00059: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0957 - acc: 0.9667 - val_loss: 0.0839 - val_acc: 0.9750\n",
            "Epoch 60/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0730 - acc: 0.9675\n",
            "Epoch 00060: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0830 - acc: 0.9646 - val_loss: 0.1141 - val_acc: 0.9583\n",
            "Epoch 61/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0621 - acc: 0.9800\n",
            "Epoch 00061: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0673 - acc: 0.9771 - val_loss: 0.0888 - val_acc: 0.9667\n",
            "Epoch 62/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0645 - acc: 0.9775\n",
            "Epoch 00062: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0789 - acc: 0.9750 - val_loss: 0.0986 - val_acc: 0.9750\n",
            "Epoch 63/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0946 - acc: 0.9675\n",
            "Epoch 00063: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0852 - acc: 0.9729 - val_loss: 0.0931 - val_acc: 0.9750\n",
            "Epoch 64/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0579 - acc: 0.9775\n",
            "Epoch 00064: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0585 - acc: 0.9771 - val_loss: 0.1087 - val_acc: 0.9583\n",
            "Epoch 65/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0529 - acc: 0.9775\n",
            "Epoch 00065: val_acc improved from 0.97500 to 0.98333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/01_weights-improvement-65-0.98333-0.97083.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0708 - acc: 0.9708 - val_loss: 0.0951 - val_acc: 0.9833\n",
            "Epoch 66/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0487 - acc: 0.9925\n",
            "Epoch 00066: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0546 - acc: 0.9896 - val_loss: 0.1011 - val_acc: 0.9750\n",
            "Epoch 67/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0428 - acc: 0.9850\n",
            "Epoch 00067: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0427 - acc: 0.9854 - val_loss: 0.1094 - val_acc: 0.9833\n",
            "Epoch 68/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0468 - acc: 0.9825\n",
            "Epoch 00068: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0473 - acc: 0.9812 - val_loss: 0.1027 - val_acc: 0.9833\n",
            "Epoch 69/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0673 - acc: 0.9825\n",
            "Epoch 00069: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0793 - acc: 0.9771 - val_loss: 0.1006 - val_acc: 0.9667\n",
            "Epoch 70/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0826 - acc: 0.9675\n",
            "Epoch 00070: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0738 - acc: 0.9708 - val_loss: 0.0809 - val_acc: 0.9583\n",
            "Epoch 71/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0696 - acc: 0.9825\n",
            "Epoch 00071: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0797 - acc: 0.9833 - val_loss: 0.0868 - val_acc: 0.9583\n",
            "Epoch 72/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0658 - acc: 0.9675\n",
            "Epoch 00072: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0639 - acc: 0.9688 - val_loss: 0.1029 - val_acc: 0.9667\n",
            "Epoch 73/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0604 - acc: 0.9725\n",
            "Epoch 00073: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0585 - acc: 0.9729 - val_loss: 0.1199 - val_acc: 0.9583\n",
            "Epoch 74/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0601 - acc: 0.9825\n",
            "Epoch 00074: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0548 - acc: 0.9833 - val_loss: 0.0948 - val_acc: 0.9667\n",
            "Epoch 75/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0415 - acc: 0.9850\n",
            "Epoch 00075: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0427 - acc: 0.9854 - val_loss: 0.1091 - val_acc: 0.9583\n",
            "Epoch 76/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0315 - acc: 0.9850\n",
            "Epoch 00076: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0440 - acc: 0.9812 - val_loss: 0.1070 - val_acc: 0.9667\n",
            "Epoch 77/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0497 - acc: 0.9825\n",
            "Epoch 00077: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0651 - acc: 0.9812 - val_loss: 0.1162 - val_acc: 0.9667\n",
            "Epoch 78/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0569 - acc: 0.9650\n",
            "Epoch 00078: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0544 - acc: 0.9667 - val_loss: 0.0953 - val_acc: 0.9750\n",
            "Epoch 79/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0329 - acc: 0.9900\n",
            "Epoch 00079: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0432 - acc: 0.9875 - val_loss: 0.0897 - val_acc: 0.9750\n",
            "Epoch 80/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0544 - acc: 0.9850\n",
            "Epoch 00080: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0589 - acc: 0.9833 - val_loss: 0.1253 - val_acc: 0.9750\n",
            "Epoch 81/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0528 - acc: 0.9825\n",
            "Epoch 00081: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0507 - acc: 0.9812 - val_loss: 0.1427 - val_acc: 0.9750\n",
            "Epoch 82/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0523 - acc: 0.9775\n",
            "Epoch 00082: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0500 - acc: 0.9771 - val_loss: 0.1330 - val_acc: 0.9750\n",
            "Epoch 83/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0404 - acc: 0.9825\n",
            "Epoch 00083: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0399 - acc: 0.9812 - val_loss: 0.1201 - val_acc: 0.9750\n",
            "Epoch 84/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0597 - acc: 0.9875\n",
            "Epoch 00084: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0646 - acc: 0.9833 - val_loss: 0.0933 - val_acc: 0.9750\n",
            "Epoch 85/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0355 - acc: 0.9850\n",
            "Epoch 00085: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0391 - acc: 0.9833 - val_loss: 0.1258 - val_acc: 0.9750\n",
            "Epoch 86/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0587 - acc: 0.9775\n",
            "Epoch 00086: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0560 - acc: 0.9792 - val_loss: 0.1346 - val_acc: 0.9667\n",
            "Epoch 87/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0397 - acc: 0.9850\n",
            "Epoch 00087: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0389 - acc: 0.9854 - val_loss: 0.1399 - val_acc: 0.9750\n",
            "Epoch 88/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0414 - acc: 0.9875\n",
            "Epoch 00088: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0418 - acc: 0.9854 - val_loss: 0.1287 - val_acc: 0.9667\n",
            "Epoch 89/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0569 - acc: 0.9800\n",
            "Epoch 00089: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0497 - acc: 0.9833 - val_loss: 0.1373 - val_acc: 0.9667\n",
            "Epoch 90/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0395 - acc: 0.9800\n",
            "Epoch 00090: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0525 - acc: 0.9792 - val_loss: 0.1448 - val_acc: 0.9750\n",
            "Epoch 91/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0288 - acc: 0.9875\n",
            "Epoch 00091: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0292 - acc: 0.9875 - val_loss: 0.1139 - val_acc: 0.9833\n",
            "Epoch 92/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0418 - acc: 0.9900\n",
            "Epoch 00092: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0388 - acc: 0.9896 - val_loss: 0.1214 - val_acc: 0.9583\n",
            "Epoch 93/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0476 - acc: 0.9800\n",
            "Epoch 00093: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0426 - acc: 0.9833 - val_loss: 0.1162 - val_acc: 0.9583\n",
            "Epoch 94/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0353 - acc: 0.9900\n",
            "Epoch 00094: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0333 - acc: 0.9896 - val_loss: 0.1114 - val_acc: 0.9667\n",
            "Epoch 95/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0316 - acc: 0.9925\n",
            "Epoch 00095: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0356 - acc: 0.9896 - val_loss: 0.1031 - val_acc: 0.9583\n",
            "Epoch 96/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0409 - acc: 0.9775\n",
            "Epoch 00096: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0391 - acc: 0.9812 - val_loss: 0.0791 - val_acc: 0.9750\n",
            "Epoch 97/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0582 - acc: 0.9725\n",
            "Epoch 00097: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0498 - acc: 0.9771 - val_loss: 0.0754 - val_acc: 0.9667\n",
            "Epoch 98/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0248 - acc: 0.9925\n",
            "Epoch 00098: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0217 - acc: 0.9937 - val_loss: 0.1309 - val_acc: 0.9583\n",
            "Epoch 99/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0630 - acc: 0.9725\n",
            "Epoch 00099: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0541 - acc: 0.9771 - val_loss: 0.1037 - val_acc: 0.9750\n",
            "Epoch 100/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0397 - acc: 0.9825\n",
            "Epoch 00100: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0419 - acc: 0.9812 - val_loss: 0.0828 - val_acc: 0.9750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.981249988079071,validation accuracy: 0.9750000238418579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzpz3iHgzl--",
        "colab_type": "code",
        "outputId": "a0472aef-18fb-4150-cbf0-7a6bc7b70226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_test_happy_stnd)\n",
        "accuracy_score(y_test_happy, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9733333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsyrCX-nq0-e",
        "colab_type": "code",
        "outputId": "dfb9d40f-5675-4730-a2dd-f32bcb35878b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 02 \n",
        "# scaled x train\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 3)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_happy_scld,\n",
        "    y_train_happy_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=100,\n",
        "    callbacks=check('02'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 480 samples, validate on 120 samples\n",
            "Epoch 1/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 4.4944 - acc: 0.4975\n",
            "Epoch 00001: val_acc improved from -inf to 0.55000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-01-0.55000-0.49583.hdf5\n",
            "480/480 [==============================] - 7s 14ms/sample - loss: 3.9167 - acc: 0.4958 - val_loss: 0.6870 - val_acc: 0.5500\n",
            "Epoch 2/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.8139 - acc: 0.4525\n",
            "Epoch 00002: val_acc improved from 0.55000 to 0.55833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-02-0.55833-0.46042.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.7997 - acc: 0.4604 - val_loss: 0.6904 - val_acc: 0.5583\n",
            "Epoch 3/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.7000 - acc: 0.5175\n",
            "Epoch 00003: val_acc did not improve from 0.55833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.7000 - acc: 0.5146 - val_loss: 0.6914 - val_acc: 0.5583\n",
            "Epoch 4/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6817 - acc: 0.5800\n",
            "Epoch 00004: val_acc did not improve from 0.55833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6824 - acc: 0.5813 - val_loss: 0.6909 - val_acc: 0.5500\n",
            "Epoch 5/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6839 - acc: 0.5700\n",
            "Epoch 00005: val_acc did not improve from 0.55833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6851 - acc: 0.5667 - val_loss: 0.6893 - val_acc: 0.5500\n",
            "Epoch 6/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6827 - acc: 0.5575\n",
            "Epoch 00006: val_acc did not improve from 0.55833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6831 - acc: 0.5479 - val_loss: 0.6890 - val_acc: 0.5500\n",
            "Epoch 7/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6674 - acc: 0.6225\n",
            "Epoch 00007: val_acc did not improve from 0.55833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6655 - acc: 0.6292 - val_loss: 0.6903 - val_acc: 0.5500\n",
            "Epoch 8/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6637 - acc: 0.6025\n",
            "Epoch 00008: val_acc did not improve from 0.55833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6681 - acc: 0.5917 - val_loss: 0.6865 - val_acc: 0.5500\n",
            "Epoch 9/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6413 - acc: 0.6300\n",
            "Epoch 00009: val_acc improved from 0.55833 to 0.57500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-09-0.57500-0.62917.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6418 - acc: 0.6292 - val_loss: 0.6822 - val_acc: 0.5750\n",
            "Epoch 10/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6165 - acc: 0.6650\n",
            "Epoch 00010: val_acc improved from 0.57500 to 0.58333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-10-0.58333-0.68542.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6082 - acc: 0.6854 - val_loss: 0.6745 - val_acc: 0.5833\n",
            "Epoch 11/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5935 - acc: 0.6575\n",
            "Epoch 00011: val_acc improved from 0.58333 to 0.64167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-11-0.64167-0.66042.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5918 - acc: 0.6604 - val_loss: 0.6648 - val_acc: 0.6417\n",
            "Epoch 12/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5670 - acc: 0.7100\n",
            "Epoch 00012: val_acc improved from 0.64167 to 0.70833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-12-0.70833-0.71667.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5551 - acc: 0.7167 - val_loss: 0.6592 - val_acc: 0.7083\n",
            "Epoch 13/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5035 - acc: 0.7400\n",
            "Epoch 00013: val_acc did not improve from 0.70833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5106 - acc: 0.7354 - val_loss: 0.6544 - val_acc: 0.5917\n",
            "Epoch 14/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4426 - acc: 0.7925\n",
            "Epoch 00014: val_acc improved from 0.70833 to 0.72500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-14-0.72500-0.79167.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4412 - acc: 0.7917 - val_loss: 0.6336 - val_acc: 0.7250\n",
            "Epoch 15/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4045 - acc: 0.8175\n",
            "Epoch 00015: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4195 - acc: 0.8042 - val_loss: 0.7733 - val_acc: 0.4667\n",
            "Epoch 16/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4058 - acc: 0.8150\n",
            "Epoch 00016: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4081 - acc: 0.8188 - val_loss: 0.6180 - val_acc: 0.6583\n",
            "Epoch 17/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3412 - acc: 0.8525\n",
            "Epoch 00017: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3487 - acc: 0.8479 - val_loss: 0.6327 - val_acc: 0.6500\n",
            "Epoch 18/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3110 - acc: 0.8600\n",
            "Epoch 00018: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3213 - acc: 0.8542 - val_loss: 0.6292 - val_acc: 0.6583\n",
            "Epoch 19/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2845 - acc: 0.8775\n",
            "Epoch 00019: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2967 - acc: 0.8667 - val_loss: 0.8817 - val_acc: 0.5250\n",
            "Epoch 20/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3184 - acc: 0.8600\n",
            "Epoch 00020: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3119 - acc: 0.8542 - val_loss: 0.6741 - val_acc: 0.6083\n",
            "Epoch 21/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3101 - acc: 0.8525\n",
            "Epoch 00021: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3020 - acc: 0.8562 - val_loss: 0.7286 - val_acc: 0.6250\n",
            "Epoch 22/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2779 - acc: 0.8725\n",
            "Epoch 00022: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2727 - acc: 0.8771 - val_loss: 0.6881 - val_acc: 0.7000\n",
            "Epoch 23/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2576 - acc: 0.8850\n",
            "Epoch 00023: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2386 - acc: 0.8917 - val_loss: 0.6897 - val_acc: 0.6833\n",
            "Epoch 24/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2418 - acc: 0.9000\n",
            "Epoch 00024: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2389 - acc: 0.9042 - val_loss: 0.6147 - val_acc: 0.7167\n",
            "Epoch 25/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2005 - acc: 0.9125\n",
            "Epoch 00025: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2010 - acc: 0.9146 - val_loss: 0.5894 - val_acc: 0.6917\n",
            "Epoch 26/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1653 - acc: 0.9425\n",
            "Epoch 00026: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1682 - acc: 0.9375 - val_loss: 0.6126 - val_acc: 0.6583\n",
            "Epoch 27/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1944 - acc: 0.9200\n",
            "Epoch 00027: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1878 - acc: 0.9229 - val_loss: 0.7805 - val_acc: 0.5583\n",
            "Epoch 28/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1889 - acc: 0.9250\n",
            "Epoch 00028: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1814 - acc: 0.9312 - val_loss: 0.6998 - val_acc: 0.6083\n",
            "Epoch 29/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1694 - acc: 0.9350\n",
            "Epoch 00029: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1685 - acc: 0.9375 - val_loss: 0.6285 - val_acc: 0.6917\n",
            "Epoch 30/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1485 - acc: 0.9400\n",
            "Epoch 00030: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1540 - acc: 0.9375 - val_loss: 0.6145 - val_acc: 0.7083\n",
            "Epoch 31/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1649 - acc: 0.9400\n",
            "Epoch 00031: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1605 - acc: 0.9417 - val_loss: 0.5759 - val_acc: 0.7000\n",
            "Epoch 32/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1289 - acc: 0.9475\n",
            "Epoch 00032: val_acc did not improve from 0.72500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1452 - acc: 0.9458 - val_loss: 0.5828 - val_acc: 0.6833\n",
            "Epoch 33/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1194 - acc: 0.9600\n",
            "Epoch 00033: val_acc improved from 0.72500 to 0.73333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-33-0.73333-0.95417.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1247 - acc: 0.9542 - val_loss: 0.4795 - val_acc: 0.7333\n",
            "Epoch 34/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1226 - acc: 0.9500\n",
            "Epoch 00034: val_acc improved from 0.73333 to 0.76667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-34-0.76667-0.94375.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1394 - acc: 0.9438 - val_loss: 0.4639 - val_acc: 0.7667\n",
            "Epoch 35/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0942 - acc: 0.9600\n",
            "Epoch 00035: val_acc improved from 0.76667 to 0.78333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-35-0.78333-0.96042.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0946 - acc: 0.9604 - val_loss: 0.5056 - val_acc: 0.7833\n",
            "Epoch 36/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1062 - acc: 0.9650\n",
            "Epoch 00036: val_acc did not improve from 0.78333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0991 - acc: 0.9667 - val_loss: 0.5042 - val_acc: 0.7583\n",
            "Epoch 37/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1375 - acc: 0.9425\n",
            "Epoch 00037: val_acc did not improve from 0.78333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1486 - acc: 0.9396 - val_loss: 0.5758 - val_acc: 0.7000\n",
            "Epoch 38/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0961 - acc: 0.9600\n",
            "Epoch 00038: val_acc did not improve from 0.78333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0978 - acc: 0.9604 - val_loss: 0.5595 - val_acc: 0.6917\n",
            "Epoch 39/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1060 - acc: 0.9625\n",
            "Epoch 00039: val_acc improved from 0.78333 to 0.80000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-39-0.80000-0.96458.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0993 - acc: 0.9646 - val_loss: 0.3632 - val_acc: 0.8000\n",
            "Epoch 40/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1199 - acc: 0.9500\n",
            "Epoch 00040: val_acc did not improve from 0.80000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1221 - acc: 0.9500 - val_loss: 0.3748 - val_acc: 0.8000\n",
            "Epoch 41/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1679 - acc: 0.9500\n",
            "Epoch 00041: val_acc did not improve from 0.80000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1518 - acc: 0.9563 - val_loss: 0.4715 - val_acc: 0.7583\n",
            "Epoch 42/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1592 - acc: 0.9400\n",
            "Epoch 00042: val_acc improved from 0.80000 to 0.86667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-42-0.86667-0.94792.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1483 - acc: 0.9479 - val_loss: 0.2946 - val_acc: 0.8667\n",
            "Epoch 43/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1305 - acc: 0.9425\n",
            "Epoch 00043: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1183 - acc: 0.9500 - val_loss: 0.3502 - val_acc: 0.8417\n",
            "Epoch 44/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1212 - acc: 0.9600\n",
            "Epoch 00044: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1082 - acc: 0.9667 - val_loss: 0.3095 - val_acc: 0.8583\n",
            "Epoch 45/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0941 - acc: 0.9575\n",
            "Epoch 00045: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0873 - acc: 0.9625 - val_loss: 0.3448 - val_acc: 0.8333\n",
            "Epoch 46/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1022 - acc: 0.9675\n",
            "Epoch 00046: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0991 - acc: 0.9646 - val_loss: 0.3127 - val_acc: 0.8167\n",
            "Epoch 47/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1002 - acc: 0.9550\n",
            "Epoch 00047: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0946 - acc: 0.9563 - val_loss: 0.3988 - val_acc: 0.8083\n",
            "Epoch 48/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1203 - acc: 0.9500\n",
            "Epoch 00048: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1203 - acc: 0.9500 - val_loss: 0.3514 - val_acc: 0.8000\n",
            "Epoch 49/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0886 - acc: 0.9600\n",
            "Epoch 00049: val_acc improved from 0.86667 to 0.87500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-49-0.87500-0.95417.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1128 - acc: 0.9542 - val_loss: 0.2575 - val_acc: 0.8750\n",
            "Epoch 50/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1029 - acc: 0.9600\n",
            "Epoch 00050: val_acc improved from 0.87500 to 0.91667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-50-0.91667-0.96458.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0945 - acc: 0.9646 - val_loss: 0.2414 - val_acc: 0.9167\n",
            "Epoch 51/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1033 - acc: 0.9625\n",
            "Epoch 00051: val_acc improved from 0.91667 to 0.94167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-51-0.94167-0.96458.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0941 - acc: 0.9646 - val_loss: 0.1303 - val_acc: 0.9417\n",
            "Epoch 52/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1148 - acc: 0.9500\n",
            "Epoch 00052: val_acc did not improve from 0.94167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1149 - acc: 0.9500 - val_loss: 0.2270 - val_acc: 0.9000\n",
            "Epoch 53/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1205 - acc: 0.9525\n",
            "Epoch 00053: val_acc did not improve from 0.94167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1155 - acc: 0.9563 - val_loss: 0.1703 - val_acc: 0.9333\n",
            "Epoch 54/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0624 - acc: 0.9875\n",
            "Epoch 00054: val_acc improved from 0.94167 to 0.97500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-54-0.97500-0.98333.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0647 - acc: 0.9833 - val_loss: 0.1411 - val_acc: 0.9750\n",
            "Epoch 55/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0785 - acc: 0.9775\n",
            "Epoch 00055: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0732 - acc: 0.9792 - val_loss: 0.1071 - val_acc: 0.9667\n",
            "Epoch 56/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0635 - acc: 0.9825\n",
            "Epoch 00056: val_acc improved from 0.97500 to 0.98333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-56-0.98333-0.98125.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0606 - acc: 0.9812 - val_loss: 0.0857 - val_acc: 0.9833\n",
            "Epoch 57/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0743 - acc: 0.9700\n",
            "Epoch 00057: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0688 - acc: 0.9708 - val_loss: 0.1263 - val_acc: 0.9500\n",
            "Epoch 58/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0837 - acc: 0.9725\n",
            "Epoch 00058: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1043 - acc: 0.9646 - val_loss: 0.0799 - val_acc: 0.9833\n",
            "Epoch 59/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0981 - acc: 0.9525\n",
            "Epoch 00059: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0924 - acc: 0.9563 - val_loss: 0.1347 - val_acc: 0.9667\n",
            "Epoch 60/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0741 - acc: 0.9725\n",
            "Epoch 00060: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0744 - acc: 0.9729 - val_loss: 0.1323 - val_acc: 0.9667\n",
            "Epoch 61/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0675 - acc: 0.9800\n",
            "Epoch 00061: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0611 - acc: 0.9833 - val_loss: 0.0777 - val_acc: 0.9833\n",
            "Epoch 62/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0575 - acc: 0.9775\n",
            "Epoch 00062: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0571 - acc: 0.9750 - val_loss: 0.1200 - val_acc: 0.9583\n",
            "Epoch 63/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0660 - acc: 0.9750\n",
            "Epoch 00063: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0694 - acc: 0.9750 - val_loss: 0.1214 - val_acc: 0.9583\n",
            "Epoch 64/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0799 - acc: 0.9700\n",
            "Epoch 00064: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0728 - acc: 0.9729 - val_loss: 0.2681 - val_acc: 0.9000\n",
            "Epoch 65/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1110 - acc: 0.9600\n",
            "Epoch 00065: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1010 - acc: 0.9625 - val_loss: 0.1061 - val_acc: 0.9583\n",
            "Epoch 66/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0717 - acc: 0.9750\n",
            "Epoch 00066: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0760 - acc: 0.9729 - val_loss: 0.0938 - val_acc: 0.9667\n",
            "Epoch 67/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0515 - acc: 0.9825\n",
            "Epoch 00067: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0501 - acc: 0.9833 - val_loss: 0.1489 - val_acc: 0.9250\n",
            "Epoch 68/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0526 - acc: 0.9775\n",
            "Epoch 00068: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0554 - acc: 0.9771 - val_loss: 0.1273 - val_acc: 0.9500\n",
            "Epoch 69/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0583 - acc: 0.9800\n",
            "Epoch 00069: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0539 - acc: 0.9812 - val_loss: 0.0802 - val_acc: 0.9667\n",
            "Epoch 70/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0686 - acc: 0.9825\n",
            "Epoch 00070: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0606 - acc: 0.9854 - val_loss: 0.0808 - val_acc: 0.9750\n",
            "Epoch 71/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0736 - acc: 0.9825\n",
            "Epoch 00071: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0710 - acc: 0.9833 - val_loss: 0.1345 - val_acc: 0.9583\n",
            "Epoch 72/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0703 - acc: 0.9725\n",
            "Epoch 00072: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0715 - acc: 0.9708 - val_loss: 0.0806 - val_acc: 0.9667\n",
            "Epoch 73/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0421 - acc: 0.9850\n",
            "Epoch 00073: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0419 - acc: 0.9833 - val_loss: 0.0759 - val_acc: 0.9750\n",
            "Epoch 74/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0570 - acc: 0.9875\n",
            "Epoch 00074: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0634 - acc: 0.9833 - val_loss: 0.0901 - val_acc: 0.9667\n",
            "Epoch 75/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0521 - acc: 0.9775\n",
            "Epoch 00075: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0533 - acc: 0.9771 - val_loss: 0.0495 - val_acc: 0.9750\n",
            "Epoch 76/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0590 - acc: 0.9775\n",
            "Epoch 00076: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0617 - acc: 0.9750 - val_loss: 0.0563 - val_acc: 0.9833\n",
            "Epoch 77/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0554 - acc: 0.9725\n",
            "Epoch 00077: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0547 - acc: 0.9750 - val_loss: 0.0883 - val_acc: 0.9583\n",
            "Epoch 78/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0505 - acc: 0.9825\n",
            "Epoch 00078: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0455 - acc: 0.9854 - val_loss: 0.0776 - val_acc: 0.9750\n",
            "Epoch 79/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0669 - acc: 0.9750\n",
            "Epoch 00079: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0621 - acc: 0.9771 - val_loss: 0.0729 - val_acc: 0.9750\n",
            "Epoch 80/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0538 - acc: 0.9775\n",
            "Epoch 00080: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0539 - acc: 0.9792 - val_loss: 0.0767 - val_acc: 0.9833\n",
            "Epoch 81/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0614 - acc: 0.9825\n",
            "Epoch 00081: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0576 - acc: 0.9833 - val_loss: 0.0979 - val_acc: 0.9833\n",
            "Epoch 82/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0437 - acc: 0.9850\n",
            "Epoch 00082: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0445 - acc: 0.9854 - val_loss: 0.0868 - val_acc: 0.9833\n",
            "Epoch 83/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0569 - acc: 0.9825\n",
            "Epoch 00083: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0520 - acc: 0.9833 - val_loss: 0.0937 - val_acc: 0.9833\n",
            "Epoch 84/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0338 - acc: 0.9925\n",
            "Epoch 00084: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0348 - acc: 0.9917 - val_loss: 0.0690 - val_acc: 0.9833\n",
            "Epoch 85/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0567 - acc: 0.9850\n",
            "Epoch 00085: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0490 - acc: 0.9875 - val_loss: 0.0727 - val_acc: 0.9833\n",
            "Epoch 86/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0398 - acc: 0.9825\n",
            "Epoch 00086: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0385 - acc: 0.9854 - val_loss: 0.0666 - val_acc: 0.9833\n",
            "Epoch 87/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0335 - acc: 0.9875\n",
            "Epoch 00087: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0350 - acc: 0.9854 - val_loss: 0.0793 - val_acc: 0.9667\n",
            "Epoch 88/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0374 - acc: 0.9825\n",
            "Epoch 00088: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0398 - acc: 0.9833 - val_loss: 0.0645 - val_acc: 0.9750\n",
            "Epoch 89/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0373 - acc: 0.9825\n",
            "Epoch 00089: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0331 - acc: 0.9854 - val_loss: 0.0747 - val_acc: 0.9750\n",
            "Epoch 90/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0352 - acc: 0.9900\n",
            "Epoch 00090: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0372 - acc: 0.9896 - val_loss: 0.0703 - val_acc: 0.9833\n",
            "Epoch 91/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0433 - acc: 0.9825\n",
            "Epoch 00091: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0365 - acc: 0.9854 - val_loss: 0.1051 - val_acc: 0.9667\n",
            "Epoch 92/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0510 - acc: 0.9850\n",
            "Epoch 00092: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0499 - acc: 0.9833 - val_loss: 0.1232 - val_acc: 0.9583\n",
            "Epoch 93/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0471 - acc: 0.9800\n",
            "Epoch 00093: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0720 - acc: 0.9750 - val_loss: 0.0660 - val_acc: 0.9750\n",
            "Epoch 94/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0409 - acc: 0.9875\n",
            "Epoch 00094: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0457 - acc: 0.9833 - val_loss: 0.0893 - val_acc: 0.9833\n",
            "Epoch 95/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0625 - acc: 0.9800\n",
            "Epoch 00095: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0570 - acc: 0.9812 - val_loss: 0.0652 - val_acc: 0.9833\n",
            "Epoch 96/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0515 - acc: 0.9800\n",
            "Epoch 00096: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0470 - acc: 0.9812 - val_loss: 0.0689 - val_acc: 0.9667\n",
            "Epoch 97/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0329 - acc: 0.9875\n",
            "Epoch 00097: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0387 - acc: 0.9833 - val_loss: 0.0526 - val_acc: 0.9833\n",
            "Epoch 98/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0427 - acc: 0.9850\n",
            "Epoch 00098: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0372 - acc: 0.9875 - val_loss: 0.0694 - val_acc: 0.9833\n",
            "Epoch 99/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0330 - acc: 0.9900\n",
            "Epoch 00099: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0458 - acc: 0.9833 - val_loss: 0.0842 - val_acc: 0.9750\n",
            "Epoch 100/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0475 - acc: 0.9825\n",
            "Epoch 00100: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0506 - acc: 0.9833 - val_loss: 0.0694 - val_acc: 0.9667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9833333492279053,validation accuracy: 0.9666666388511658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9rrg_kosv7M",
        "colab_type": "code",
        "outputId": "3af83958-e6b6-4197-91dd-671130a4259c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_test_happy_scld)\n",
        "accuracy_score(y_test_happy, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.98"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ-g79ToGR-a",
        "colab_type": "text"
      },
      "source": [
        "##### unlabeled dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzFnNVGbzfGk",
        "colab_type": "code",
        "outputId": "bd13b90f-d6c0-42d8-ebcc-c2aa72a02ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 03 \n",
        "# unscaled x train\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=200,\n",
        "    callbacks=check('03'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.6908 - acc: 0.5950\n",
            "Epoch 00001: val_acc improved from -inf to 0.40625, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-01-0.40625-0.57422.hdf5\n",
            "256/256 [==============================] - 7s 28ms/sample - loss: 3.1451 - acc: 0.5742 - val_loss: 0.7296 - val_acc: 0.4062\n",
            "Epoch 2/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.2632 - acc: 0.4650\n",
            "Epoch 00002: val_acc improved from 0.40625 to 0.59375, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-02-0.59375-0.49609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 1.1699 - acc: 0.4961 - val_loss: 0.6749 - val_acc: 0.5938\n",
            "Epoch 3/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8265 - acc: 0.6100\n",
            "Epoch 00003: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7935 - acc: 0.6211 - val_loss: 0.6853 - val_acc: 0.5938\n",
            "Epoch 4/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6403 - acc: 0.6350\n",
            "Epoch 00004: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6368 - acc: 0.6445 - val_loss: 0.6884 - val_acc: 0.5938\n",
            "Epoch 5/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6419 - acc: 0.6750\n",
            "Epoch 00005: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6431 - acc: 0.6719 - val_loss: 0.6780 - val_acc: 0.5938\n",
            "Epoch 6/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6342 - acc: 0.6650\n",
            "Epoch 00006: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6302 - acc: 0.6758 - val_loss: 0.6662 - val_acc: 0.5938\n",
            "Epoch 7/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6173 - acc: 0.6850\n",
            "Epoch 00007: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6117 - acc: 0.6875 - val_loss: 0.6787 - val_acc: 0.5938\n",
            "Epoch 8/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6249 - acc: 0.6900\n",
            "Epoch 00008: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6068 - acc: 0.7070 - val_loss: 0.6658 - val_acc: 0.5938\n",
            "Epoch 9/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5753 - acc: 0.6900\n",
            "Epoch 00009: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5577 - acc: 0.7188 - val_loss: 0.6527 - val_acc: 0.5938\n",
            "Epoch 10/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5372 - acc: 0.7650\n",
            "Epoch 00010: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5321 - acc: 0.7852 - val_loss: 0.6518 - val_acc: 0.5938\n",
            "Epoch 11/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5152 - acc: 0.7550\n",
            "Epoch 00011: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5119 - acc: 0.7539 - val_loss: 0.6479 - val_acc: 0.5938\n",
            "Epoch 12/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4816 - acc: 0.7850\n",
            "Epoch 00012: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4801 - acc: 0.7891 - val_loss: 0.6350 - val_acc: 0.5938\n",
            "Epoch 13/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4201 - acc: 0.8150\n",
            "Epoch 00013: val_acc did not improve from 0.59375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4491 - acc: 0.8008 - val_loss: 0.6234 - val_acc: 0.5938\n",
            "Epoch 14/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4555 - acc: 0.7800\n",
            "Epoch 00014: val_acc improved from 0.59375 to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-14-0.67188-0.79688.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.4425 - acc: 0.7969 - val_loss: 0.6243 - val_acc: 0.6719\n",
            "Epoch 15/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3861 - acc: 0.8650\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3774 - acc: 0.8555 - val_loss: 0.6343 - val_acc: 0.5938\n",
            "Epoch 16/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3124 - acc: 0.8850\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3545 - acc: 0.8555 - val_loss: 0.6473 - val_acc: 0.5938\n",
            "Epoch 17/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2973 - acc: 0.8500\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3229 - acc: 0.8438 - val_loss: 0.6352 - val_acc: 0.5938\n",
            "Epoch 18/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3363 - acc: 0.8600\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3334 - acc: 0.8555 - val_loss: 0.6464 - val_acc: 0.5938\n",
            "Epoch 19/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2826 - acc: 0.8750\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2968 - acc: 0.8633 - val_loss: 0.6557 - val_acc: 0.6094\n",
            "Epoch 20/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2387 - acc: 0.8650\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2454 - acc: 0.8672 - val_loss: 0.7014 - val_acc: 0.5938\n",
            "Epoch 21/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2392 - acc: 0.8800\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2269 - acc: 0.8867 - val_loss: 0.6910 - val_acc: 0.6094\n",
            "Epoch 22/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2086 - acc: 0.9050\n",
            "Epoch 00022: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2019 - acc: 0.9102 - val_loss: 0.7038 - val_acc: 0.6094\n",
            "Epoch 23/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2165 - acc: 0.9000\n",
            "Epoch 00023: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2351 - acc: 0.8984 - val_loss: 0.7236 - val_acc: 0.6094\n",
            "Epoch 24/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1833 - acc: 0.9250\n",
            "Epoch 00024: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2028 - acc: 0.9102 - val_loss: 0.7117 - val_acc: 0.6406\n",
            "Epoch 25/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2331 - acc: 0.9000\n",
            "Epoch 00025: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2261 - acc: 0.8984 - val_loss: 0.7606 - val_acc: 0.5938\n",
            "Epoch 26/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2090 - acc: 0.9250\n",
            "Epoch 00026: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2004 - acc: 0.9297 - val_loss: 0.7951 - val_acc: 0.5938\n",
            "Epoch 27/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2108 - acc: 0.9100\n",
            "Epoch 00027: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2107 - acc: 0.9102 - val_loss: 0.8670 - val_acc: 0.5938\n",
            "Epoch 28/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2604 - acc: 0.8800\n",
            "Epoch 00028: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2588 - acc: 0.8906 - val_loss: 0.8423 - val_acc: 0.5938\n",
            "Epoch 29/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1952 - acc: 0.9050\n",
            "Epoch 00029: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1964 - acc: 0.9062 - val_loss: 0.9708 - val_acc: 0.5938\n",
            "Epoch 30/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1671 - acc: 0.9500\n",
            "Epoch 00030: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1592 - acc: 0.9492 - val_loss: 0.9708 - val_acc: 0.5938\n",
            "Epoch 31/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1471 - acc: 0.9550\n",
            "Epoch 00031: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1466 - acc: 0.9453 - val_loss: 1.0065 - val_acc: 0.5938\n",
            "Epoch 32/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1324 - acc: 0.9500\n",
            "Epoch 00032: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1397 - acc: 0.9492 - val_loss: 1.2779 - val_acc: 0.5938\n",
            "Epoch 33/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1128 - acc: 0.9450\n",
            "Epoch 00033: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1100 - acc: 0.9492 - val_loss: 1.1789 - val_acc: 0.5938\n",
            "Epoch 34/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1146 - acc: 0.9600\n",
            "Epoch 00034: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1361 - acc: 0.9531 - val_loss: 1.2620 - val_acc: 0.5938\n",
            "Epoch 35/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1610 - acc: 0.9400\n",
            "Epoch 00035: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1688 - acc: 0.9414 - val_loss: 1.3571 - val_acc: 0.5938\n",
            "Epoch 36/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1438 - acc: 0.9250\n",
            "Epoch 00036: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1497 - acc: 0.9297 - val_loss: 1.2594 - val_acc: 0.5938\n",
            "Epoch 37/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0991 - acc: 0.9750\n",
            "Epoch 00037: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1134 - acc: 0.9609 - val_loss: 1.3123 - val_acc: 0.5938\n",
            "Epoch 38/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0828 - acc: 0.9700\n",
            "Epoch 00038: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0909 - acc: 0.9688 - val_loss: 1.3794 - val_acc: 0.5938\n",
            "Epoch 39/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1050 - acc: 0.9400\n",
            "Epoch 00039: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1011 - acc: 0.9492 - val_loss: 1.4404 - val_acc: 0.5938\n",
            "Epoch 40/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1130 - acc: 0.9550\n",
            "Epoch 00040: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1021 - acc: 0.9570 - val_loss: 1.6978 - val_acc: 0.5938\n",
            "Epoch 41/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1322 - acc: 0.9400\n",
            "Epoch 00041: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1114 - acc: 0.9531 - val_loss: 1.6289 - val_acc: 0.5938\n",
            "Epoch 42/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0856 - acc: 0.9750\n",
            "Epoch 00042: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1002 - acc: 0.9688 - val_loss: 1.7080 - val_acc: 0.5938\n",
            "Epoch 43/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0532 - acc: 0.9800\n",
            "Epoch 00043: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0780 - acc: 0.9688 - val_loss: 1.5897 - val_acc: 0.5938\n",
            "Epoch 44/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0716 - acc: 0.9700\n",
            "Epoch 00044: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0712 - acc: 0.9688 - val_loss: 1.7002 - val_acc: 0.5938\n",
            "Epoch 45/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0852 - acc: 0.9550\n",
            "Epoch 00045: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0912 - acc: 0.9570 - val_loss: 2.2054 - val_acc: 0.5938\n",
            "Epoch 46/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1333 - acc: 0.9450\n",
            "Epoch 00046: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1325 - acc: 0.9453 - val_loss: 1.6754 - val_acc: 0.5938\n",
            "Epoch 47/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1404 - acc: 0.9550\n",
            "Epoch 00047: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1341 - acc: 0.9531 - val_loss: 1.9072 - val_acc: 0.5938\n",
            "Epoch 48/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1014 - acc: 0.9650\n",
            "Epoch 00048: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0890 - acc: 0.9688 - val_loss: 2.1492 - val_acc: 0.5938\n",
            "Epoch 49/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1216 - acc: 0.9550\n",
            "Epoch 00049: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1092 - acc: 0.9609 - val_loss: 1.5107 - val_acc: 0.5938\n",
            "Epoch 50/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0947 - acc: 0.9600\n",
            "Epoch 00050: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0827 - acc: 0.9688 - val_loss: 1.5705 - val_acc: 0.5938\n",
            "Epoch 51/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0748 - acc: 0.9750\n",
            "Epoch 00051: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0685 - acc: 0.9805 - val_loss: 1.9362 - val_acc: 0.5938\n",
            "Epoch 52/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0607 - acc: 0.9750\n",
            "Epoch 00052: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0600 - acc: 0.9766 - val_loss: 1.9459 - val_acc: 0.5938\n",
            "Epoch 53/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0717 - acc: 0.9850\n",
            "Epoch 00053: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0637 - acc: 0.9844 - val_loss: 1.9538 - val_acc: 0.5938\n",
            "Epoch 54/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0697 - acc: 0.9750\n",
            "Epoch 00054: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0684 - acc: 0.9727 - val_loss: 2.2050 - val_acc: 0.5938\n",
            "Epoch 55/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0615 - acc: 0.9750\n",
            "Epoch 00055: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0560 - acc: 0.9766 - val_loss: 2.6147 - val_acc: 0.5938\n",
            "Epoch 56/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0429 - acc: 0.9950\n",
            "Epoch 00056: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0398 - acc: 0.9961 - val_loss: 2.6805 - val_acc: 0.5938\n",
            "Epoch 57/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0320 - acc: 0.9900\n",
            "Epoch 00057: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0498 - acc: 0.9766 - val_loss: 3.0767 - val_acc: 0.5938\n",
            "Epoch 58/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0423 - acc: 0.9850\n",
            "Epoch 00058: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0466 - acc: 0.9844 - val_loss: 3.5350 - val_acc: 0.5938\n",
            "Epoch 59/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0818 - acc: 0.9750\n",
            "Epoch 00059: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0733 - acc: 0.9766 - val_loss: 3.0661 - val_acc: 0.5938\n",
            "Epoch 60/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0234 - acc: 0.9950\n",
            "Epoch 00060: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0439 - acc: 0.9883 - val_loss: 2.7930 - val_acc: 0.5938\n",
            "Epoch 61/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0264 - acc: 0.9950\n",
            "Epoch 00061: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0251 - acc: 0.9922 - val_loss: 2.6521 - val_acc: 0.5938\n",
            "Epoch 62/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0249 - acc: 0.9950\n",
            "Epoch 00062: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0363 - acc: 0.9922 - val_loss: 2.5713 - val_acc: 0.5938\n",
            "Epoch 63/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0792 - acc: 0.9800\n",
            "Epoch 00063: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0673 - acc: 0.9805 - val_loss: 2.3166 - val_acc: 0.6094\n",
            "Epoch 64/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0763 - acc: 0.9850\n",
            "Epoch 00064: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0799 - acc: 0.9805 - val_loss: 2.8425 - val_acc: 0.5938\n",
            "Epoch 65/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0388 - acc: 0.9850\n",
            "Epoch 00065: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0310 - acc: 0.9883 - val_loss: 3.7716 - val_acc: 0.5938\n",
            "Epoch 66/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0914 - acc: 0.9750\n",
            "Epoch 00066: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0874 - acc: 0.9766 - val_loss: 3.3698 - val_acc: 0.5938\n",
            "Epoch 67/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0664 - acc: 0.9750\n",
            "Epoch 00067: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0549 - acc: 0.9805 - val_loss: 2.4786 - val_acc: 0.5938\n",
            "Epoch 68/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0433 - acc: 0.9850\n",
            "Epoch 00068: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0371 - acc: 0.9883 - val_loss: 1.7964 - val_acc: 0.6094\n",
            "Epoch 69/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0814 - acc: 0.9550\n",
            "Epoch 00069: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0670 - acc: 0.9648 - val_loss: 2.1081 - val_acc: 0.6094\n",
            "Epoch 70/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0431 - acc: 0.9900\n",
            "Epoch 00070: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0538 - acc: 0.9883 - val_loss: 2.1404 - val_acc: 0.6094\n",
            "Epoch 71/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0272 - acc: 0.9900\n",
            "Epoch 00071: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0325 - acc: 0.9844 - val_loss: 1.6559 - val_acc: 0.6719\n",
            "Epoch 72/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0487 - acc: 0.9850\n",
            "Epoch 00072: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-72-0.68750-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0454 - acc: 0.9844 - val_loss: 1.7821 - val_acc: 0.6875\n",
            "Epoch 73/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0325 - acc: 0.9800\n",
            "Epoch 00073: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0394 - acc: 0.9805 - val_loss: 2.4130 - val_acc: 0.6250\n",
            "Epoch 74/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0753 - acc: 0.9650\n",
            "Epoch 00074: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0611 - acc: 0.9727 - val_loss: 2.1678 - val_acc: 0.6562\n",
            "Epoch 75/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0346 - acc: 0.9900\n",
            "Epoch 00075: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0303 - acc: 0.9922 - val_loss: 1.7615 - val_acc: 0.6875\n",
            "Epoch 76/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0831 - acc: 0.9750\n",
            "Epoch 00076: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0668 - acc: 0.9805 - val_loss: 2.3450 - val_acc: 0.6094\n",
            "Epoch 77/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0469 - acc: 0.9850\n",
            "Epoch 00077: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0448 - acc: 0.9844 - val_loss: 3.2024 - val_acc: 0.6094\n",
            "Epoch 78/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0426 - acc: 0.9900\n",
            "Epoch 00078: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0363 - acc: 0.9922 - val_loss: 3.1677 - val_acc: 0.6094\n",
            "Epoch 79/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0136 - acc: 1.0000\n",
            "Epoch 00079: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0208 - acc: 0.9961 - val_loss: 3.1811 - val_acc: 0.6094\n",
            "Epoch 80/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0367 - acc: 0.9900\n",
            "Epoch 00080: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0314 - acc: 0.9922 - val_loss: 3.5072 - val_acc: 0.5938\n",
            "Epoch 81/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0231 - acc: 0.9850\n",
            "Epoch 00081: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0244 - acc: 0.9844 - val_loss: 3.7464 - val_acc: 0.5938\n",
            "Epoch 82/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0136 - acc: 0.9950\n",
            "Epoch 00082: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0115 - acc: 0.9961 - val_loss: 3.6704 - val_acc: 0.6094\n",
            "Epoch 83/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0182 - acc: 0.9950\n",
            "Epoch 00083: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0166 - acc: 0.9961 - val_loss: 3.5192 - val_acc: 0.6094\n",
            "Epoch 84/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0076 - acc: 1.0000\n",
            "Epoch 00084: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0071 - acc: 1.0000 - val_loss: 3.6265 - val_acc: 0.6094\n",
            "Epoch 85/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0111 - acc: 1.0000\n",
            "Epoch 00085: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0092 - acc: 1.0000 - val_loss: 3.4706 - val_acc: 0.6094\n",
            "Epoch 86/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0199 - acc: 0.9950\n",
            "Epoch 00086: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0162 - acc: 0.9961 - val_loss: 2.9028 - val_acc: 0.6094\n",
            "Epoch 87/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0250 - acc: 0.9900\n",
            "Epoch 00087: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0206 - acc: 0.9922 - val_loss: 2.8344 - val_acc: 0.6094\n",
            "Epoch 88/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0116 - acc: 0.9950\n",
            "Epoch 00088: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0139 - acc: 0.9922 - val_loss: 2.7868 - val_acc: 0.6406\n",
            "Epoch 89/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0331 - acc: 0.9900\n",
            "Epoch 00089: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0260 - acc: 0.9922 - val_loss: 3.1878 - val_acc: 0.6094\n",
            "Epoch 90/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0396 - acc: 0.9850\n",
            "Epoch 00090: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0490 - acc: 0.9844 - val_loss: 2.5223 - val_acc: 0.6875\n",
            "Epoch 91/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0157 - acc: 0.9900\n",
            "Epoch 00091: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0446 - acc: 0.9844 - val_loss: 2.4535 - val_acc: 0.6719\n",
            "Epoch 92/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0086 - acc: 1.0000\n",
            "Epoch 00092: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0174 - acc: 0.9922 - val_loss: 3.3972 - val_acc: 0.6094\n",
            "Epoch 93/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0565 - acc: 0.9850\n",
            "Epoch 00093: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0458 - acc: 0.9883 - val_loss: 2.6539 - val_acc: 0.6250\n",
            "Epoch 94/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0257 - acc: 0.9900\n",
            "Epoch 00094: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0569 - acc: 0.9766 - val_loss: 2.2242 - val_acc: 0.6719\n",
            "Epoch 95/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0315 - acc: 0.9950\n",
            "Epoch 00095: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0276 - acc: 0.9961 - val_loss: 2.6118 - val_acc: 0.6719\n",
            "Epoch 96/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0591 - acc: 0.9800\n",
            "Epoch 00096: val_acc improved from 0.68750 to 0.70312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-96-0.70312-0.97266.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0847 - acc: 0.9727 - val_loss: 1.8466 - val_acc: 0.7031\n",
            "Epoch 97/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0520 - acc: 0.9750\n",
            "Epoch 00097: val_acc improved from 0.70312 to 0.81250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-97-0.81250-0.97656.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0519 - acc: 0.9766 - val_loss: 0.6957 - val_acc: 0.8125\n",
            "Epoch 98/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0421 - acc: 0.9800\n",
            "Epoch 00098: val_acc improved from 0.81250 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-98-0.82812-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0378 - acc: 0.9844 - val_loss: 0.5577 - val_acc: 0.8281\n",
            "Epoch 99/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0293 - acc: 0.9900\n",
            "Epoch 00099: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0411 - acc: 0.9883 - val_loss: 0.6308 - val_acc: 0.8125\n",
            "Epoch 100/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0623 - acc: 0.9700\n",
            "Epoch 00100: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0530 - acc: 0.9727 - val_loss: 0.5989 - val_acc: 0.8125\n",
            "Epoch 101/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0228 - acc: 0.9950\n",
            "Epoch 00101: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0314 - acc: 0.9922 - val_loss: 0.5485 - val_acc: 0.8125\n",
            "Epoch 102/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0167 - acc: 0.9950\n",
            "Epoch 00102: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0310 - acc: 0.9922 - val_loss: 0.6039 - val_acc: 0.8281\n",
            "Epoch 103/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0466 - acc: 0.9800\n",
            "Epoch 00103: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0402 - acc: 0.9844 - val_loss: 0.8708 - val_acc: 0.7969\n",
            "Epoch 104/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0163 - acc: 1.0000\n",
            "Epoch 00104: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0154 - acc: 1.0000 - val_loss: 0.9174 - val_acc: 0.7969\n",
            "Epoch 105/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0435 - acc: 0.9800\n",
            "Epoch 00105: val_acc improved from 0.82812 to 0.85938, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-105-0.85938-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.0377 - acc: 0.9844 - val_loss: 0.6193 - val_acc: 0.8594\n",
            "Epoch 106/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0160 - acc: 0.9950\n",
            "Epoch 00106: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0154 - acc: 0.9961 - val_loss: 0.5251 - val_acc: 0.8281\n",
            "Epoch 107/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0265 - acc: 0.9900\n",
            "Epoch 00107: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0274 - acc: 0.9883 - val_loss: 0.5616 - val_acc: 0.8594\n",
            "Epoch 108/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0290 - acc: 0.9850\n",
            "Epoch 00108: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0241 - acc: 0.9883 - val_loss: 0.5806 - val_acc: 0.8594\n",
            "Epoch 109/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0149 - acc: 0.9950\n",
            "Epoch 00109: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0144 - acc: 0.9961 - val_loss: 0.5962 - val_acc: 0.8594\n",
            "Epoch 110/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 00110: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0064 - acc: 1.0000 - val_loss: 0.6757 - val_acc: 0.8594\n",
            "Epoch 111/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0217 - acc: 0.9950\n",
            "Epoch 00111: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0172 - acc: 0.9961 - val_loss: 1.1332 - val_acc: 0.7500\n",
            "Epoch 112/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0398 - acc: 0.9800\n",
            "Epoch 00112: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0562 - acc: 0.9766 - val_loss: 0.9375 - val_acc: 0.8125\n",
            "Epoch 113/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0110 - acc: 1.0000\n",
            "Epoch 00113: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0140 - acc: 1.0000 - val_loss: 0.6697 - val_acc: 0.8594\n",
            "Epoch 114/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0832 - acc: 0.9800\n",
            "Epoch 00114: val_acc improved from 0.85938 to 0.87500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-114-0.87500-0.98047.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0698 - acc: 0.9805 - val_loss: 0.5307 - val_acc: 0.8750\n",
            "Epoch 115/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0135 - acc: 0.9950\n",
            "Epoch 00115: val_acc improved from 0.87500 to 0.89062, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-115-0.89062-0.99219.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0178 - acc: 0.9922 - val_loss: 0.4244 - val_acc: 0.8906\n",
            "Epoch 116/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0183 - acc: 0.9900\n",
            "Epoch 00116: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0239 - acc: 0.9883 - val_loss: 0.3854 - val_acc: 0.8906\n",
            "Epoch 117/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0221 - acc: 0.9950\n",
            "Epoch 00117: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0232 - acc: 0.9922 - val_loss: 0.4853 - val_acc: 0.8906\n",
            "Epoch 118/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0091 - acc: 1.0000\n",
            "Epoch 00118: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0077 - acc: 1.0000 - val_loss: 0.6123 - val_acc: 0.8594\n",
            "Epoch 119/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 00119: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0191 - acc: 0.9961 - val_loss: 0.7087 - val_acc: 0.8438\n",
            "Epoch 120/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0200 - acc: 0.9950\n",
            "Epoch 00120: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0220 - acc: 0.9922 - val_loss: 0.6654 - val_acc: 0.8281\n",
            "Epoch 121/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0245 - acc: 0.9900\n",
            "Epoch 00121: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0236 - acc: 0.9883 - val_loss: 0.4480 - val_acc: 0.8750\n",
            "Epoch 122/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0245 - acc: 0.9950\n",
            "Epoch 00122: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0208 - acc: 0.9961 - val_loss: 0.3527 - val_acc: 0.8906\n",
            "Epoch 123/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0271 - acc: 0.9900\n",
            "Epoch 00123: val_acc improved from 0.89062 to 0.92188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-123-0.92188-0.99219.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0239 - acc: 0.9922 - val_loss: 0.2877 - val_acc: 0.9219\n",
            "Epoch 124/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0243 - acc: 0.9950\n",
            "Epoch 00124: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0270 - acc: 0.9922 - val_loss: 0.2768 - val_acc: 0.9219\n",
            "Epoch 125/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0197 - acc: 0.9950\n",
            "Epoch 00125: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0165 - acc: 0.9961 - val_loss: 0.3154 - val_acc: 0.9062\n",
            "Epoch 126/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0466 - acc: 0.9750\n",
            "Epoch 00126: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0376 - acc: 0.9805 - val_loss: 0.3904 - val_acc: 0.9219\n",
            "Epoch 127/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0340 - acc: 0.9900\n",
            "Epoch 00127: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0405 - acc: 0.9844 - val_loss: 0.4799 - val_acc: 0.9219\n",
            "Epoch 128/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0183 - acc: 0.9950\n",
            "Epoch 00128: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0159 - acc: 0.9961 - val_loss: 0.5120 - val_acc: 0.9219\n",
            "Epoch 129/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 00129: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0160 - acc: 0.9961 - val_loss: 0.5666 - val_acc: 0.8906\n",
            "Epoch 130/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 00130: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0077 - acc: 1.0000 - val_loss: 0.6125 - val_acc: 0.8750\n",
            "Epoch 131/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0367 - acc: 0.9800\n",
            "Epoch 00131: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0392 - acc: 0.9766 - val_loss: 0.4871 - val_acc: 0.8906\n",
            "Epoch 132/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0129 - acc: 0.9950\n",
            "Epoch 00132: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0288 - acc: 0.9844 - val_loss: 0.4003 - val_acc: 0.9219\n",
            "Epoch 133/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0222 - acc: 0.9850\n",
            "Epoch 00133: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0385 - acc: 0.9805 - val_loss: 0.5265 - val_acc: 0.9062\n",
            "Epoch 134/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0310 - acc: 0.9800\n",
            "Epoch 00134: val_acc improved from 0.92188 to 0.95312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/03_weights-improvement-134-0.95312-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0266 - acc: 0.9844 - val_loss: 0.4765 - val_acc: 0.9531\n",
            "Epoch 135/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0083 - acc: 0.9950\n",
            "Epoch 00135: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0085 - acc: 0.9961 - val_loss: 0.4741 - val_acc: 0.9219\n",
            "Epoch 136/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0194 - acc: 0.9950\n",
            "Epoch 00136: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0201 - acc: 0.9922 - val_loss: 0.4722 - val_acc: 0.9219\n",
            "Epoch 137/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 00137: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.4607 - val_acc: 0.9375\n",
            "Epoch 138/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0099 - acc: 1.0000\n",
            "Epoch 00138: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0097 - acc: 1.0000 - val_loss: 0.4442 - val_acc: 0.9375\n",
            "Epoch 139/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 00139: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0089 - acc: 0.9961 - val_loss: 0.4226 - val_acc: 0.9375\n",
            "Epoch 140/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0306 - acc: 0.9900\n",
            "Epoch 00140: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0253 - acc: 0.9922 - val_loss: 0.4645 - val_acc: 0.9219\n",
            "Epoch 141/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 00141: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.5405 - val_acc: 0.9219\n",
            "Epoch 142/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0153 - acc: 0.9950\n",
            "Epoch 00142: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0136 - acc: 0.9961 - val_loss: 0.5911 - val_acc: 0.9219\n",
            "Epoch 143/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0068 - acc: 1.0000\n",
            "Epoch 00143: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0072 - acc: 1.0000 - val_loss: 0.6429 - val_acc: 0.9062\n",
            "Epoch 144/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 00144: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0262 - acc: 0.9961 - val_loss: 0.7169 - val_acc: 0.8906\n",
            "Epoch 145/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0284 - acc: 0.9950\n",
            "Epoch 00145: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0415 - acc: 0.9883 - val_loss: 0.7471 - val_acc: 0.8906\n",
            "Epoch 146/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0268 - acc: 0.9850\n",
            "Epoch 00146: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0352 - acc: 0.9844 - val_loss: 0.5307 - val_acc: 0.9219\n",
            "Epoch 147/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0444 - acc: 0.9850\n",
            "Epoch 00147: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0350 - acc: 0.9883 - val_loss: 0.4589 - val_acc: 0.9375\n",
            "Epoch 148/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 00148: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0075 - acc: 0.9961 - val_loss: 0.4842 - val_acc: 0.9531\n",
            "Epoch 149/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0442 - acc: 0.9900\n",
            "Epoch 00149: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0351 - acc: 0.9922 - val_loss: 0.4694 - val_acc: 0.9375\n",
            "Epoch 150/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0532 - acc: 0.9700\n",
            "Epoch 00150: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0467 - acc: 0.9766 - val_loss: 0.5164 - val_acc: 0.9062\n",
            "Epoch 151/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0260 - acc: 0.9900\n",
            "Epoch 00151: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0224 - acc: 0.9922 - val_loss: 0.7268 - val_acc: 0.8906\n",
            "Epoch 152/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0274 - acc: 0.9900\n",
            "Epoch 00152: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0226 - acc: 0.9922 - val_loss: 0.7324 - val_acc: 0.9062\n",
            "Epoch 153/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0331 - acc: 0.9900\n",
            "Epoch 00153: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0401 - acc: 0.9883 - val_loss: 0.5923 - val_acc: 0.9062\n",
            "Epoch 154/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0080 - acc: 0.9950\n",
            "Epoch 00154: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0075 - acc: 0.9961 - val_loss: 0.5155 - val_acc: 0.9062\n",
            "Epoch 155/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0216 - acc: 0.9900\n",
            "Epoch 00155: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0201 - acc: 0.9922 - val_loss: 0.4742 - val_acc: 0.8906\n",
            "Epoch 156/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0139 - acc: 0.9950\n",
            "Epoch 00156: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0125 - acc: 0.9961 - val_loss: 0.4770 - val_acc: 0.8750\n",
            "Epoch 157/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0239 - acc: 0.9900\n",
            "Epoch 00157: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0234 - acc: 0.9883 - val_loss: 0.4761 - val_acc: 0.9062\n",
            "Epoch 158/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0263 - acc: 0.9900\n",
            "Epoch 00158: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0270 - acc: 0.9883 - val_loss: 0.4549 - val_acc: 0.8906\n",
            "Epoch 159/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0123 - acc: 0.9950\n",
            "Epoch 00159: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0109 - acc: 0.9961 - val_loss: 0.4643 - val_acc: 0.8906\n",
            "Epoch 160/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 00160: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.5271 - val_acc: 0.8750\n",
            "Epoch 161/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0345 - acc: 0.9900\n",
            "Epoch 00161: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0280 - acc: 0.9922 - val_loss: 0.4947 - val_acc: 0.8750\n",
            "Epoch 162/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 00162: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 0.4773 - val_acc: 0.9062\n",
            "Epoch 163/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0015 - acc: 1.0000    \n",
            "Epoch 00163: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5013 - val_acc: 0.9062\n",
            "Epoch 164/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 00164: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0048 - acc: 1.0000 - val_loss: 0.5019 - val_acc: 0.9219\n",
            "Epoch 165/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0121 - acc: 0.9900\n",
            "Epoch 00165: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0096 - acc: 0.9922 - val_loss: 0.4949 - val_acc: 0.9062\n",
            "Epoch 166/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0048 - acc: 1.0000    \n",
            "Epoch 00166: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0040 - acc: 1.0000 - val_loss: 0.4921 - val_acc: 0.9219\n",
            "Epoch 167/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0218 - acc: 0.9900\n",
            "Epoch 00167: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0173 - acc: 0.9922 - val_loss: 0.4843 - val_acc: 0.9062\n",
            "Epoch 168/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0017 - acc: 1.0000    \n",
            "Epoch 00168: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0055 - acc: 0.9961 - val_loss: 0.4613 - val_acc: 0.9062\n",
            "Epoch 169/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 00169: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.4935 - val_acc: 0.9219\n",
            "Epoch 170/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0139 - acc: 0.9950\n",
            "Epoch 00170: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0110 - acc: 0.9961 - val_loss: 0.4574 - val_acc: 0.9219\n",
            "Epoch 171/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0308 - acc: 0.9950\n",
            "Epoch 00171: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0245 - acc: 0.9961 - val_loss: 0.3924 - val_acc: 0.9375\n",
            "Epoch 172/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0225 - acc: 0.9900\n",
            "Epoch 00172: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0180 - acc: 0.9922 - val_loss: 0.4139 - val_acc: 0.8906\n",
            "Epoch 173/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0096 - acc: 0.9950\n",
            "Epoch 00173: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0086 - acc: 0.9961 - val_loss: 0.5394 - val_acc: 0.8438\n",
            "Epoch 174/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 00174: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 0.5960 - val_acc: 0.8438\n",
            "Epoch 175/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
            "Epoch 00175: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0072 - acc: 1.0000 - val_loss: 0.5278 - val_acc: 0.8594\n",
            "Epoch 176/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 00176: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 0.5443 - val_acc: 0.8906\n",
            "Epoch 177/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0180 - acc: 0.9900\n",
            "Epoch 00177: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0144 - acc: 0.9922 - val_loss: 0.5973 - val_acc: 0.8906\n",
            "Epoch 178/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0108 - acc: 0.9900\n",
            "Epoch 00178: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0154 - acc: 0.9883 - val_loss: 0.5967 - val_acc: 0.8750\n",
            "Epoch 179/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 00179: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0046 - acc: 1.0000 - val_loss: 0.7366 - val_acc: 0.8594\n",
            "Epoch 180/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 00180: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0065 - acc: 0.9961 - val_loss: 0.8965 - val_acc: 0.8281\n",
            "Epoch 181/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0069 - acc: 0.9950    \n",
            "Epoch 00181: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0241 - acc: 0.9922 - val_loss: 0.8456 - val_acc: 0.8438\n",
            "Epoch 182/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0105 - acc: 0.9950    \n",
            "Epoch 00182: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0083 - acc: 0.9961 - val_loss: 0.7350 - val_acc: 0.8594\n",
            "Epoch 183/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0151 - acc: 0.9950\n",
            "Epoch 00183: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0121 - acc: 0.9961 - val_loss: 0.6394 - val_acc: 0.8594\n",
            "Epoch 184/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0288 - acc: 0.9850\n",
            "Epoch 00184: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0277 - acc: 0.9883 - val_loss: 0.6342 - val_acc: 0.8438\n",
            "Epoch 185/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 00185: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0076 - acc: 0.9961 - val_loss: 0.6807 - val_acc: 0.8594\n",
            "Epoch 186/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0165 - acc: 0.9900\n",
            "Epoch 00186: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0130 - acc: 0.9922 - val_loss: 0.7321 - val_acc: 0.8594\n",
            "Epoch 187/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0263 - acc: 0.9950\n",
            "Epoch 00187: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0261 - acc: 0.9922 - val_loss: 0.6915 - val_acc: 0.8594\n",
            "Epoch 188/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 00188: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 0.5891 - val_acc: 0.8906\n",
            "Epoch 189/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 00189: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.5544 - val_acc: 0.8906\n",
            "Epoch 190/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 00190: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0047 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 0.8906\n",
            "Epoch 191/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0269 - acc: 0.9850\n",
            "Epoch 00191: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0212 - acc: 0.9883 - val_loss: 0.5568 - val_acc: 0.9062\n",
            "Epoch 192/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 00192: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0050 - acc: 1.0000 - val_loss: 0.5715 - val_acc: 0.9062\n",
            "Epoch 193/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0018 - acc: 1.0000    \n",
            "Epoch 00193: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0050 - acc: 0.9961 - val_loss: 0.5810 - val_acc: 0.9219\n",
            "Epoch 194/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0221 - acc: 0.9900\n",
            "Epoch 00194: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0174 - acc: 0.9922 - val_loss: 0.5975 - val_acc: 0.9219\n",
            "Epoch 195/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 00195: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.6321 - val_acc: 0.8750\n",
            "Epoch 196/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0089 - acc: 0.9950\n",
            "Epoch 00196: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0070 - acc: 0.9961 - val_loss: 0.7076 - val_acc: 0.8594\n",
            "Epoch 197/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0298 - acc: 0.9950\n",
            "Epoch 00197: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0240 - acc: 0.9961 - val_loss: 0.7647 - val_acc: 0.8594\n",
            "Epoch 198/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0090 - acc: 0.9950\n",
            "Epoch 00198: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0080 - acc: 0.9961 - val_loss: 0.7444 - val_acc: 0.8750\n",
            "Epoch 199/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0212 - acc: 0.9900\n",
            "Epoch 00199: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0166 - acc: 0.9922 - val_loss: 0.6381 - val_acc: 0.8906\n",
            "Epoch 200/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.5159e-04 - acc: 1.0000\n",
            "Epoch 00200: val_acc did not improve from 0.95312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 1.3061e-04 - acc: 1.0000 - val_loss: 0.5844 - val_acc: 0.9219\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUEop5fT/W+IIAtkvk5gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 1.0,validation accuracy: 0.921875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtTsbHsKFmbp",
        "colab_type": "code",
        "outputId": "142266a9-a836-4716-8b75-881b5ab09bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycPXDZVtLjiq",
        "colab_type": "code",
        "outputId": "1b7e2288-a7af-4248-e3ae-a1d1e9c4c7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 04 \n",
        "# scaled x train\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=200,\n",
        "    callbacks=check('04'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.7427 - acc: 0.4750\n",
            "Epoch 00001: val_acc improved from -inf to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.67188-0.50391.hdf5\n",
            "256/256 [==============================] - 4s 18ms/sample - loss: 3.0289 - acc: 0.5039 - val_loss: 0.6219 - val_acc: 0.6719\n",
            "Epoch 2/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.2099 - acc: 0.5750\n",
            "Epoch 00002: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 2.6649 - acc: 0.5859 - val_loss: 0.7071 - val_acc: 0.3281\n",
            "Epoch 3/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8968 - acc: 0.4000\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.8654 - acc: 0.4102 - val_loss: 0.6949 - val_acc: 0.2812\n",
            "Epoch 4/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7350 - acc: 0.4050\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7208 - acc: 0.4453 - val_loss: 0.6888 - val_acc: 0.6719\n",
            "Epoch 5/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6820 - acc: 0.5700\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6848 - acc: 0.5508 - val_loss: 0.6857 - val_acc: 0.6719\n",
            "Epoch 6/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6823 - acc: 0.5700\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6788 - acc: 0.5898 - val_loss: 0.6824 - val_acc: 0.6719\n",
            "Epoch 7/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6796 - acc: 0.5800\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6830 - acc: 0.5664 - val_loss: 0.6758 - val_acc: 0.6719\n",
            "Epoch 8/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6622 - acc: 0.6100\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6616 - acc: 0.6133 - val_loss: 0.6630 - val_acc: 0.6719\n",
            "Epoch 9/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6641 - acc: 0.5900\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6553 - acc: 0.6094 - val_loss: 0.6445 - val_acc: 0.6719\n",
            "Epoch 10/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6341 - acc: 0.6300\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6444 - acc: 0.6094 - val_loss: 0.6249 - val_acc: 0.6719\n",
            "Epoch 11/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6357 - acc: 0.6050\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6349 - acc: 0.6055 - val_loss: 0.6155 - val_acc: 0.6719\n",
            "Epoch 12/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6047 - acc: 0.6300\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6099 - acc: 0.6328 - val_loss: 0.6105 - val_acc: 0.6719\n",
            "Epoch 13/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6015 - acc: 0.7050\n",
            "Epoch 00013: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6062 - acc: 0.6953 - val_loss: 0.6051 - val_acc: 0.6719\n",
            "Epoch 14/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5919 - acc: 0.6750\n",
            "Epoch 00014: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5894 - acc: 0.6953 - val_loss: 0.6023 - val_acc: 0.6719\n",
            "Epoch 15/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5667 - acc: 0.6850\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5570 - acc: 0.7031 - val_loss: 0.5945 - val_acc: 0.6719\n",
            "Epoch 16/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5448 - acc: 0.7250\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5471 - acc: 0.7148 - val_loss: 0.5890 - val_acc: 0.6719\n",
            "Epoch 17/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4826 - acc: 0.7550\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4967 - acc: 0.7422 - val_loss: 0.5873 - val_acc: 0.6719\n",
            "Epoch 18/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5339 - acc: 0.7550\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5129 - acc: 0.7578 - val_loss: 0.5730 - val_acc: 0.6719\n",
            "Epoch 19/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4664 - acc: 0.8150\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4653 - acc: 0.8164 - val_loss: 0.5644 - val_acc: 0.6719\n",
            "Epoch 20/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4364 - acc: 0.8150\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4389 - acc: 0.8086 - val_loss: 0.5580 - val_acc: 0.6719\n",
            "Epoch 21/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4214 - acc: 0.8100\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4358 - acc: 0.8047 - val_loss: 0.5568 - val_acc: 0.6719\n",
            "Epoch 22/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3772 - acc: 0.8550\n",
            "Epoch 00022: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3974 - acc: 0.8242 - val_loss: 0.5583 - val_acc: 0.6719\n",
            "Epoch 23/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3992 - acc: 0.8200\n",
            "Epoch 00023: val_acc improved from 0.67188 to 0.70312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-23-0.70312-0.82031.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.4034 - acc: 0.8203 - val_loss: 0.5565 - val_acc: 0.7031\n",
            "Epoch 24/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3735 - acc: 0.8250\n",
            "Epoch 00024: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3919 - acc: 0.8125 - val_loss: 0.5523 - val_acc: 0.6875\n",
            "Epoch 25/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3196 - acc: 0.8700\n",
            "Epoch 00025: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3348 - acc: 0.8516 - val_loss: 0.5514 - val_acc: 0.6875\n",
            "Epoch 26/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3258 - acc: 0.8850\n",
            "Epoch 00026: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3080 - acc: 0.8906 - val_loss: 0.5532 - val_acc: 0.6875\n",
            "Epoch 27/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3774 - acc: 0.8550\n",
            "Epoch 00027: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3505 - acc: 0.8711 - val_loss: 0.5592 - val_acc: 0.7031\n",
            "Epoch 28/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2713 - acc: 0.8800\n",
            "Epoch 00028: val_acc improved from 0.70312 to 0.75000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-28-0.75000-0.86719.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.3024 - acc: 0.8672 - val_loss: 0.6346 - val_acc: 0.7500\n",
            "Epoch 29/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3220 - acc: 0.8650\n",
            "Epoch 00029: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3256 - acc: 0.8594 - val_loss: 0.6316 - val_acc: 0.6875\n",
            "Epoch 30/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2843 - acc: 0.8800\n",
            "Epoch 00030: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2767 - acc: 0.8828 - val_loss: 0.5879 - val_acc: 0.6719\n",
            "Epoch 31/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2813 - acc: 0.8850\n",
            "Epoch 00031: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2846 - acc: 0.8867 - val_loss: 0.5624 - val_acc: 0.6719\n",
            "Epoch 32/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1870 - acc: 0.9350\n",
            "Epoch 00032: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2138 - acc: 0.9141 - val_loss: 0.5532 - val_acc: 0.6719\n",
            "Epoch 33/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2409 - acc: 0.9050\n",
            "Epoch 00033: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2213 - acc: 0.9141 - val_loss: 0.5592 - val_acc: 0.6719\n",
            "Epoch 34/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2084 - acc: 0.9300\n",
            "Epoch 00034: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2104 - acc: 0.9219 - val_loss: 0.5920 - val_acc: 0.6875\n",
            "Epoch 35/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1974 - acc: 0.9250\n",
            "Epoch 00035: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2068 - acc: 0.9219 - val_loss: 0.6436 - val_acc: 0.6875\n",
            "Epoch 36/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1862 - acc: 0.9250\n",
            "Epoch 00036: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1700 - acc: 0.9336 - val_loss: 0.6695 - val_acc: 0.6719\n",
            "Epoch 37/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1571 - acc: 0.9400\n",
            "Epoch 00037: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1485 - acc: 0.9453 - val_loss: 0.6133 - val_acc: 0.7031\n",
            "Epoch 38/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1786 - acc: 0.9250\n",
            "Epoch 00038: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1932 - acc: 0.9102 - val_loss: 0.6192 - val_acc: 0.6875\n",
            "Epoch 39/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1916 - acc: 0.9150\n",
            "Epoch 00039: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1708 - acc: 0.9258 - val_loss: 0.5618 - val_acc: 0.7188\n",
            "Epoch 40/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1821 - acc: 0.9100\n",
            "Epoch 00040: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1730 - acc: 0.9219 - val_loss: 0.6065 - val_acc: 0.6875\n",
            "Epoch 41/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1551 - acc: 0.9350\n",
            "Epoch 00041: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1519 - acc: 0.9375 - val_loss: 0.6355 - val_acc: 0.6875\n",
            "Epoch 42/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1025 - acc: 0.9600\n",
            "Epoch 00042: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1292 - acc: 0.9492 - val_loss: 0.7176 - val_acc: 0.6875\n",
            "Epoch 43/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1341 - acc: 0.9450\n",
            "Epoch 00043: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1232 - acc: 0.9531 - val_loss: 0.6290 - val_acc: 0.7188\n",
            "Epoch 44/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1310 - acc: 0.9350\n",
            "Epoch 00044: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1411 - acc: 0.9297 - val_loss: 0.6055 - val_acc: 0.7188\n",
            "Epoch 45/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1018 - acc: 0.9650\n",
            "Epoch 00045: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1207 - acc: 0.9531 - val_loss: 0.6808 - val_acc: 0.6875\n",
            "Epoch 46/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0968 - acc: 0.9600\n",
            "Epoch 00046: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0996 - acc: 0.9609 - val_loss: 0.8295 - val_acc: 0.6875\n",
            "Epoch 47/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0960 - acc: 0.9500\n",
            "Epoch 00047: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1006 - acc: 0.9492 - val_loss: 0.7806 - val_acc: 0.6875\n",
            "Epoch 48/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0773 - acc: 0.9700\n",
            "Epoch 00048: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0822 - acc: 0.9688 - val_loss: 0.7453 - val_acc: 0.6875\n",
            "Epoch 49/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0861 - acc: 0.9750\n",
            "Epoch 00049: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0893 - acc: 0.9727 - val_loss: 0.8697 - val_acc: 0.6875\n",
            "Epoch 50/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0989 - acc: 0.9650\n",
            "Epoch 00050: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0973 - acc: 0.9609 - val_loss: 0.9432 - val_acc: 0.6875\n",
            "Epoch 51/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0974 - acc: 0.9550\n",
            "Epoch 00051: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0971 - acc: 0.9570 - val_loss: 0.8818 - val_acc: 0.7031\n",
            "Epoch 52/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1231 - acc: 0.9300\n",
            "Epoch 00052: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1225 - acc: 0.9414 - val_loss: 0.9712 - val_acc: 0.7031\n",
            "Epoch 53/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1004 - acc: 0.9650\n",
            "Epoch 00053: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0987 - acc: 0.9609 - val_loss: 0.8392 - val_acc: 0.7344\n",
            "Epoch 54/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1294 - acc: 0.9500\n",
            "Epoch 00054: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1323 - acc: 0.9414 - val_loss: 0.8744 - val_acc: 0.7188\n",
            "Epoch 55/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1021 - acc: 0.9550\n",
            "Epoch 00055: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0951 - acc: 0.9609 - val_loss: 1.0229 - val_acc: 0.6875\n",
            "Epoch 56/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1104 - acc: 0.9550\n",
            "Epoch 00056: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1186 - acc: 0.9531 - val_loss: 0.8969 - val_acc: 0.7031\n",
            "Epoch 57/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0609 - acc: 0.9800\n",
            "Epoch 00057: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0674 - acc: 0.9766 - val_loss: 0.9822 - val_acc: 0.7031\n",
            "Epoch 58/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0693 - acc: 0.9850\n",
            "Epoch 00058: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0637 - acc: 0.9844 - val_loss: 1.1995 - val_acc: 0.6875\n",
            "Epoch 59/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0483 - acc: 0.9850\n",
            "Epoch 00059: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0530 - acc: 0.9805 - val_loss: 1.0096 - val_acc: 0.7344\n",
            "Epoch 60/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1082 - acc: 0.9450\n",
            "Epoch 00060: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0920 - acc: 0.9531 - val_loss: 0.9802 - val_acc: 0.7344\n",
            "Epoch 61/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0544 - acc: 0.9850\n",
            "Epoch 00061: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0674 - acc: 0.9805 - val_loss: 0.9175 - val_acc: 0.7344\n",
            "Epoch 62/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0471 - acc: 0.9850\n",
            "Epoch 00062: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0592 - acc: 0.9805 - val_loss: 1.2845 - val_acc: 0.7031\n",
            "Epoch 63/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0741 - acc: 0.9700\n",
            "Epoch 00063: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0811 - acc: 0.9688 - val_loss: 1.1860 - val_acc: 0.7031\n",
            "Epoch 64/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0976 - acc: 0.9800\n",
            "Epoch 00064: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0948 - acc: 0.9766 - val_loss: 1.0875 - val_acc: 0.7188\n",
            "Epoch 65/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0668 - acc: 0.9750\n",
            "Epoch 00065: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0773 - acc: 0.9727 - val_loss: 1.4589 - val_acc: 0.6719\n",
            "Epoch 66/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1075 - acc: 0.9700\n",
            "Epoch 00066: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1032 - acc: 0.9688 - val_loss: 0.8978 - val_acc: 0.7344\n",
            "Epoch 67/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0604 - acc: 0.9700\n",
            "Epoch 00067: val_acc improved from 0.75000 to 0.85938, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-67-0.85938-0.97656.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0513 - acc: 0.9766 - val_loss: 0.5794 - val_acc: 0.8594\n",
            "Epoch 68/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0665 - acc: 0.9800\n",
            "Epoch 00068: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0787 - acc: 0.9766 - val_loss: 0.6229 - val_acc: 0.8594\n",
            "Epoch 69/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0957 - acc: 0.9550\n",
            "Epoch 00069: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0801 - acc: 0.9609 - val_loss: 0.5464 - val_acc: 0.8281\n",
            "Epoch 70/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0398 - acc: 0.9900\n",
            "Epoch 00070: val_acc improved from 0.85938 to 0.87500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-70-0.87500-0.98828.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0461 - acc: 0.9883 - val_loss: 0.4330 - val_acc: 0.8750\n",
            "Epoch 71/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0484 - acc: 0.9800\n",
            "Epoch 00071: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0633 - acc: 0.9688 - val_loss: 0.3945 - val_acc: 0.8438\n",
            "Epoch 72/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0440 - acc: 0.9850\n",
            "Epoch 00072: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0457 - acc: 0.9883 - val_loss: 0.4076 - val_acc: 0.8594\n",
            "Epoch 73/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1112 - acc: 0.9650\n",
            "Epoch 00073: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0935 - acc: 0.9727 - val_loss: 0.5067 - val_acc: 0.8281\n",
            "Epoch 74/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0422 - acc: 0.9850\n",
            "Epoch 00074: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0457 - acc: 0.9805 - val_loss: 0.5882 - val_acc: 0.8281\n",
            "Epoch 75/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0266 - acc: 0.9850\n",
            "Epoch 00075: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0320 - acc: 0.9844 - val_loss: 0.6008 - val_acc: 0.8281\n",
            "Epoch 76/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0107 - acc: 1.0000\n",
            "Epoch 00076: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0187 - acc: 1.0000 - val_loss: 0.6756 - val_acc: 0.8125\n",
            "Epoch 77/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0532 - acc: 0.9850\n",
            "Epoch 00077: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0465 - acc: 0.9883 - val_loss: 0.6645 - val_acc: 0.8125\n",
            "Epoch 78/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0310 - acc: 0.9950\n",
            "Epoch 00078: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0366 - acc: 0.9922 - val_loss: 0.5915 - val_acc: 0.8594\n",
            "Epoch 79/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0350 - acc: 0.9900\n",
            "Epoch 00079: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0313 - acc: 0.9922 - val_loss: 0.5353 - val_acc: 0.8594\n",
            "Epoch 80/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0387 - acc: 0.9850\n",
            "Epoch 00080: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0495 - acc: 0.9844 - val_loss: 0.5679 - val_acc: 0.8594\n",
            "Epoch 81/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0358 - acc: 0.9900\n",
            "Epoch 00081: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0406 - acc: 0.9844 - val_loss: 0.6933 - val_acc: 0.8281\n",
            "Epoch 82/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0386 - acc: 0.9900\n",
            "Epoch 00082: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0325 - acc: 0.9922 - val_loss: 0.7333 - val_acc: 0.8281\n",
            "Epoch 83/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0265 - acc: 0.9900\n",
            "Epoch 00083: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0242 - acc: 0.9922 - val_loss: 0.8410 - val_acc: 0.8281\n",
            "Epoch 84/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0388 - acc: 0.9800\n",
            "Epoch 00084: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0462 - acc: 0.9766 - val_loss: 1.0475 - val_acc: 0.7500\n",
            "Epoch 85/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0251 - acc: 0.9900\n",
            "Epoch 00085: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0231 - acc: 0.9922 - val_loss: 1.1941 - val_acc: 0.7188\n",
            "Epoch 86/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0204 - acc: 0.9950\n",
            "Epoch 00086: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0192 - acc: 0.9961 - val_loss: 1.0824 - val_acc: 0.7500\n",
            "Epoch 87/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0151 - acc: 1.0000\n",
            "Epoch 00087: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0180 - acc: 0.9961 - val_loss: 0.9377 - val_acc: 0.8125\n",
            "Epoch 88/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0281 - acc: 0.9900\n",
            "Epoch 00088: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0280 - acc: 0.9883 - val_loss: 0.9107 - val_acc: 0.8125\n",
            "Epoch 89/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0340 - acc: 0.9850\n",
            "Epoch 00089: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0314 - acc: 0.9844 - val_loss: 1.0263 - val_acc: 0.7969\n",
            "Epoch 90/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0248 - acc: 0.9900\n",
            "Epoch 00090: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0297 - acc: 0.9883 - val_loss: 1.2107 - val_acc: 0.7812\n",
            "Epoch 91/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0582 - acc: 0.9800\n",
            "Epoch 00091: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0662 - acc: 0.9727 - val_loss: 0.8453 - val_acc: 0.7969\n",
            "Epoch 92/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0351 - acc: 0.9850\n",
            "Epoch 00092: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0500 - acc: 0.9844 - val_loss: 0.5594 - val_acc: 0.8750\n",
            "Epoch 93/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0177 - acc: 0.9950\n",
            "Epoch 00093: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0187 - acc: 0.9922 - val_loss: 0.5791 - val_acc: 0.8594\n",
            "Epoch 94/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0424 - acc: 0.9800\n",
            "Epoch 00094: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0364 - acc: 0.9844 - val_loss: 0.6101 - val_acc: 0.8594\n",
            "Epoch 95/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0421 - acc: 0.9850\n",
            "Epoch 00095: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0444 - acc: 0.9844 - val_loss: 0.5946 - val_acc: 0.8594\n",
            "Epoch 96/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0577 - acc: 0.9800\n",
            "Epoch 00096: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0657 - acc: 0.9688 - val_loss: 0.5814 - val_acc: 0.8594\n",
            "Epoch 97/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0391 - acc: 0.9900\n",
            "Epoch 00097: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0336 - acc: 0.9922 - val_loss: 0.6649 - val_acc: 0.8750\n",
            "Epoch 98/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0377 - acc: 0.9900\n",
            "Epoch 00098: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0467 - acc: 0.9883 - val_loss: 0.6387 - val_acc: 0.8750\n",
            "Epoch 99/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0534 - acc: 0.9850\n",
            "Epoch 00099: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0454 - acc: 0.9883 - val_loss: 0.5846 - val_acc: 0.8594\n",
            "Epoch 100/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0264 - acc: 0.9900\n",
            "Epoch 00100: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0286 - acc: 0.9883 - val_loss: 0.4864 - val_acc: 0.8750\n",
            "Epoch 101/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0139 - acc: 0.9950\n",
            "Epoch 00101: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0140 - acc: 0.9961 - val_loss: 0.4754 - val_acc: 0.8750\n",
            "Epoch 102/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0249 - acc: 0.9900\n",
            "Epoch 00102: val_acc improved from 0.87500 to 0.89062, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-102-0.89062-0.98828.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0392 - acc: 0.9883 - val_loss: 0.4774 - val_acc: 0.8906\n",
            "Epoch 103/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0491 - acc: 0.9800\n",
            "Epoch 00103: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0465 - acc: 0.9805 - val_loss: 0.4656 - val_acc: 0.8594\n",
            "Epoch 104/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0269 - acc: 0.9900\n",
            "Epoch 00104: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0358 - acc: 0.9883 - val_loss: 0.4799 - val_acc: 0.8438\n",
            "Epoch 105/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0291 - acc: 0.9900\n",
            "Epoch 00105: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0321 - acc: 0.9883 - val_loss: 0.4387 - val_acc: 0.8438\n",
            "Epoch 106/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0240 - acc: 0.9950\n",
            "Epoch 00106: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0334 - acc: 0.9922 - val_loss: 0.4007 - val_acc: 0.8594\n",
            "Epoch 107/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0374 - acc: 0.9900\n",
            "Epoch 00107: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0365 - acc: 0.9922 - val_loss: 0.3968 - val_acc: 0.8438\n",
            "Epoch 108/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0227 - acc: 0.9950\n",
            "Epoch 00108: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0189 - acc: 0.9961 - val_loss: 0.4686 - val_acc: 0.8281\n",
            "Epoch 109/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0147 - acc: 0.9950\n",
            "Epoch 00109: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0162 - acc: 0.9922 - val_loss: 0.5296 - val_acc: 0.8594\n",
            "Epoch 110/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0207 - acc: 0.9900\n",
            "Epoch 00110: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0173 - acc: 0.9922 - val_loss: 0.5545 - val_acc: 0.8438\n",
            "Epoch 111/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0367 - acc: 0.9850\n",
            "Epoch 00111: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0321 - acc: 0.9883 - val_loss: 0.5427 - val_acc: 0.8438\n",
            "Epoch 112/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0384 - acc: 0.9900\n",
            "Epoch 00112: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0480 - acc: 0.9844 - val_loss: 0.5240 - val_acc: 0.8594\n",
            "Epoch 113/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0199 - acc: 0.9950\n",
            "Epoch 00113: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0174 - acc: 0.9961 - val_loss: 0.4440 - val_acc: 0.8750\n",
            "Epoch 114/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0164 - acc: 1.0000\n",
            "Epoch 00114: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0153 - acc: 1.0000 - val_loss: 0.3934 - val_acc: 0.8750\n",
            "Epoch 115/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0280 - acc: 0.9900\n",
            "Epoch 00115: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0243 - acc: 0.9922 - val_loss: 0.3950 - val_acc: 0.8906\n",
            "Epoch 116/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0141 - acc: 0.9950\n",
            "Epoch 00116: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0175 - acc: 0.9922 - val_loss: 0.4584 - val_acc: 0.8750\n",
            "Epoch 117/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0532 - acc: 0.9800\n",
            "Epoch 00117: val_acc improved from 0.89062 to 0.90625, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-117-0.90625-0.97656.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0530 - acc: 0.9766 - val_loss: 0.4166 - val_acc: 0.9062\n",
            "Epoch 118/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0112 - acc: 0.9950\n",
            "Epoch 00118: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0111 - acc: 0.9961 - val_loss: 0.4579 - val_acc: 0.8750\n",
            "Epoch 119/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0511 - acc: 0.9800\n",
            "Epoch 00119: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0433 - acc: 0.9844 - val_loss: 0.5129 - val_acc: 0.8750\n",
            "Epoch 120/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0406 - acc: 0.9850\n",
            "Epoch 00120: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0337 - acc: 0.9883 - val_loss: 0.6471 - val_acc: 0.8750\n",
            "Epoch 121/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0850 - acc: 0.9850\n",
            "Epoch 00121: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0706 - acc: 0.9883 - val_loss: 0.7487 - val_acc: 0.8594\n",
            "Epoch 122/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0216 - acc: 0.9950\n",
            "Epoch 00122: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0198 - acc: 0.9961 - val_loss: 0.6456 - val_acc: 0.9062\n",
            "Epoch 123/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0313 - acc: 0.9900\n",
            "Epoch 00123: val_acc improved from 0.90625 to 0.92188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-123-0.92188-0.98828.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0304 - acc: 0.9883 - val_loss: 0.5087 - val_acc: 0.9219\n",
            "Epoch 124/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0526 - acc: 0.9850\n",
            "Epoch 00124: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0439 - acc: 0.9883 - val_loss: 0.3820 - val_acc: 0.9062\n",
            "Epoch 125/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0142 - acc: 0.9900\n",
            "Epoch 00125: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0192 - acc: 0.9883 - val_loss: 0.3524 - val_acc: 0.9062\n",
            "Epoch 126/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0116 - acc: 0.9950\n",
            "Epoch 00126: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0125 - acc: 0.9961 - val_loss: 0.3895 - val_acc: 0.9062\n",
            "Epoch 127/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0476 - acc: 0.9750\n",
            "Epoch 00127: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0390 - acc: 0.9805 - val_loss: 0.4880 - val_acc: 0.9062\n",
            "Epoch 128/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0119 - acc: 0.9950\n",
            "Epoch 00128: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0118 - acc: 0.9961 - val_loss: 0.5652 - val_acc: 0.8906\n",
            "Epoch 129/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0402 - acc: 0.9850\n",
            "Epoch 00129: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0331 - acc: 0.9883 - val_loss: 0.5640 - val_acc: 0.8906\n",
            "Epoch 130/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0103 - acc: 0.9950\n",
            "Epoch 00130: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0109 - acc: 0.9961 - val_loss: 0.5890 - val_acc: 0.8594\n",
            "Epoch 131/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0632 - acc: 0.9850\n",
            "Epoch 00131: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0506 - acc: 0.9883 - val_loss: 0.5878 - val_acc: 0.8750\n",
            "Epoch 132/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0199 - acc: 0.9900\n",
            "Epoch 00132: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0178 - acc: 0.9922 - val_loss: 0.6160 - val_acc: 0.8906\n",
            "Epoch 133/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0211 - acc: 0.9950\n",
            "Epoch 00133: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0185 - acc: 0.9961 - val_loss: 0.6170 - val_acc: 0.8906\n",
            "Epoch 134/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 00134: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0040 - acc: 1.0000 - val_loss: 0.5869 - val_acc: 0.8906\n",
            "Epoch 135/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0256 - acc: 0.9900\n",
            "Epoch 00135: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0228 - acc: 0.9922 - val_loss: 0.5492 - val_acc: 0.8906\n",
            "Epoch 136/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0191 - acc: 0.9900\n",
            "Epoch 00136: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0153 - acc: 0.9922 - val_loss: 0.5735 - val_acc: 0.9062\n",
            "Epoch 137/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0244 - acc: 0.9850\n",
            "Epoch 00137: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0230 - acc: 0.9883 - val_loss: 0.5623 - val_acc: 0.9062\n",
            "Epoch 138/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0170 - acc: 0.9900\n",
            "Epoch 00138: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0157 - acc: 0.9922 - val_loss: 0.5140 - val_acc: 0.8906\n",
            "Epoch 139/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0114 - acc: 0.9950\n",
            "Epoch 00139: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0282 - acc: 0.9805 - val_loss: 0.5378 - val_acc: 0.8750\n",
            "Epoch 140/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0179 - acc: 0.9900\n",
            "Epoch 00140: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0142 - acc: 0.9922 - val_loss: 0.5852 - val_acc: 0.8906\n",
            "Epoch 141/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0182 - acc: 0.9950\n",
            "Epoch 00141: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0149 - acc: 0.9961 - val_loss: 0.6610 - val_acc: 0.9062\n",
            "Epoch 142/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0070 - acc: 0.9950\n",
            "Epoch 00142: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0076 - acc: 0.9961 - val_loss: 0.6555 - val_acc: 0.9062\n",
            "Epoch 143/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 00143: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.6387 - val_acc: 0.8906\n",
            "Epoch 144/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0185 - acc: 0.9950\n",
            "Epoch 00144: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0200 - acc: 0.9922 - val_loss: 0.6399 - val_acc: 0.8750\n",
            "Epoch 145/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0078 - acc: 1.0000\n",
            "Epoch 00145: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0190 - acc: 0.9961 - val_loss: 0.6589 - val_acc: 0.9062\n",
            "Epoch 146/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 7.8836e-04 - acc: 1.0000\n",
            "Epoch 00146: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.6494 - val_acc: 0.9062\n",
            "Epoch 147/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 00147: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0144 - acc: 0.9961 - val_loss: 0.6315 - val_acc: 0.8906\n",
            "Epoch 148/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 00148: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.6323 - val_acc: 0.8906\n",
            "Epoch 149/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0070 - acc: 1.0000\n",
            "Epoch 00149: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.6565 - val_acc: 0.8906\n",
            "Epoch 150/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0175 - acc: 0.9900\n",
            "Epoch 00150: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0149 - acc: 0.9922 - val_loss: 0.6632 - val_acc: 0.8906\n",
            "Epoch 151/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 00151: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 0.6717 - val_acc: 0.8906\n",
            "Epoch 152/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0092 - acc: 1.0000\n",
            "Epoch 00152: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0077 - acc: 1.0000 - val_loss: 0.6583 - val_acc: 0.8906\n",
            "Epoch 153/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 00153: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.6574 - val_acc: 0.8906\n",
            "Epoch 154/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 6.5588e-04 - acc: 1.0000\n",
            "Epoch 00154: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 0.6604 - val_acc: 0.9062\n",
            "Epoch 155/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0231 - acc: 0.9850\n",
            "Epoch 00155: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0187 - acc: 0.9883 - val_loss: 0.7111 - val_acc: 0.8750\n",
            "Epoch 156/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0084 - acc: 0.9950    \n",
            "Epoch 00156: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0078 - acc: 0.9961 - val_loss: 0.7365 - val_acc: 0.8750\n",
            "Epoch 157/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0081 - acc: 0.9950\n",
            "Epoch 00157: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0064 - acc: 0.9961 - val_loss: 0.6959 - val_acc: 0.8906\n",
            "Epoch 158/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0022 - acc: 1.0000    \n",
            "Epoch 00158: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0050 - acc: 0.9961 - val_loss: 0.6976 - val_acc: 0.8750\n",
            "Epoch 159/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0296 - acc: 0.9850\n",
            "Epoch 00159: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0232 - acc: 0.9883 - val_loss: 0.6728 - val_acc: 0.8750\n",
            "Epoch 160/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0132 - acc: 0.9950\n",
            "Epoch 00160: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0106 - acc: 0.9961 - val_loss: 0.6622 - val_acc: 0.8594\n",
            "Epoch 161/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 00161: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7157 - val_acc: 0.8750\n",
            "Epoch 162/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0097 - acc: 0.9900\n",
            "Epoch 00162: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0079 - acc: 0.9922 - val_loss: 0.7297 - val_acc: 0.8750\n",
            "Epoch 163/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0129 - acc: 0.9950\n",
            "Epoch 00163: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0420 - acc: 0.9883 - val_loss: 0.6645 - val_acc: 0.8750\n",
            "Epoch 164/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0027 - acc: 1.0000    \n",
            "Epoch 00164: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.7960 - val_acc: 0.9062\n",
            "Epoch 165/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 00165: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0485 - acc: 0.9922 - val_loss: 0.8508 - val_acc: 0.8906\n",
            "Epoch 166/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0413 - acc: 0.9850\n",
            "Epoch 00166: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0326 - acc: 0.9883 - val_loss: 0.8800 - val_acc: 0.8750\n",
            "Epoch 167/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0255 - acc: 0.9900\n",
            "Epoch 00167: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0203 - acc: 0.9922 - val_loss: 0.9368 - val_acc: 0.8438\n",
            "Epoch 168/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0128 - acc: 0.9900\n",
            "Epoch 00168: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0121 - acc: 0.9922 - val_loss: 0.8921 - val_acc: 0.8438\n",
            "Epoch 169/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0251 - acc: 0.9900\n",
            "Epoch 00169: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0205 - acc: 0.9922 - val_loss: 0.8488 - val_acc: 0.8281\n",
            "Epoch 170/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0082 - acc: 1.0000    \n",
            "Epoch 00170: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0102 - acc: 1.0000 - val_loss: 0.7929 - val_acc: 0.8438\n",
            "Epoch 171/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0429 - acc: 0.9950\n",
            "Epoch 00171: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0380 - acc: 0.9961 - val_loss: 0.6835 - val_acc: 0.8750\n",
            "Epoch 172/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0129 - acc: 0.9950\n",
            "Epoch 00172: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0123 - acc: 0.9961 - val_loss: 0.6627 - val_acc: 0.8750\n",
            "Epoch 173/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0241 - acc: 0.9900\n",
            "Epoch 00173: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0240 - acc: 0.9883 - val_loss: 0.6942 - val_acc: 0.8750\n",
            "Epoch 174/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 00174: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0069 - acc: 1.0000 - val_loss: 0.7016 - val_acc: 0.8750\n",
            "Epoch 175/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0121 - acc: 0.9950\n",
            "Epoch 00175: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0098 - acc: 0.9961 - val_loss: 0.6615 - val_acc: 0.8750\n",
            "Epoch 176/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0334 - acc: 0.9900\n",
            "Epoch 00176: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0404 - acc: 0.9883 - val_loss: 0.4803 - val_acc: 0.8438\n",
            "Epoch 177/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0228 - acc: 0.9950\n",
            "Epoch 00177: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0184 - acc: 0.9961 - val_loss: 0.4427 - val_acc: 0.8594\n",
            "Epoch 178/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0178 - acc: 0.9850\n",
            "Epoch 00178: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0160 - acc: 0.9883 - val_loss: 0.3908 - val_acc: 0.8594\n",
            "Epoch 179/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0644 - acc: 0.9850\n",
            "Epoch 00179: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0728 - acc: 0.9805 - val_loss: 0.4685 - val_acc: 0.8594\n",
            "Epoch 180/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0148 - acc: 0.9950\n",
            "Epoch 00180: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0275 - acc: 0.9883 - val_loss: 0.6309 - val_acc: 0.7969\n",
            "Epoch 181/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0261 - acc: 0.9900\n",
            "Epoch 00181: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0216 - acc: 0.9922 - val_loss: 0.8002 - val_acc: 0.7812\n",
            "Epoch 182/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0087 - acc: 1.0000\n",
            "Epoch 00182: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0137 - acc: 0.9961 - val_loss: 0.8167 - val_acc: 0.7656\n",
            "Epoch 183/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0162 - acc: 0.9950\n",
            "Epoch 00183: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0143 - acc: 0.9961 - val_loss: 0.6784 - val_acc: 0.8281\n",
            "Epoch 184/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0191 - acc: 0.9900\n",
            "Epoch 00184: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0181 - acc: 0.9922 - val_loss: 0.6548 - val_acc: 0.8125\n",
            "Epoch 185/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0421 - acc: 0.9800\n",
            "Epoch 00185: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0362 - acc: 0.9844 - val_loss: 0.7415 - val_acc: 0.8125\n",
            "Epoch 186/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0340 - acc: 0.9800\n",
            "Epoch 00186: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0274 - acc: 0.9844 - val_loss: 0.7868 - val_acc: 0.8125\n",
            "Epoch 187/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0196 - acc: 0.9950\n",
            "Epoch 00187: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0235 - acc: 0.9883 - val_loss: 0.6059 - val_acc: 0.8281\n",
            "Epoch 188/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0084 - acc: 0.9950\n",
            "Epoch 00188: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0083 - acc: 0.9961 - val_loss: 0.5584 - val_acc: 0.8281\n",
            "Epoch 189/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0215 - acc: 0.9950\n",
            "Epoch 00189: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0171 - acc: 0.9961 - val_loss: 0.5029 - val_acc: 0.8281\n",
            "Epoch 190/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0086 - acc: 1.0000\n",
            "Epoch 00190: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0084 - acc: 1.0000 - val_loss: 0.5736 - val_acc: 0.8281\n",
            "Epoch 191/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 00191: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0046 - acc: 1.0000 - val_loss: 0.6700 - val_acc: 0.8438\n",
            "Epoch 192/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0091 - acc: 0.9950\n",
            "Epoch 00192: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0072 - acc: 0.9961 - val_loss: 0.7985 - val_acc: 0.8125\n",
            "Epoch 193/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0191 - acc: 0.9950\n",
            "Epoch 00193: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0154 - acc: 0.9961 - val_loss: 0.8935 - val_acc: 0.7969\n",
            "Epoch 194/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 00194: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0058 - acc: 0.9961 - val_loss: 0.9186 - val_acc: 0.8125\n",
            "Epoch 195/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 00195: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 0.9158 - val_acc: 0.8438\n",
            "Epoch 196/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 00196: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.8975 - val_acc: 0.8594\n",
            "Epoch 197/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 00197: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.8843 - val_acc: 0.8594\n",
            "Epoch 198/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0283 - acc: 0.9900\n",
            "Epoch 00198: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0249 - acc: 0.9922 - val_loss: 0.8329 - val_acc: 0.8594\n",
            "Epoch 199/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0088 - acc: 0.9900    \n",
            "Epoch 00199: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0069 - acc: 0.9922 - val_loss: 0.8722 - val_acc: 0.8438\n",
            "Epoch 200/200\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0156 - acc: 0.9950\n",
            "Epoch 00200: val_acc did not improve from 0.92188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0180 - acc: 0.9922 - val_loss: 0.8900 - val_acc: 0.8594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSinloElBKaWUgyYFpZRSDv8PdPQInWdVBYYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9921875,validation accuracy: 0.859375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvoKzrHGS8QZ",
        "colab_type": "code",
        "outputId": "c92b5a9d-f471-4633-fce1-be097e35a560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3bHUnPwikd8",
        "colab_type": "code",
        "outputId": "9f7f72c1-1677-425b-9192-48a839b4df06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 05 \n",
        "# increased neurons in convolutional layers (64 -> 256) \n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(256, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(256, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=200,\n",
        "    callbacks=check('05'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/200\n",
            "200/256 [======================>.......] - ETA: 4s - loss: 24.3304 - acc: 0.5000\n",
            "Epoch 00001: val_acc improved from -inf to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.67188-0.48047.hdf5\n",
            "256/256 [==============================] - 28s 110ms/sample - loss: 19.4077 - acc: 0.4805 - val_loss: 0.6386 - val_acc: 0.6719\n",
            "Epoch 2/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.9308 - acc: 0.6000\n",
            "Epoch 00002: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.8732 - acc: 0.6016 - val_loss: 0.6699 - val_acc: 0.6719\n",
            "Epoch 3/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.6632 - acc: 0.6000\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.6601 - acc: 0.6094 - val_loss: 0.6898 - val_acc: 0.6719\n",
            "Epoch 4/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.6558 - acc: 0.6150\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.6485 - acc: 0.6094 - val_loss: 0.6718 - val_acc: 0.6719\n",
            "Epoch 5/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.6260 - acc: 0.6100\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.6421 - acc: 0.5977 - val_loss: 0.6715 - val_acc: 0.6719\n",
            "Epoch 6/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.6024 - acc: 0.6500\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.6119 - acc: 0.6328 - val_loss: 0.6810 - val_acc: 0.6719\n",
            "Epoch 7/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.5981 - acc: 0.7050\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.6165 - acc: 0.6758 - val_loss: 0.6519 - val_acc: 0.6719\n",
            "Epoch 8/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.5605 - acc: 0.7150\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.5579 - acc: 0.6953 - val_loss: 0.6163 - val_acc: 0.6719\n",
            "Epoch 9/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.5505 - acc: 0.7200\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.5487 - acc: 0.7227 - val_loss: 0.6007 - val_acc: 0.6719\n",
            "Epoch 10/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.5692 - acc: 0.6750\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.5490 - acc: 0.6992 - val_loss: 0.5808 - val_acc: 0.6719\n",
            "Epoch 11/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.5136 - acc: 0.7150\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.4996 - acc: 0.7305 - val_loss: 0.5854 - val_acc: 0.6719\n",
            "Epoch 12/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.4462 - acc: 0.7400\n",
            "Epoch 00012: val_acc improved from 0.67188 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-12-0.73438-0.73438.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.4610 - acc: 0.7344 - val_loss: 0.5738 - val_acc: 0.7344\n",
            "Epoch 13/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.4435 - acc: 0.7650\n",
            "Epoch 00013: val_acc improved from 0.73438 to 0.76562, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-13-0.76562-0.77344.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.4503 - acc: 0.7734 - val_loss: 0.5729 - val_acc: 0.7656\n",
            "Epoch 14/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.4558 - acc: 0.7800\n",
            "Epoch 00014: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.4470 - acc: 0.7695 - val_loss: 0.5628 - val_acc: 0.6719\n",
            "Epoch 15/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.4061 - acc: 0.7850\n",
            "Epoch 00015: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.4102 - acc: 0.7852 - val_loss: 0.5616 - val_acc: 0.6875\n",
            "Epoch 16/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.3461 - acc: 0.8750\n",
            "Epoch 00016: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.3564 - acc: 0.8477 - val_loss: 0.5651 - val_acc: 0.7344\n",
            "Epoch 17/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.3679 - acc: 0.8200\n",
            "Epoch 00017: val_acc improved from 0.76562 to 0.79688, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-17-0.79688-0.83594.hdf5\n",
            "256/256 [==============================] - 6s 21ms/sample - loss: 0.3500 - acc: 0.8359 - val_loss: 0.5623 - val_acc: 0.7969\n",
            "Epoch 18/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.3442 - acc: 0.8450\n",
            "Epoch 00018: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.3468 - acc: 0.8359 - val_loss: 0.5736 - val_acc: 0.6875\n",
            "Epoch 19/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.3528 - acc: 0.8650\n",
            "Epoch 00019: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.3458 - acc: 0.8633 - val_loss: 0.5415 - val_acc: 0.7812\n",
            "Epoch 20/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.4375 - acc: 0.8100\n",
            "Epoch 00020: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.4024 - acc: 0.8242 - val_loss: 0.5985 - val_acc: 0.6719\n",
            "Epoch 21/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2376 - acc: 0.9200\n",
            "Epoch 00021: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2992 - acc: 0.8945 - val_loss: 0.5882 - val_acc: 0.6875\n",
            "Epoch 22/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2793 - acc: 0.8550\n",
            "Epoch 00022: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2933 - acc: 0.8555 - val_loss: 0.5712 - val_acc: 0.6875\n",
            "Epoch 23/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2909 - acc: 0.8850\n",
            "Epoch 00023: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2883 - acc: 0.8906 - val_loss: 0.6447 - val_acc: 0.6719\n",
            "Epoch 24/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2996 - acc: 0.8550\n",
            "Epoch 00024: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2841 - acc: 0.8633 - val_loss: 0.6012 - val_acc: 0.6875\n",
            "Epoch 25/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2696 - acc: 0.9000\n",
            "Epoch 00025: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2622 - acc: 0.8984 - val_loss: 0.5749 - val_acc: 0.6875\n",
            "Epoch 26/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2743 - acc: 0.8650\n",
            "Epoch 00026: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2652 - acc: 0.8594 - val_loss: 0.9335 - val_acc: 0.6719\n",
            "Epoch 27/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.3280 - acc: 0.8750\n",
            "Epoch 00027: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.3181 - acc: 0.8789 - val_loss: 0.5412 - val_acc: 0.7188\n",
            "Epoch 28/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2532 - acc: 0.8850\n",
            "Epoch 00028: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2707 - acc: 0.8828 - val_loss: 0.6872 - val_acc: 0.6719\n",
            "Epoch 29/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2484 - acc: 0.8850\n",
            "Epoch 00029: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2527 - acc: 0.8906 - val_loss: 0.7351 - val_acc: 0.6719\n",
            "Epoch 30/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2621 - acc: 0.8700\n",
            "Epoch 00030: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2807 - acc: 0.8750 - val_loss: 0.6572 - val_acc: 0.6719\n",
            "Epoch 31/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2529 - acc: 0.9150\n",
            "Epoch 00031: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2580 - acc: 0.9141 - val_loss: 0.7103 - val_acc: 0.6719\n",
            "Epoch 32/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2274 - acc: 0.9150\n",
            "Epoch 00032: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2398 - acc: 0.9141 - val_loss: 0.7654 - val_acc: 0.6719\n",
            "Epoch 33/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2040 - acc: 0.9050\n",
            "Epoch 00033: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2105 - acc: 0.9102 - val_loss: 0.7556 - val_acc: 0.6562\n",
            "Epoch 34/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2103 - acc: 0.9100\n",
            "Epoch 00034: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2477 - acc: 0.9062 - val_loss: 0.8612 - val_acc: 0.6719\n",
            "Epoch 35/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2175 - acc: 0.8850\n",
            "Epoch 00035: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2473 - acc: 0.8828 - val_loss: 0.8235 - val_acc: 0.6719\n",
            "Epoch 36/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2242 - acc: 0.9100\n",
            "Epoch 00036: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2198 - acc: 0.9062 - val_loss: 0.7779 - val_acc: 0.6719\n",
            "Epoch 37/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2503 - acc: 0.8750\n",
            "Epoch 00037: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2543 - acc: 0.8828 - val_loss: 0.7301 - val_acc: 0.7031\n",
            "Epoch 38/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2203 - acc: 0.9250\n",
            "Epoch 00038: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2542 - acc: 0.9062 - val_loss: 0.7987 - val_acc: 0.7031\n",
            "Epoch 39/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2377 - acc: 0.8800\n",
            "Epoch 00039: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2467 - acc: 0.8867 - val_loss: 0.9335 - val_acc: 0.7031\n",
            "Epoch 40/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1949 - acc: 0.9100\n",
            "Epoch 00040: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1867 - acc: 0.9141 - val_loss: 0.9426 - val_acc: 0.7188\n",
            "Epoch 41/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1242 - acc: 0.9600\n",
            "Epoch 00041: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1831 - acc: 0.9297 - val_loss: 1.1270 - val_acc: 0.7344\n",
            "Epoch 42/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1703 - acc: 0.9500\n",
            "Epoch 00042: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1674 - acc: 0.9375 - val_loss: 1.1280 - val_acc: 0.7188\n",
            "Epoch 43/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.2137 - acc: 0.9050\n",
            "Epoch 00043: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2128 - acc: 0.9102 - val_loss: 0.7747 - val_acc: 0.7812\n",
            "Epoch 44/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1978 - acc: 0.9350\n",
            "Epoch 00044: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2115 - acc: 0.9219 - val_loss: 0.6838 - val_acc: 0.7812\n",
            "Epoch 45/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1466 - acc: 0.9550\n",
            "Epoch 00045: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1348 - acc: 0.9609 - val_loss: 0.8020 - val_acc: 0.7812\n",
            "Epoch 46/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1779 - acc: 0.9150\n",
            "Epoch 00046: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1518 - acc: 0.9258 - val_loss: 0.9691 - val_acc: 0.7500\n",
            "Epoch 47/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1932 - acc: 0.9400\n",
            "Epoch 00047: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2218 - acc: 0.9219 - val_loss: 0.9752 - val_acc: 0.7812\n",
            "Epoch 48/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1825 - acc: 0.9250\n",
            "Epoch 00048: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.2105 - acc: 0.9141 - val_loss: 1.6345 - val_acc: 0.6875\n",
            "Epoch 49/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1357 - acc: 0.9500\n",
            "Epoch 00049: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1444 - acc: 0.9492 - val_loss: 2.2155 - val_acc: 0.6719\n",
            "Epoch 50/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1648 - acc: 0.9400\n",
            "Epoch 00050: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1770 - acc: 0.9375 - val_loss: 1.2594 - val_acc: 0.7656\n",
            "Epoch 51/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1108 - acc: 0.9600\n",
            "Epoch 00051: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1208 - acc: 0.9531 - val_loss: 0.7880 - val_acc: 0.7812\n",
            "Epoch 52/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1677 - acc: 0.9400\n",
            "Epoch 00052: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1653 - acc: 0.9453 - val_loss: 0.8358 - val_acc: 0.7812\n",
            "Epoch 53/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1736 - acc: 0.9350\n",
            "Epoch 00053: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1582 - acc: 0.9453 - val_loss: 0.7804 - val_acc: 0.7812\n",
            "Epoch 54/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1628 - acc: 0.9500\n",
            "Epoch 00054: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1588 - acc: 0.9531 - val_loss: 0.8118 - val_acc: 0.7969\n",
            "Epoch 55/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1476 - acc: 0.9450\n",
            "Epoch 00055: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1510 - acc: 0.9375 - val_loss: 0.7875 - val_acc: 0.7969\n",
            "Epoch 56/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1470 - acc: 0.9500\n",
            "Epoch 00056: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1328 - acc: 0.9531 - val_loss: 0.9879 - val_acc: 0.7812\n",
            "Epoch 57/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1356 - acc: 0.9450\n",
            "Epoch 00057: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1309 - acc: 0.9414 - val_loss: 0.9733 - val_acc: 0.7812\n",
            "Epoch 58/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1366 - acc: 0.9550\n",
            "Epoch 00058: val_acc improved from 0.79688 to 0.81250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-58-0.81250-0.94922.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.1499 - acc: 0.9492 - val_loss: 0.7778 - val_acc: 0.8125\n",
            "Epoch 59/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1495 - acc: 0.9450\n",
            "Epoch 00059: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1371 - acc: 0.9531 - val_loss: 1.1383 - val_acc: 0.7656\n",
            "Epoch 60/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0827 - acc: 0.9650\n",
            "Epoch 00060: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0809 - acc: 0.9688 - val_loss: 1.5087 - val_acc: 0.7500\n",
            "Epoch 61/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0931 - acc: 0.9750\n",
            "Epoch 00061: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1023 - acc: 0.9688 - val_loss: 1.3475 - val_acc: 0.7656\n",
            "Epoch 62/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1025 - acc: 0.9500\n",
            "Epoch 00062: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1165 - acc: 0.9492 - val_loss: 1.0444 - val_acc: 0.7969\n",
            "Epoch 63/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0996 - acc: 0.9750\n",
            "Epoch 00063: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0966 - acc: 0.9727 - val_loss: 1.0895 - val_acc: 0.7500\n",
            "Epoch 64/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1245 - acc: 0.9650\n",
            "Epoch 00064: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1191 - acc: 0.9570 - val_loss: 1.4390 - val_acc: 0.7500\n",
            "Epoch 65/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1361 - acc: 0.9550\n",
            "Epoch 00065: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1221 - acc: 0.9609 - val_loss: 1.7311 - val_acc: 0.7656\n",
            "Epoch 66/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1925 - acc: 0.9250\n",
            "Epoch 00066: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1661 - acc: 0.9297 - val_loss: 1.0652 - val_acc: 0.7500\n",
            "Epoch 67/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1031 - acc: 0.9600\n",
            "Epoch 00067: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1074 - acc: 0.9609 - val_loss: 0.8265 - val_acc: 0.7344\n",
            "Epoch 68/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1047 - acc: 0.9500\n",
            "Epoch 00068: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1069 - acc: 0.9531 - val_loss: 0.9926 - val_acc: 0.7500\n",
            "Epoch 69/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0815 - acc: 0.9650\n",
            "Epoch 00069: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0888 - acc: 0.9648 - val_loss: 1.1044 - val_acc: 0.7656\n",
            "Epoch 70/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1527 - acc: 0.9400\n",
            "Epoch 00070: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1399 - acc: 0.9453 - val_loss: 0.9009 - val_acc: 0.8125\n",
            "Epoch 71/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1092 - acc: 0.9550\n",
            "Epoch 00071: val_acc improved from 0.81250 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-71-0.82812-0.95312.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.1061 - acc: 0.9531 - val_loss: 0.7096 - val_acc: 0.8281\n",
            "Epoch 72/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1237 - acc: 0.9600\n",
            "Epoch 00072: val_acc improved from 0.82812 to 0.84375, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-72-0.84375-0.96484.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.1084 - acc: 0.9648 - val_loss: 0.7644 - val_acc: 0.8438\n",
            "Epoch 73/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0574 - acc: 0.9800\n",
            "Epoch 00073: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0671 - acc: 0.9805 - val_loss: 0.8923 - val_acc: 0.8281\n",
            "Epoch 74/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0650 - acc: 0.9800\n",
            "Epoch 00074: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0714 - acc: 0.9805 - val_loss: 1.0334 - val_acc: 0.7656\n",
            "Epoch 75/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0883 - acc: 0.9650\n",
            "Epoch 00075: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0786 - acc: 0.9688 - val_loss: 1.4549 - val_acc: 0.7500\n",
            "Epoch 76/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0557 - acc: 0.9850\n",
            "Epoch 00076: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0887 - acc: 0.9727 - val_loss: 1.3467 - val_acc: 0.7500\n",
            "Epoch 77/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1079 - acc: 0.9550\n",
            "Epoch 00077: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1025 - acc: 0.9570 - val_loss: 0.8742 - val_acc: 0.7344\n",
            "Epoch 78/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0818 - acc: 0.9750\n",
            "Epoch 00078: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0836 - acc: 0.9688 - val_loss: 0.7506 - val_acc: 0.7812\n",
            "Epoch 79/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0466 - acc: 0.9900\n",
            "Epoch 00079: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0438 - acc: 0.9922 - val_loss: 0.6706 - val_acc: 0.8281\n",
            "Epoch 80/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0465 - acc: 0.9900\n",
            "Epoch 00080: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0632 - acc: 0.9805 - val_loss: 0.7059 - val_acc: 0.8281\n",
            "Epoch 81/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0785 - acc: 0.9750\n",
            "Epoch 00081: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0791 - acc: 0.9727 - val_loss: 0.7670 - val_acc: 0.7656\n",
            "Epoch 82/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0870 - acc: 0.9550\n",
            "Epoch 00082: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1205 - acc: 0.9531 - val_loss: 0.9734 - val_acc: 0.7188\n",
            "Epoch 83/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1086 - acc: 0.9650\n",
            "Epoch 00083: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1039 - acc: 0.9609 - val_loss: 1.1073 - val_acc: 0.7656\n",
            "Epoch 84/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0681 - acc: 0.9800\n",
            "Epoch 00084: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0637 - acc: 0.9844 - val_loss: 0.8118 - val_acc: 0.7656\n",
            "Epoch 85/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0705 - acc: 0.9750\n",
            "Epoch 00085: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0840 - acc: 0.9766 - val_loss: 0.6788 - val_acc: 0.7812\n",
            "Epoch 86/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0866 - acc: 0.9550\n",
            "Epoch 00086: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0813 - acc: 0.9531 - val_loss: 0.6490 - val_acc: 0.7656\n",
            "Epoch 87/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0377 - acc: 0.9950\n",
            "Epoch 00087: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0445 - acc: 0.9883 - val_loss: 0.7713 - val_acc: 0.7812\n",
            "Epoch 88/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0686 - acc: 0.9650\n",
            "Epoch 00088: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0677 - acc: 0.9648 - val_loss: 1.0598 - val_acc: 0.7344\n",
            "Epoch 89/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0419 - acc: 0.9800\n",
            "Epoch 00089: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0672 - acc: 0.9688 - val_loss: 1.1301 - val_acc: 0.7812\n",
            "Epoch 90/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0463 - acc: 0.9900\n",
            "Epoch 00090: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0686 - acc: 0.9844 - val_loss: 0.8019 - val_acc: 0.8281\n",
            "Epoch 91/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0537 - acc: 0.9850\n",
            "Epoch 00091: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0845 - acc: 0.9727 - val_loss: 0.6455 - val_acc: 0.7969\n",
            "Epoch 92/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0976 - acc: 0.9600\n",
            "Epoch 00092: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0874 - acc: 0.9688 - val_loss: 0.6465 - val_acc: 0.8438\n",
            "Epoch 93/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0574 - acc: 0.9750\n",
            "Epoch 00093: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0525 - acc: 0.9805 - val_loss: 0.8108 - val_acc: 0.8438\n",
            "Epoch 94/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0770 - acc: 0.9850\n",
            "Epoch 00094: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0801 - acc: 0.9766 - val_loss: 0.8316 - val_acc: 0.8281\n",
            "Epoch 95/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0739 - acc: 0.9600\n",
            "Epoch 00095: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0605 - acc: 0.9688 - val_loss: 0.7046 - val_acc: 0.7969\n",
            "Epoch 96/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0491 - acc: 0.9800\n",
            "Epoch 00096: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0745 - acc: 0.9727 - val_loss: 0.6625 - val_acc: 0.8438\n",
            "Epoch 97/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0647 - acc: 0.9750\n",
            "Epoch 00097: val_acc improved from 0.84375 to 0.85938, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-97-0.85938-0.97656.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.0575 - acc: 0.9766 - val_loss: 0.7405 - val_acc: 0.8594\n",
            "Epoch 98/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0956 - acc: 0.9550\n",
            "Epoch 00098: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1139 - acc: 0.9570 - val_loss: 0.6382 - val_acc: 0.8438\n",
            "Epoch 99/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0806 - acc: 0.9650\n",
            "Epoch 00099: val_acc improved from 0.85938 to 0.87500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-99-0.87500-0.96484.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.0837 - acc: 0.9648 - val_loss: 0.5460 - val_acc: 0.8750\n",
            "Epoch 100/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0905 - acc: 0.9600\n",
            "Epoch 00100: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0815 - acc: 0.9688 - val_loss: 0.5184 - val_acc: 0.8281\n",
            "Epoch 101/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0811 - acc: 0.9700\n",
            "Epoch 00101: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0760 - acc: 0.9688 - val_loss: 0.5167 - val_acc: 0.8125\n",
            "Epoch 102/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0594 - acc: 0.9800\n",
            "Epoch 00102: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0737 - acc: 0.9727 - val_loss: 0.5455 - val_acc: 0.8594\n",
            "Epoch 103/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0935 - acc: 0.9650\n",
            "Epoch 00103: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0758 - acc: 0.9727 - val_loss: 0.6476 - val_acc: 0.8594\n",
            "Epoch 104/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1030 - acc: 0.9700\n",
            "Epoch 00104: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1080 - acc: 0.9648 - val_loss: 0.6754 - val_acc: 0.8594\n",
            "Epoch 105/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1140 - acc: 0.9650\n",
            "Epoch 00105: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0974 - acc: 0.9727 - val_loss: 0.5592 - val_acc: 0.8438\n",
            "Epoch 106/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0502 - acc: 0.9750\n",
            "Epoch 00106: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0674 - acc: 0.9727 - val_loss: 0.5651 - val_acc: 0.8438\n",
            "Epoch 107/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0852 - acc: 0.9650\n",
            "Epoch 00107: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0729 - acc: 0.9688 - val_loss: 0.5877 - val_acc: 0.8594\n",
            "Epoch 108/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0561 - acc: 0.9750\n",
            "Epoch 00108: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0811 - acc: 0.9688 - val_loss: 0.5880 - val_acc: 0.8750\n",
            "Epoch 109/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0529 - acc: 0.9850\n",
            "Epoch 00109: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0622 - acc: 0.9805 - val_loss: 0.4940 - val_acc: 0.8594\n",
            "Epoch 110/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0377 - acc: 0.9800\n",
            "Epoch 00110: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0312 - acc: 0.9844 - val_loss: 0.4935 - val_acc: 0.8594\n",
            "Epoch 111/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0462 - acc: 0.9850\n",
            "Epoch 00111: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0427 - acc: 0.9844 - val_loss: 0.5194 - val_acc: 0.8438\n",
            "Epoch 112/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0677 - acc: 0.9800\n",
            "Epoch 00112: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0643 - acc: 0.9805 - val_loss: 0.5678 - val_acc: 0.8281\n",
            "Epoch 113/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0632 - acc: 0.9650\n",
            "Epoch 00113: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0699 - acc: 0.9648 - val_loss: 0.6288 - val_acc: 0.8594\n",
            "Epoch 114/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0488 - acc: 0.9750\n",
            "Epoch 00114: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0466 - acc: 0.9805 - val_loss: 0.5834 - val_acc: 0.8594\n",
            "Epoch 115/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0562 - acc: 0.9800\n",
            "Epoch 00115: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0488 - acc: 0.9844 - val_loss: 0.5553 - val_acc: 0.8750\n",
            "Epoch 116/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0851 - acc: 0.9750\n",
            "Epoch 00116: val_acc improved from 0.87500 to 0.89062, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-116-0.89062-0.97266.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.0825 - acc: 0.9727 - val_loss: 0.5367 - val_acc: 0.8906\n",
            "Epoch 117/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0308 - acc: 0.9900\n",
            "Epoch 00117: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0357 - acc: 0.9883 - val_loss: 0.5569 - val_acc: 0.8125\n",
            "Epoch 118/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0321 - acc: 0.9850\n",
            "Epoch 00118: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0476 - acc: 0.9844 - val_loss: 0.6123 - val_acc: 0.8438\n",
            "Epoch 119/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0306 - acc: 0.9950\n",
            "Epoch 00119: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0436 - acc: 0.9922 - val_loss: 0.5217 - val_acc: 0.8438\n",
            "Epoch 120/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0564 - acc: 0.9800\n",
            "Epoch 00120: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0649 - acc: 0.9805 - val_loss: 0.5736 - val_acc: 0.8125\n",
            "Epoch 121/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0534 - acc: 0.9850\n",
            "Epoch 00121: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0919 - acc: 0.9766 - val_loss: 0.6304 - val_acc: 0.8125\n",
            "Epoch 122/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0523 - acc: 0.9800\n",
            "Epoch 00122: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0477 - acc: 0.9844 - val_loss: 0.5924 - val_acc: 0.7969\n",
            "Epoch 123/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0813 - acc: 0.9650\n",
            "Epoch 00123: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0892 - acc: 0.9570 - val_loss: 0.6495 - val_acc: 0.8281\n",
            "Epoch 124/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0509 - acc: 0.9800\n",
            "Epoch 00124: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0414 - acc: 0.9844 - val_loss: 0.6293 - val_acc: 0.8438\n",
            "Epoch 125/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0576 - acc: 0.9850\n",
            "Epoch 00125: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0620 - acc: 0.9805 - val_loss: 0.6506 - val_acc: 0.8281\n",
            "Epoch 126/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0380 - acc: 0.9800\n",
            "Epoch 00126: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0470 - acc: 0.9727 - val_loss: 0.6391 - val_acc: 0.8281\n",
            "Epoch 127/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0507 - acc: 0.9750\n",
            "Epoch 00127: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0606 - acc: 0.9727 - val_loss: 0.5466 - val_acc: 0.8438\n",
            "Epoch 128/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0437 - acc: 0.9900\n",
            "Epoch 00128: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0451 - acc: 0.9883 - val_loss: 0.4694 - val_acc: 0.8438\n",
            "Epoch 129/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0449 - acc: 0.9800\n",
            "Epoch 00129: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0379 - acc: 0.9844 - val_loss: 0.4869 - val_acc: 0.8594\n",
            "Epoch 130/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0338 - acc: 0.9900\n",
            "Epoch 00130: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0371 - acc: 0.9883 - val_loss: 0.5190 - val_acc: 0.8594\n",
            "Epoch 131/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0541 - acc: 0.9850\n",
            "Epoch 00131: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0529 - acc: 0.9844 - val_loss: 0.5019 - val_acc: 0.8594\n",
            "Epoch 132/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0811 - acc: 0.9650\n",
            "Epoch 00132: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0761 - acc: 0.9688 - val_loss: 0.5468 - val_acc: 0.8281\n",
            "Epoch 133/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0382 - acc: 0.9800\n",
            "Epoch 00133: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0584 - acc: 0.9727 - val_loss: 0.5111 - val_acc: 0.8438\n",
            "Epoch 134/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0227 - acc: 0.9850\n",
            "Epoch 00134: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0245 - acc: 0.9844 - val_loss: 0.4364 - val_acc: 0.8750\n",
            "Epoch 135/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0294 - acc: 0.9950\n",
            "Epoch 00135: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0295 - acc: 0.9922 - val_loss: 0.3954 - val_acc: 0.8906\n",
            "Epoch 136/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0844 - acc: 0.9650\n",
            "Epoch 00136: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0688 - acc: 0.9727 - val_loss: 0.3193 - val_acc: 0.8906\n",
            "Epoch 137/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0393 - acc: 0.9750\n",
            "Epoch 00137: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0375 - acc: 0.9805 - val_loss: 0.2994 - val_acc: 0.8906\n",
            "Epoch 138/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0436 - acc: 0.9800\n",
            "Epoch 00138: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0464 - acc: 0.9805 - val_loss: 0.3486 - val_acc: 0.8750\n",
            "Epoch 139/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0177 - acc: 0.9950\n",
            "Epoch 00139: val_acc improved from 0.89062 to 0.90625, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-139-0.90625-0.98438.hdf5\n",
            "256/256 [==============================] - 5s 21ms/sample - loss: 0.0458 - acc: 0.9844 - val_loss: 0.3738 - val_acc: 0.9062\n",
            "Epoch 140/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0301 - acc: 0.9900\n",
            "Epoch 00140: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0247 - acc: 0.9922 - val_loss: 0.3827 - val_acc: 0.9062\n",
            "Epoch 141/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0392 - acc: 0.9850\n",
            "Epoch 00141: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0455 - acc: 0.9766 - val_loss: 0.4199 - val_acc: 0.9062\n",
            "Epoch 142/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0343 - acc: 0.9850\n",
            "Epoch 00142: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0331 - acc: 0.9844 - val_loss: 0.4336 - val_acc: 0.8906\n",
            "Epoch 143/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0186 - acc: 0.9950\n",
            "Epoch 00143: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0162 - acc: 0.9961 - val_loss: 0.5131 - val_acc: 0.8750\n",
            "Epoch 144/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0667 - acc: 0.9850\n",
            "Epoch 00144: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0539 - acc: 0.9883 - val_loss: 0.5031 - val_acc: 0.8750\n",
            "Epoch 145/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0347 - acc: 0.9850\n",
            "Epoch 00145: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0431 - acc: 0.9844 - val_loss: 0.4743 - val_acc: 0.8906\n",
            "Epoch 146/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0335 - acc: 0.9850\n",
            "Epoch 00146: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0493 - acc: 0.9805 - val_loss: 0.4262 - val_acc: 0.8750\n",
            "Epoch 147/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0125 - acc: 1.0000\n",
            "Epoch 00147: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0166 - acc: 1.0000 - val_loss: 0.5211 - val_acc: 0.8906\n",
            "Epoch 148/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0224 - acc: 0.9900\n",
            "Epoch 00148: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0188 - acc: 0.9922 - val_loss: 0.6313 - val_acc: 0.9062\n",
            "Epoch 149/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0121 - acc: 1.0000\n",
            "Epoch 00149: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0300 - acc: 0.9922 - val_loss: 0.5634 - val_acc: 0.8438\n",
            "Epoch 150/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0524 - acc: 0.9800\n",
            "Epoch 00150: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0677 - acc: 0.9766 - val_loss: 0.6351 - val_acc: 0.8438\n",
            "Epoch 151/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0530 - acc: 0.9900\n",
            "Epoch 00151: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0542 - acc: 0.9844 - val_loss: 0.6894 - val_acc: 0.8125\n",
            "Epoch 152/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.1216 - acc: 0.9800\n",
            "Epoch 00152: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.1122 - acc: 0.9805 - val_loss: 0.5895 - val_acc: 0.8281\n",
            "Epoch 153/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0559 - acc: 0.9800\n",
            "Epoch 00153: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0551 - acc: 0.9805 - val_loss: 0.6279 - val_acc: 0.8281\n",
            "Epoch 154/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0507 - acc: 0.9900\n",
            "Epoch 00154: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0456 - acc: 0.9922 - val_loss: 0.6105 - val_acc: 0.7812\n",
            "Epoch 155/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0581 - acc: 0.9850\n",
            "Epoch 00155: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0548 - acc: 0.9883 - val_loss: 0.6420 - val_acc: 0.7656\n",
            "Epoch 156/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0853 - acc: 0.9800\n",
            "Epoch 00156: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0717 - acc: 0.9844 - val_loss: 0.7173 - val_acc: 0.7812\n",
            "Epoch 157/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0293 - acc: 0.9900\n",
            "Epoch 00157: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0394 - acc: 0.9844 - val_loss: 0.7914 - val_acc: 0.8125\n",
            "Epoch 158/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0474 - acc: 0.9800\n",
            "Epoch 00158: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0386 - acc: 0.9844 - val_loss: 0.8157 - val_acc: 0.7969\n",
            "Epoch 159/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0180 - acc: 0.9900\n",
            "Epoch 00159: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0289 - acc: 0.9844 - val_loss: 0.7213 - val_acc: 0.8125\n",
            "Epoch 160/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0287 - acc: 0.9850\n",
            "Epoch 00160: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0230 - acc: 0.9883 - val_loss: 0.6173 - val_acc: 0.8438\n",
            "Epoch 161/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0312 - acc: 0.9950\n",
            "Epoch 00161: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0289 - acc: 0.9961 - val_loss: 0.7102 - val_acc: 0.8438\n",
            "Epoch 162/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0386 - acc: 0.9850\n",
            "Epoch 00162: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0355 - acc: 0.9844 - val_loss: 0.8450 - val_acc: 0.8438\n",
            "Epoch 163/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0197 - acc: 0.9900\n",
            "Epoch 00163: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0386 - acc: 0.9805 - val_loss: 0.8895 - val_acc: 0.8281\n",
            "Epoch 164/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0490 - acc: 0.9850\n",
            "Epoch 00164: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0447 - acc: 0.9844 - val_loss: 0.7887 - val_acc: 0.8438\n",
            "Epoch 165/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0294 - acc: 0.9950\n",
            "Epoch 00165: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0295 - acc: 0.9961 - val_loss: 0.7059 - val_acc: 0.8750\n",
            "Epoch 166/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0292 - acc: 0.9850\n",
            "Epoch 00166: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0263 - acc: 0.9844 - val_loss: 0.6736 - val_acc: 0.8438\n",
            "Epoch 167/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0493 - acc: 0.9800\n",
            "Epoch 00167: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0399 - acc: 0.9844 - val_loss: 0.6736 - val_acc: 0.8438\n",
            "Epoch 168/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0227 - acc: 0.9850\n",
            "Epoch 00168: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0201 - acc: 0.9883 - val_loss: 0.6714 - val_acc: 0.8594\n",
            "Epoch 169/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0279 - acc: 0.9850\n",
            "Epoch 00169: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0234 - acc: 0.9883 - val_loss: 0.6777 - val_acc: 0.8594\n",
            "Epoch 170/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0386 - acc: 0.9900\n",
            "Epoch 00170: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0340 - acc: 0.9922 - val_loss: 0.7204 - val_acc: 0.8438\n",
            "Epoch 171/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0242 - acc: 0.9950\n",
            "Epoch 00171: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0326 - acc: 0.9922 - val_loss: 0.6849 - val_acc: 0.8438\n",
            "Epoch 172/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0152 - acc: 0.9950\n",
            "Epoch 00172: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0126 - acc: 0.9961 - val_loss: 0.6123 - val_acc: 0.8438\n",
            "Epoch 173/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0104 - acc: 0.9950\n",
            "Epoch 00173: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0170 - acc: 0.9883 - val_loss: 0.6102 - val_acc: 0.8438\n",
            "Epoch 174/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0074 - acc: 1.0000\n",
            "Epoch 00174: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0095 - acc: 1.0000 - val_loss: 0.6696 - val_acc: 0.8594\n",
            "Epoch 175/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0158 - acc: 0.9950\n",
            "Epoch 00175: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0141 - acc: 0.9961 - val_loss: 0.7686 - val_acc: 0.8438\n",
            "Epoch 176/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0244 - acc: 0.9900\n",
            "Epoch 00176: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0238 - acc: 0.9922 - val_loss: 0.8886 - val_acc: 0.8438\n",
            "Epoch 177/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0803 - acc: 0.9850\n",
            "Epoch 00177: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0723 - acc: 0.9844 - val_loss: 0.9144 - val_acc: 0.8438\n",
            "Epoch 178/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0122 - acc: 0.9950\n",
            "Epoch 00178: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0181 - acc: 0.9922 - val_loss: 0.8854 - val_acc: 0.8438\n",
            "Epoch 179/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0222 - acc: 0.9900\n",
            "Epoch 00179: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0361 - acc: 0.9844 - val_loss: 0.7390 - val_acc: 0.8438\n",
            "Epoch 180/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0265 - acc: 0.9900\n",
            "Epoch 00180: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0418 - acc: 0.9883 - val_loss: 0.6230 - val_acc: 0.8438\n",
            "Epoch 181/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0380 - acc: 0.9900\n",
            "Epoch 00181: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0335 - acc: 0.9922 - val_loss: 0.5274 - val_acc: 0.8281\n",
            "Epoch 182/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0186 - acc: 0.9950\n",
            "Epoch 00182: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0220 - acc: 0.9922 - val_loss: 0.5567 - val_acc: 0.8594\n",
            "Epoch 183/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0396 - acc: 0.9800\n",
            "Epoch 00183: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0349 - acc: 0.9805 - val_loss: 0.6863 - val_acc: 0.8594\n",
            "Epoch 184/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0149 - acc: 0.9950\n",
            "Epoch 00184: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0141 - acc: 0.9961 - val_loss: 0.7983 - val_acc: 0.8281\n",
            "Epoch 185/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0235 - acc: 0.9900\n",
            "Epoch 00185: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0195 - acc: 0.9922 - val_loss: 0.6834 - val_acc: 0.8594\n",
            "Epoch 186/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0481 - acc: 0.9750\n",
            "Epoch 00186: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0392 - acc: 0.9805 - val_loss: 0.5199 - val_acc: 0.8750\n",
            "Epoch 187/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0141 - acc: 0.9950\n",
            "Epoch 00187: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0129 - acc: 0.9961 - val_loss: 0.5311 - val_acc: 0.8750\n",
            "Epoch 188/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0212 - acc: 0.9850\n",
            "Epoch 00188: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0183 - acc: 0.9883 - val_loss: 0.5534 - val_acc: 0.9062\n",
            "Epoch 189/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0485 - acc: 0.9800\n",
            "Epoch 00189: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0815 - acc: 0.9766 - val_loss: 0.4384 - val_acc: 0.8750\n",
            "Epoch 190/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0430 - acc: 0.9750\n",
            "Epoch 00190: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0631 - acc: 0.9688 - val_loss: 0.5998 - val_acc: 0.8438\n",
            "Epoch 191/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0193 - acc: 0.9950\n",
            "Epoch 00191: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0203 - acc: 0.9961 - val_loss: 1.0179 - val_acc: 0.7969\n",
            "Epoch 192/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0531 - acc: 0.9750\n",
            "Epoch 00192: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0487 - acc: 0.9805 - val_loss: 0.9869 - val_acc: 0.8125\n",
            "Epoch 193/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0839 - acc: 0.9850\n",
            "Epoch 00193: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0686 - acc: 0.9883 - val_loss: 0.6301 - val_acc: 0.8750\n",
            "Epoch 194/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0327 - acc: 0.9900\n",
            "Epoch 00194: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0283 - acc: 0.9922 - val_loss: 0.6363 - val_acc: 0.8438\n",
            "Epoch 195/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0220 - acc: 0.9950\n",
            "Epoch 00195: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0259 - acc: 0.9922 - val_loss: 0.5956 - val_acc: 0.8438\n",
            "Epoch 196/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0275 - acc: 0.9900\n",
            "Epoch 00196: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0296 - acc: 0.9883 - val_loss: 0.6044 - val_acc: 0.8594\n",
            "Epoch 197/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0164 - acc: 0.9950\n",
            "Epoch 00197: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0160 - acc: 0.9961 - val_loss: 0.6339 - val_acc: 0.8594\n",
            "Epoch 198/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0189 - acc: 0.9950\n",
            "Epoch 00198: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0247 - acc: 0.9922 - val_loss: 0.7155 - val_acc: 0.8750\n",
            "Epoch 199/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0373 - acc: 0.9800\n",
            "Epoch 00199: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0311 - acc: 0.9844 - val_loss: 0.7300 - val_acc: 0.8594\n",
            "Epoch 200/200\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 0.0096 - acc: 0.9950\n",
            "Epoch 00200: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 5s 19ms/sample - loss: 0.0079 - acc: 0.9961 - val_loss: 0.7106 - val_acc: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.99609375,validation accuracy: 0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ft1P510iktA",
        "colab_type": "code",
        "outputId": "cfae60fe-de3f-432d-a4ee-e229ca96d772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3sz7QZ6w791",
        "colab_type": "code",
        "outputId": "99689caa-75aa-4389-827b-2f382930eaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 06 \n",
        "# increased neurons in convolutional layers (64 -> 128)\n",
        "# increased sequential layer (512 -> 1024)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(128, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(128, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('06'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 1s - loss: 4.0998 - acc: 0.5950\n",
            "Epoch 00001: val_acc improved from -inf to 0.32812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.32812-0.56250.hdf5\n",
            "256/256 [==============================] - 12s 46ms/sample - loss: 6.4293 - acc: 0.5625 - val_loss: 0.8521 - val_acc: 0.3281\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.7435 - acc: 0.4000\n",
            "Epoch 00002: val_acc improved from 0.32812 to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-02-0.67188-0.41406.hdf5\n",
            "256/256 [==============================] - 2s 6ms/sample - loss: 3.0811 - acc: 0.4141 - val_loss: 0.6880 - val_acc: 0.6719\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6783 - acc: 0.5900\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6777 - acc: 0.5859 - val_loss: 0.6766 - val_acc: 0.6719\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6761 - acc: 0.5850\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6678 - acc: 0.6016 - val_loss: 0.6428 - val_acc: 0.6719\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6566 - acc: 0.6150\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6600 - acc: 0.6016 - val_loss: 0.6252 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6859 - acc: 0.5900\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6741 - acc: 0.6094 - val_loss: 0.6568 - val_acc: 0.6719\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6484 - acc: 0.6150\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6490 - acc: 0.5977 - val_loss: 0.6321 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6197 - acc: 0.6150\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6129 - acc: 0.6484 - val_loss: 0.6138 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6001 - acc: 0.6550\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5825 - acc: 0.6680 - val_loss: 0.6102 - val_acc: 0.6719\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5693 - acc: 0.7400\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5970 - acc: 0.6992 - val_loss: 0.6061 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5453 - acc: 0.6800\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5504 - acc: 0.6836 - val_loss: 0.5983 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4971 - acc: 0.7400\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5221 - acc: 0.6953 - val_loss: 0.5939 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5724 - acc: 0.6900\n",
            "Epoch 00013: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5567 - acc: 0.7070 - val_loss: 0.5993 - val_acc: 0.6719\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5303 - acc: 0.7200\n",
            "Epoch 00014: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5118 - acc: 0.7305 - val_loss: 0.5961 - val_acc: 0.6719\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4973 - acc: 0.7700\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5150 - acc: 0.7422 - val_loss: 0.5889 - val_acc: 0.6719\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4827 - acc: 0.7850\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4847 - acc: 0.7617 - val_loss: 0.5772 - val_acc: 0.6719\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5009 - acc: 0.7350\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4827 - acc: 0.7461 - val_loss: 0.5753 - val_acc: 0.6719\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4440 - acc: 0.7700\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4369 - acc: 0.7734 - val_loss: 0.5963 - val_acc: 0.6719\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4002 - acc: 0.8000\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4109 - acc: 0.7930 - val_loss: 0.6089 - val_acc: 0.6719\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3760 - acc: 0.7900\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3955 - acc: 0.7734 - val_loss: 0.6247 - val_acc: 0.6719\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4025 - acc: 0.8550\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3991 - acc: 0.8477 - val_loss: 0.6178 - val_acc: 0.6719\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4302 - acc: 0.7700\n",
            "Epoch 00022: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-22-0.68750-0.79688.hdf5\n",
            "256/256 [==============================] - 2s 6ms/sample - loss: 0.4021 - acc: 0.7969 - val_loss: 0.6029 - val_acc: 0.6875\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3286 - acc: 0.8700\n",
            "Epoch 00023: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3429 - acc: 0.8594 - val_loss: 0.5807 - val_acc: 0.6875\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3377 - acc: 0.8650\n",
            "Epoch 00024: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3775 - acc: 0.8398 - val_loss: 0.6001 - val_acc: 0.6875\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3237 - acc: 0.8450\n",
            "Epoch 00025: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3413 - acc: 0.8398 - val_loss: 0.6834 - val_acc: 0.6094\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3780 - acc: 0.8350\n",
            "Epoch 00026: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3812 - acc: 0.8242 - val_loss: 0.6143 - val_acc: 0.6875\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3594 - acc: 0.8500\n",
            "Epoch 00027: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4060 - acc: 0.8281 - val_loss: 0.6921 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4028 - acc: 0.8150\n",
            "Epoch 00028: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3846 - acc: 0.8398 - val_loss: 0.6238 - val_acc: 0.6875\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3618 - acc: 0.8400\n",
            "Epoch 00029: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3702 - acc: 0.8438 - val_loss: 0.8464 - val_acc: 0.6719\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3357 - acc: 0.8450\n",
            "Epoch 00030: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3630 - acc: 0.8477 - val_loss: 0.6918 - val_acc: 0.6875\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3020 - acc: 0.8500\n",
            "Epoch 00031: val_acc improved from 0.68750 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-31-0.73438-0.84375.hdf5\n",
            "256/256 [==============================] - 2s 6ms/sample - loss: 0.3225 - acc: 0.8438 - val_loss: 0.5555 - val_acc: 0.7344\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3996 - acc: 0.8100\n",
            "Epoch 00032: val_acc improved from 0.73438 to 0.76562, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-32-0.76562-0.82422.hdf5\n",
            "256/256 [==============================] - 2s 6ms/sample - loss: 0.3804 - acc: 0.8242 - val_loss: 0.5482 - val_acc: 0.7656\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3357 - acc: 0.8650\n",
            "Epoch 00033: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3333 - acc: 0.8594 - val_loss: 0.6822 - val_acc: 0.6875\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2646 - acc: 0.9050\n",
            "Epoch 00034: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2401 - acc: 0.9102 - val_loss: 0.7311 - val_acc: 0.6875\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3843 - acc: 0.8600\n",
            "Epoch 00035: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4030 - acc: 0.8477 - val_loss: 0.6348 - val_acc: 0.6875\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3544 - acc: 0.8350\n",
            "Epoch 00036: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3881 - acc: 0.8125 - val_loss: 0.5580 - val_acc: 0.7344\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3068 - acc: 0.8850\n",
            "Epoch 00037: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3130 - acc: 0.8789 - val_loss: 0.6715 - val_acc: 0.6875\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2784 - acc: 0.9000\n",
            "Epoch 00038: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3135 - acc: 0.8867 - val_loss: 0.7095 - val_acc: 0.6875\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2586 - acc: 0.9000\n",
            "Epoch 00039: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2724 - acc: 0.8984 - val_loss: 0.5101 - val_acc: 0.7188\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2873 - acc: 0.8950\n",
            "Epoch 00040: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2970 - acc: 0.8867 - val_loss: 0.5331 - val_acc: 0.6875\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2590 - acc: 0.9000\n",
            "Epoch 00041: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2581 - acc: 0.8984 - val_loss: 0.6937 - val_acc: 0.6875\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2534 - acc: 0.8950\n",
            "Epoch 00042: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2574 - acc: 0.8945 - val_loss: 0.7892 - val_acc: 0.6875\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2318 - acc: 0.9100\n",
            "Epoch 00043: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2444 - acc: 0.9062 - val_loss: 0.6626 - val_acc: 0.6875\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2274 - acc: 0.8900\n",
            "Epoch 00044: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2283 - acc: 0.8945 - val_loss: 0.6468 - val_acc: 0.6875\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2309 - acc: 0.9050\n",
            "Epoch 00045: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2449 - acc: 0.9023 - val_loss: 0.6092 - val_acc: 0.6875\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1843 - acc: 0.9150\n",
            "Epoch 00046: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1820 - acc: 0.9219 - val_loss: 0.6265 - val_acc: 0.6875\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2171 - acc: 0.9000\n",
            "Epoch 00047: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2109 - acc: 0.9062 - val_loss: 0.7277 - val_acc: 0.6875\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2520 - acc: 0.8850\n",
            "Epoch 00048: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2432 - acc: 0.8867 - val_loss: 0.5750 - val_acc: 0.6875\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2061 - acc: 0.9250\n",
            "Epoch 00049: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1933 - acc: 0.9336 - val_loss: 0.5440 - val_acc: 0.6875\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1951 - acc: 0.9200\n",
            "Epoch 00050: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1949 - acc: 0.9219 - val_loss: 0.7956 - val_acc: 0.6875\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2243 - acc: 0.9050\n",
            "Epoch 00051: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2122 - acc: 0.9102 - val_loss: 1.1855 - val_acc: 0.6875\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1888 - acc: 0.9200\n",
            "Epoch 00052: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1846 - acc: 0.9219 - val_loss: 0.9407 - val_acc: 0.6875\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2246 - acc: 0.8950\n",
            "Epoch 00053: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2006 - acc: 0.9102 - val_loss: 0.6225 - val_acc: 0.6875\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2082 - acc: 0.9100\n",
            "Epoch 00054: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1904 - acc: 0.9180 - val_loss: 0.9980 - val_acc: 0.6875\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1364 - acc: 0.9550\n",
            "Epoch 00055: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1439 - acc: 0.9453 - val_loss: 1.2039 - val_acc: 0.6875\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2759 - acc: 0.8900\n",
            "Epoch 00056: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2578 - acc: 0.8945 - val_loss: 0.9219 - val_acc: 0.6875\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2184 - acc: 0.9250\n",
            "Epoch 00057: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2231 - acc: 0.9219 - val_loss: 0.7664 - val_acc: 0.6875\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1663 - acc: 0.9550\n",
            "Epoch 00058: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1625 - acc: 0.9531 - val_loss: 0.8788 - val_acc: 0.6875\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1739 - acc: 0.9100\n",
            "Epoch 00059: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1761 - acc: 0.9141 - val_loss: 0.8722 - val_acc: 0.6875\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1745 - acc: 0.9250\n",
            "Epoch 00060: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1782 - acc: 0.9219 - val_loss: 0.9032 - val_acc: 0.6875\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1678 - acc: 0.9250\n",
            "Epoch 00061: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1721 - acc: 0.9258 - val_loss: 0.9365 - val_acc: 0.6875\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1303 - acc: 0.9500\n",
            "Epoch 00062: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1552 - acc: 0.9375 - val_loss: 1.1874 - val_acc: 0.6875\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1578 - acc: 0.9500\n",
            "Epoch 00063: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1594 - acc: 0.9414 - val_loss: 0.9246 - val_acc: 0.6875\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1427 - acc: 0.9350\n",
            "Epoch 00064: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1383 - acc: 0.9375 - val_loss: 0.7000 - val_acc: 0.7031\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1182 - acc: 0.9650\n",
            "Epoch 00065: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1155 - acc: 0.9688 - val_loss: 0.7800 - val_acc: 0.7031\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1813 - acc: 0.9200\n",
            "Epoch 00066: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1750 - acc: 0.9258 - val_loss: 1.0381 - val_acc: 0.6875\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1314 - acc: 0.9500\n",
            "Epoch 00067: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1359 - acc: 0.9414 - val_loss: 0.9690 - val_acc: 0.6875\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1398 - acc: 0.9600\n",
            "Epoch 00068: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1234 - acc: 0.9648 - val_loss: 1.1086 - val_acc: 0.6875\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1137 - acc: 0.9650\n",
            "Epoch 00069: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1180 - acc: 0.9609 - val_loss: 1.5265 - val_acc: 0.6875\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1355 - acc: 0.9350\n",
            "Epoch 00070: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1218 - acc: 0.9453 - val_loss: 1.2772 - val_acc: 0.7031\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1577 - acc: 0.9250\n",
            "Epoch 00071: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1424 - acc: 0.9336 - val_loss: 1.3368 - val_acc: 0.7031\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1404 - acc: 0.9400\n",
            "Epoch 00072: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1337 - acc: 0.9492 - val_loss: 1.5612 - val_acc: 0.6875\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1253 - acc: 0.9600\n",
            "Epoch 00073: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1366 - acc: 0.9531 - val_loss: 1.4968 - val_acc: 0.6875\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1466 - acc: 0.9500\n",
            "Epoch 00074: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1400 - acc: 0.9531 - val_loss: 1.2822 - val_acc: 0.6875\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0931 - acc: 0.9700\n",
            "Epoch 00075: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1232 - acc: 0.9570 - val_loss: 0.9299 - val_acc: 0.7188\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1402 - acc: 0.9600\n",
            "Epoch 00076: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1394 - acc: 0.9570 - val_loss: 1.1914 - val_acc: 0.6875\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1507 - acc: 0.9200\n",
            "Epoch 00077: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1397 - acc: 0.9297 - val_loss: 0.8237 - val_acc: 0.7656\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1031 - acc: 0.9600\n",
            "Epoch 00078: val_acc improved from 0.76562 to 0.81250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-78-0.81250-0.96094.hdf5\n",
            "256/256 [==============================] - 2s 6ms/sample - loss: 0.1017 - acc: 0.9609 - val_loss: 0.8246 - val_acc: 0.8125\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1383 - acc: 0.9350\n",
            "Epoch 00079: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1414 - acc: 0.9414 - val_loss: 0.7584 - val_acc: 0.7969\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1135 - acc: 0.9550\n",
            "Epoch 00080: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1189 - acc: 0.9570 - val_loss: 0.7182 - val_acc: 0.7656\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1282 - acc: 0.9450\n",
            "Epoch 00081: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1320 - acc: 0.9492 - val_loss: 0.7302 - val_acc: 0.7188\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0649 - acc: 0.9950\n",
            "Epoch 00082: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0912 - acc: 0.9844 - val_loss: 0.7171 - val_acc: 0.7344\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1349 - acc: 0.9550\n",
            "Epoch 00083: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1256 - acc: 0.9570 - val_loss: 0.5646 - val_acc: 0.7812\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1299 - acc: 0.9500\n",
            "Epoch 00084: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1356 - acc: 0.9531 - val_loss: 0.6983 - val_acc: 0.7500\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0943 - acc: 0.9700\n",
            "Epoch 00085: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0976 - acc: 0.9727 - val_loss: 0.5041 - val_acc: 0.7500\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1427 - acc: 0.9450\n",
            "Epoch 00086: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1194 - acc: 0.9570 - val_loss: 0.5200 - val_acc: 0.7812\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1431 - acc: 0.9300\n",
            "Epoch 00087: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1409 - acc: 0.9297 - val_loss: 0.5946 - val_acc: 0.7812\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0953 - acc: 0.9650\n",
            "Epoch 00088: val_acc improved from 0.81250 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-88-0.82812-0.95703.hdf5\n",
            "256/256 [==============================] - 1s 6ms/sample - loss: 0.0960 - acc: 0.9570 - val_loss: 0.4867 - val_acc: 0.8281\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1123 - acc: 0.9550\n",
            "Epoch 00089: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1269 - acc: 0.9492 - val_loss: 0.4670 - val_acc: 0.8281\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0947 - acc: 0.9650\n",
            "Epoch 00090: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0902 - acc: 0.9688 - val_loss: 0.3675 - val_acc: 0.8281\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0642 - acc: 0.9750\n",
            "Epoch 00091: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0718 - acc: 0.9766 - val_loss: 0.3611 - val_acc: 0.8125\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0513 - acc: 0.9950\n",
            "Epoch 00092: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0485 - acc: 0.9922 - val_loss: 0.3559 - val_acc: 0.8281\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0800 - acc: 0.9650\n",
            "Epoch 00093: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0837 - acc: 0.9648 - val_loss: 0.4165 - val_acc: 0.7969\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1138 - acc: 0.9800\n",
            "Epoch 00094: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0998 - acc: 0.9805 - val_loss: 0.4083 - val_acc: 0.7969\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0569 - acc: 0.9750\n",
            "Epoch 00095: val_acc improved from 0.82812 to 0.85938, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-95-0.85938-0.97266.hdf5\n",
            "256/256 [==============================] - 1s 6ms/sample - loss: 0.0641 - acc: 0.9727 - val_loss: 0.4049 - val_acc: 0.8594\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0808 - acc: 0.9650\n",
            "Epoch 00096: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0774 - acc: 0.9648 - val_loss: 0.6542 - val_acc: 0.8125\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1051 - acc: 0.9700\n",
            "Epoch 00097: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0977 - acc: 0.9688 - val_loss: 0.6611 - val_acc: 0.7969\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0947 - acc: 0.9700\n",
            "Epoch 00098: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1392 - acc: 0.9453 - val_loss: 0.5371 - val_acc: 0.7812\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0802 - acc: 0.9700\n",
            "Epoch 00099: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1145 - acc: 0.9648 - val_loss: 0.5833 - val_acc: 0.7969\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1076 - acc: 0.9700\n",
            "Epoch 00100: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1041 - acc: 0.9688 - val_loss: 0.3889 - val_acc: 0.8438\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1126 - acc: 0.9700\n",
            "Epoch 00101: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1211 - acc: 0.9570 - val_loss: 0.3704 - val_acc: 0.8438\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1754 - acc: 0.9550\n",
            "Epoch 00102: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1471 - acc: 0.9648 - val_loss: 0.3700 - val_acc: 0.8438\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1196 - acc: 0.9500\n",
            "Epoch 00103: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1229 - acc: 0.9453 - val_loss: 0.4520 - val_acc: 0.8125\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1190 - acc: 0.9600\n",
            "Epoch 00104: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1076 - acc: 0.9648 - val_loss: 0.3937 - val_acc: 0.8438\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1024 - acc: 0.9650\n",
            "Epoch 00105: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0871 - acc: 0.9688 - val_loss: 0.3648 - val_acc: 0.8281\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1351 - acc: 0.9350\n",
            "Epoch 00106: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1501 - acc: 0.9336 - val_loss: 0.4590 - val_acc: 0.8438\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1337 - acc: 0.9450\n",
            "Epoch 00107: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1299 - acc: 0.9453 - val_loss: 0.5345 - val_acc: 0.8281\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1265 - acc: 0.9500\n",
            "Epoch 00108: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1263 - acc: 0.9531 - val_loss: 0.3275 - val_acc: 0.8438\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1188 - acc: 0.9450\n",
            "Epoch 00109: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1043 - acc: 0.9570 - val_loss: 0.2942 - val_acc: 0.8438\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1607 - acc: 0.9500\n",
            "Epoch 00110: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1675 - acc: 0.9453 - val_loss: 0.3249 - val_acc: 0.8594\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0947 - acc: 0.9700\n",
            "Epoch 00111: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0985 - acc: 0.9727 - val_loss: 0.4274 - val_acc: 0.8438\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1024 - acc: 0.9650\n",
            "Epoch 00112: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1148 - acc: 0.9609 - val_loss: 0.3776 - val_acc: 0.8438\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0703 - acc: 0.9700\n",
            "Epoch 00113: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0736 - acc: 0.9648 - val_loss: 0.3453 - val_acc: 0.8438\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1330 - acc: 0.9600\n",
            "Epoch 00114: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1246 - acc: 0.9609 - val_loss: 0.4875 - val_acc: 0.8125\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1034 - acc: 0.9500\n",
            "Epoch 00115: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0914 - acc: 0.9570 - val_loss: 0.7290 - val_acc: 0.7812\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1347 - acc: 0.9550\n",
            "Epoch 00116: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1232 - acc: 0.9531 - val_loss: 0.5042 - val_acc: 0.7969\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0766 - acc: 0.9700\n",
            "Epoch 00117: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0696 - acc: 0.9727 - val_loss: 0.4928 - val_acc: 0.8125\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0651 - acc: 0.9700\n",
            "Epoch 00118: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0608 - acc: 0.9727 - val_loss: 0.5045 - val_acc: 0.8125\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1276 - acc: 0.9600\n",
            "Epoch 00119: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1119 - acc: 0.9648 - val_loss: 0.4483 - val_acc: 0.8594\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0672 - acc: 0.9800\n",
            "Epoch 00120: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0907 - acc: 0.9766 - val_loss: 0.6128 - val_acc: 0.7969\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0823 - acc: 0.9650\n",
            "Epoch 00121: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1130 - acc: 0.9531 - val_loss: 0.5211 - val_acc: 0.8438\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0994 - acc: 0.9500\n",
            "Epoch 00122: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1011 - acc: 0.9531 - val_loss: 0.4893 - val_acc: 0.8438\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0741 - acc: 0.9800\n",
            "Epoch 00123: val_acc improved from 0.85938 to 0.89062, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-123-0.89062-0.98047.hdf5\n",
            "256/256 [==============================] - 2s 8ms/sample - loss: 0.0715 - acc: 0.9805 - val_loss: 0.4938 - val_acc: 0.8906\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0956 - acc: 0.9650\n",
            "Epoch 00124: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0842 - acc: 0.9727 - val_loss: 0.4987 - val_acc: 0.8281\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0483 - acc: 0.9750\n",
            "Epoch 00125: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0780 - acc: 0.9688 - val_loss: 0.4671 - val_acc: 0.8438\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0575 - acc: 0.9800\n",
            "Epoch 00126: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0552 - acc: 0.9805 - val_loss: 0.4283 - val_acc: 0.8750\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0630 - acc: 0.9750\n",
            "Epoch 00127: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0620 - acc: 0.9766 - val_loss: 0.4336 - val_acc: 0.8594\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0281 - acc: 0.9900\n",
            "Epoch 00128: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0513 - acc: 0.9727 - val_loss: 0.4547 - val_acc: 0.8594\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0973 - acc: 0.9600\n",
            "Epoch 00129: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0789 - acc: 0.9688 - val_loss: 0.6457 - val_acc: 0.7656\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0833 - acc: 0.9700\n",
            "Epoch 00130: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0799 - acc: 0.9727 - val_loss: 0.5106 - val_acc: 0.8281\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0674 - acc: 0.9800\n",
            "Epoch 00131: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0665 - acc: 0.9766 - val_loss: 0.3955 - val_acc: 0.8750\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1141 - acc: 0.9500\n",
            "Epoch 00132: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1028 - acc: 0.9570 - val_loss: 0.4989 - val_acc: 0.8438\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0718 - acc: 0.9700\n",
            "Epoch 00133: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0661 - acc: 0.9688 - val_loss: 0.5937 - val_acc: 0.8281\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0740 - acc: 0.9600\n",
            "Epoch 00134: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0929 - acc: 0.9570 - val_loss: 0.5644 - val_acc: 0.8281\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0790 - acc: 0.9800\n",
            "Epoch 00135: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0895 - acc: 0.9805 - val_loss: 0.4671 - val_acc: 0.8438\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0620 - acc: 0.9750\n",
            "Epoch 00136: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0624 - acc: 0.9727 - val_loss: 0.5135 - val_acc: 0.8125\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0638 - acc: 0.9750\n",
            "Epoch 00137: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0793 - acc: 0.9727 - val_loss: 0.5440 - val_acc: 0.8281\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0426 - acc: 0.9850\n",
            "Epoch 00138: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0449 - acc: 0.9844 - val_loss: 0.6770 - val_acc: 0.8125\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0509 - acc: 0.9700\n",
            "Epoch 00139: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0442 - acc: 0.9766 - val_loss: 0.4580 - val_acc: 0.8281\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0831 - acc: 0.9700\n",
            "Epoch 00140: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0663 - acc: 0.9766 - val_loss: 0.5885 - val_acc: 0.8438\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0326 - acc: 0.9900\n",
            "Epoch 00141: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0344 - acc: 0.9844 - val_loss: 0.4816 - val_acc: 0.8281\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0904 - acc: 0.9800\n",
            "Epoch 00142: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0861 - acc: 0.9805 - val_loss: 0.3875 - val_acc: 0.8750\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0872 - acc: 0.9650\n",
            "Epoch 00143: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0763 - acc: 0.9688 - val_loss: 0.3426 - val_acc: 0.8438\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1110 - acc: 0.9550\n",
            "Epoch 00144: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1048 - acc: 0.9570 - val_loss: 0.2987 - val_acc: 0.8594\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0848 - acc: 0.9800\n",
            "Epoch 00145: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0707 - acc: 0.9844 - val_loss: 0.2974 - val_acc: 0.8750\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0401 - acc: 0.9900\n",
            "Epoch 00146: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0495 - acc: 0.9805 - val_loss: 0.4234 - val_acc: 0.8594\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0192 - acc: 0.9950\n",
            "Epoch 00147: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0318 - acc: 0.9883 - val_loss: 0.4692 - val_acc: 0.8438\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0436 - acc: 0.9800\n",
            "Epoch 00148: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0408 - acc: 0.9844 - val_loss: 0.5202 - val_acc: 0.8438\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1444 - acc: 0.9750\n",
            "Epoch 00149: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1450 - acc: 0.9766 - val_loss: 0.4399 - val_acc: 0.8438\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0840 - acc: 0.9700\n",
            "Epoch 00150: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0813 - acc: 0.9688 - val_loss: 0.3809 - val_acc: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.96875,validation accuracy: 0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTfewxsVyiL3",
        "colab_type": "code",
        "outputId": "c74f2750-ffb6-4edf-dd83-87923328f10f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XrQBXKQ1zC9",
        "colab_type": "code",
        "outputId": "da111330-e900-46ea-8464-3ab83ad47b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 07 \n",
        "# added additional layers\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(128, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(128, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('07'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 6.1841 - acc: 0.5100\n",
            "Epoch 00001: val_acc improved from -inf to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.67188-0.54688.hdf5\n",
            "256/256 [==============================] - 6s 23ms/sample - loss: 5.5059 - acc: 0.5469 - val_loss: 0.6594 - val_acc: 0.6719\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7953 - acc: 0.5750\n",
            "Epoch 00002: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.7938 - acc: 0.5547 - val_loss: 0.7132 - val_acc: 0.3281\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7826 - acc: 0.3700\n",
            "Epoch 00003: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-03-0.68750-0.40625.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.7611 - acc: 0.4062 - val_loss: 0.6886 - val_acc: 0.6875\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6946 - acc: 0.5500\n",
            "Epoch 00004: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6933 - acc: 0.5586 - val_loss: 0.6815 - val_acc: 0.6719\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6890 - acc: 0.6000\n",
            "Epoch 00005: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6877 - acc: 0.6016 - val_loss: 0.6734 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6833 - acc: 0.6450\n",
            "Epoch 00006: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6830 - acc: 0.6406 - val_loss: 0.6531 - val_acc: 0.6719\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6776 - acc: 0.6650\n",
            "Epoch 00007: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6795 - acc: 0.6602 - val_loss: 0.6269 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6654 - acc: 0.6450\n",
            "Epoch 00008: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6629 - acc: 0.6641 - val_loss: 0.6208 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6423 - acc: 0.6850\n",
            "Epoch 00009: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6455 - acc: 0.6836 - val_loss: 0.6738 - val_acc: 0.6719\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6202 - acc: 0.6700\n",
            "Epoch 00010: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6265 - acc: 0.6523 - val_loss: 0.7970 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5916 - acc: 0.7300\n",
            "Epoch 00011: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5858 - acc: 0.7344 - val_loss: 0.8369 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5514 - acc: 0.7200\n",
            "Epoch 00012: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5294 - acc: 0.7500 - val_loss: 0.9692 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5438 - acc: 0.7050\n",
            "Epoch 00013: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5252 - acc: 0.7383 - val_loss: 1.1752 - val_acc: 0.6719\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4898 - acc: 0.7900\n",
            "Epoch 00014: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4776 - acc: 0.8047 - val_loss: 0.9678 - val_acc: 0.6719\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4177 - acc: 0.8550\n",
            "Epoch 00015: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.4102 - acc: 0.8555 - val_loss: 0.7194 - val_acc: 0.6719\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4236 - acc: 0.8200\n",
            "Epoch 00016: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3973 - acc: 0.8320 - val_loss: 0.6544 - val_acc: 0.6719\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4005 - acc: 0.8250\n",
            "Epoch 00017: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3845 - acc: 0.8398 - val_loss: 0.6793 - val_acc: 0.6719\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2942 - acc: 0.8750\n",
            "Epoch 00018: val_acc improved from 0.68750 to 0.70312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-18-0.70312-0.83594.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.3514 - acc: 0.8359 - val_loss: 0.6598 - val_acc: 0.7031\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3299 - acc: 0.8750\n",
            "Epoch 00019: val_acc improved from 0.70312 to 0.71875, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-19-0.71875-0.85547.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.3496 - acc: 0.8555 - val_loss: 0.6316 - val_acc: 0.7188\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3253 - acc: 0.8700\n",
            "Epoch 00020: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3112 - acc: 0.8789 - val_loss: 0.7160 - val_acc: 0.7031\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3395 - acc: 0.8550\n",
            "Epoch 00021: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3119 - acc: 0.8672 - val_loss: 0.5969 - val_acc: 0.7188\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2998 - acc: 0.8800\n",
            "Epoch 00022: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2896 - acc: 0.8828 - val_loss: 0.6474 - val_acc: 0.7188\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2511 - acc: 0.9000\n",
            "Epoch 00023: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2706 - acc: 0.8828 - val_loss: 0.7725 - val_acc: 0.6875\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2278 - acc: 0.9100\n",
            "Epoch 00024: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2413 - acc: 0.9023 - val_loss: 0.7684 - val_acc: 0.6875\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2327 - acc: 0.9100\n",
            "Epoch 00025: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2364 - acc: 0.9141 - val_loss: 0.8370 - val_acc: 0.6719\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2237 - acc: 0.8950\n",
            "Epoch 00026: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2474 - acc: 0.8867 - val_loss: 0.7942 - val_acc: 0.6875\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2232 - acc: 0.9100\n",
            "Epoch 00027: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2146 - acc: 0.9141 - val_loss: 0.7062 - val_acc: 0.7031\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2268 - acc: 0.9100\n",
            "Epoch 00028: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.2106 - acc: 0.9180 - val_loss: 0.7806 - val_acc: 0.7031\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1669 - acc: 0.9300\n",
            "Epoch 00029: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1963 - acc: 0.9219 - val_loss: 0.7357 - val_acc: 0.7031\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1850 - acc: 0.9200\n",
            "Epoch 00030: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1896 - acc: 0.9219 - val_loss: 0.6868 - val_acc: 0.7031\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1621 - acc: 0.9400\n",
            "Epoch 00031: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1522 - acc: 0.9414 - val_loss: 0.6689 - val_acc: 0.7031\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1009 - acc: 0.9700\n",
            "Epoch 00032: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1163 - acc: 0.9609 - val_loss: 0.8053 - val_acc: 0.7031\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1126 - acc: 0.9600\n",
            "Epoch 00033: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1293 - acc: 0.9531 - val_loss: 1.0721 - val_acc: 0.7031\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1225 - acc: 0.9500\n",
            "Epoch 00034: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1264 - acc: 0.9492 - val_loss: 0.9068 - val_acc: 0.7031\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1245 - acc: 0.9500\n",
            "Epoch 00035: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1337 - acc: 0.9531 - val_loss: 0.8195 - val_acc: 0.7031\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1063 - acc: 0.9500\n",
            "Epoch 00036: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1313 - acc: 0.9414 - val_loss: 0.9611 - val_acc: 0.6875\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0951 - acc: 0.9700\n",
            "Epoch 00037: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1287 - acc: 0.9570 - val_loss: 0.9682 - val_acc: 0.7031\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1355 - acc: 0.9400\n",
            "Epoch 00038: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1306 - acc: 0.9375 - val_loss: 1.1500 - val_acc: 0.6719\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1092 - acc: 0.9650\n",
            "Epoch 00039: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0945 - acc: 0.9688 - val_loss: 1.0614 - val_acc: 0.6875\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1290 - acc: 0.9450\n",
            "Epoch 00040: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1323 - acc: 0.9414 - val_loss: 1.2092 - val_acc: 0.7031\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0781 - acc: 0.9800\n",
            "Epoch 00041: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1122 - acc: 0.9570 - val_loss: 1.4783 - val_acc: 0.6875\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0497 - acc: 0.9900\n",
            "Epoch 00042: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0819 - acc: 0.9805 - val_loss: 1.5041 - val_acc: 0.6875\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1343 - acc: 0.9550\n",
            "Epoch 00043: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1314 - acc: 0.9570 - val_loss: 1.7389 - val_acc: 0.6719\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1369 - acc: 0.9450\n",
            "Epoch 00044: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1315 - acc: 0.9492 - val_loss: 1.0513 - val_acc: 0.7031\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0975 - acc: 0.9700\n",
            "Epoch 00045: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0947 - acc: 0.9727 - val_loss: 1.0170 - val_acc: 0.7188\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0954 - acc: 0.9600\n",
            "Epoch 00046: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.1010 - acc: 0.9570 - val_loss: 1.0993 - val_acc: 0.7188\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1006 - acc: 0.9700\n",
            "Epoch 00047: val_acc improved from 0.71875 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-47-0.73438-0.96484.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0967 - acc: 0.9648 - val_loss: 1.1334 - val_acc: 0.7344\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0492 - acc: 0.9900\n",
            "Epoch 00048: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0498 - acc: 0.9883 - val_loss: 1.4330 - val_acc: 0.7031\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0971 - acc: 0.9650\n",
            "Epoch 00049: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0858 - acc: 0.9688 - val_loss: 1.3637 - val_acc: 0.7031\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0664 - acc: 0.9750\n",
            "Epoch 00050: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0823 - acc: 0.9727 - val_loss: 1.1287 - val_acc: 0.7188\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0908 - acc: 0.9600\n",
            "Epoch 00051: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0786 - acc: 0.9648 - val_loss: 1.3114 - val_acc: 0.6875\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0882 - acc: 0.9700\n",
            "Epoch 00052: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0806 - acc: 0.9727 - val_loss: 1.5663 - val_acc: 0.6875\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0785 - acc: 0.9650\n",
            "Epoch 00053: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0662 - acc: 0.9727 - val_loss: 1.4004 - val_acc: 0.6875\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0303 - acc: 0.9900\n",
            "Epoch 00054: val_acc improved from 0.73438 to 0.76562, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-54-0.76562-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0515 - acc: 0.9844 - val_loss: 1.0707 - val_acc: 0.7656\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0487 - acc: 0.9850\n",
            "Epoch 00055: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0616 - acc: 0.9805 - val_loss: 1.1818 - val_acc: 0.6875\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0598 - acc: 0.9850\n",
            "Epoch 00056: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0672 - acc: 0.9805 - val_loss: 1.5116 - val_acc: 0.6875\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0676 - acc: 0.9700\n",
            "Epoch 00057: val_acc improved from 0.76562 to 0.78125, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-57-0.78125-0.96875.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0886 - acc: 0.9688 - val_loss: 0.8771 - val_acc: 0.7812\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0900 - acc: 0.9550\n",
            "Epoch 00058: val_acc did not improve from 0.78125\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0903 - acc: 0.9570 - val_loss: 0.6974 - val_acc: 0.7812\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0566 - acc: 0.9800\n",
            "Epoch 00059: val_acc did not improve from 0.78125\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0553 - acc: 0.9805 - val_loss: 0.7144 - val_acc: 0.7656\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0455 - acc: 0.9900\n",
            "Epoch 00060: val_acc improved from 0.78125 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-60-0.82812-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0492 - acc: 0.9844 - val_loss: 0.7198 - val_acc: 0.8281\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0484 - acc: 0.9850\n",
            "Epoch 00061: val_acc improved from 0.82812 to 0.84375, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-61-0.84375-0.98047.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0544 - acc: 0.9805 - val_loss: 0.9620 - val_acc: 0.8438\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0541 - acc: 0.9850\n",
            "Epoch 00062: val_acc improved from 0.84375 to 0.85938, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-62-0.85938-0.98828.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0447 - acc: 0.9883 - val_loss: 1.1604 - val_acc: 0.8594\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0487 - acc: 0.9750\n",
            "Epoch 00063: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0689 - acc: 0.9727 - val_loss: 1.1106 - val_acc: 0.8594\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0808 - acc: 0.9850\n",
            "Epoch 00064: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0745 - acc: 0.9805 - val_loss: 0.9678 - val_acc: 0.8438\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0427 - acc: 0.9800\n",
            "Epoch 00065: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0401 - acc: 0.9844 - val_loss: 0.8888 - val_acc: 0.8594\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0517 - acc: 0.9850\n",
            "Epoch 00066: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0475 - acc: 0.9844 - val_loss: 0.8821 - val_acc: 0.8594\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0428 - acc: 0.9900\n",
            "Epoch 00067: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0519 - acc: 0.9805 - val_loss: 0.9315 - val_acc: 0.8594\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0372 - acc: 0.9900\n",
            "Epoch 00068: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0748 - acc: 0.9805 - val_loss: 0.8419 - val_acc: 0.8281\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0413 - acc: 0.9850\n",
            "Epoch 00069: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0367 - acc: 0.9844 - val_loss: 1.1709 - val_acc: 0.7344\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0895 - acc: 0.9700\n",
            "Epoch 00070: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0838 - acc: 0.9688 - val_loss: 0.8365 - val_acc: 0.7969\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0453 - acc: 0.9800\n",
            "Epoch 00071: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0635 - acc: 0.9805 - val_loss: 0.8059 - val_acc: 0.8125\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0591 - acc: 0.9800\n",
            "Epoch 00072: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0770 - acc: 0.9766 - val_loss: 0.8180 - val_acc: 0.7969\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0476 - acc: 0.9750\n",
            "Epoch 00073: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0410 - acc: 0.9805 - val_loss: 0.8280 - val_acc: 0.7656\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0290 - acc: 0.9850\n",
            "Epoch 00074: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0349 - acc: 0.9805 - val_loss: 0.9006 - val_acc: 0.7656\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0946 - acc: 0.9750\n",
            "Epoch 00075: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0784 - acc: 0.9805 - val_loss: 0.9777 - val_acc: 0.7656\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0404 - acc: 0.9850\n",
            "Epoch 00076: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0451 - acc: 0.9844 - val_loss: 0.9821 - val_acc: 0.7969\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0615 - acc: 0.9700\n",
            "Epoch 00077: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0534 - acc: 0.9766 - val_loss: 0.9268 - val_acc: 0.8281\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0757 - acc: 0.9650\n",
            "Epoch 00078: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0642 - acc: 0.9727 - val_loss: 0.9012 - val_acc: 0.8594\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0153 - acc: 1.0000\n",
            "Epoch 00079: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0139 - acc: 1.0000 - val_loss: 0.8958 - val_acc: 0.8594\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0408 - acc: 0.9750\n",
            "Epoch 00080: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0332 - acc: 0.9805 - val_loss: 0.9029 - val_acc: 0.8594\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0363 - acc: 0.9900\n",
            "Epoch 00081: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0479 - acc: 0.9844 - val_loss: 0.9099 - val_acc: 0.8281\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0111 - acc: 1.0000\n",
            "Epoch 00082: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0146 - acc: 1.0000 - val_loss: 0.9276 - val_acc: 0.8125\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0510 - acc: 0.9850\n",
            "Epoch 00083: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0407 - acc: 0.9883 - val_loss: 0.8174 - val_acc: 0.8281\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0110 - acc: 0.9950\n",
            "Epoch 00084: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0107 - acc: 0.9961 - val_loss: 0.9260 - val_acc: 0.7812\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0628 - acc: 0.9850\n",
            "Epoch 00085: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0524 - acc: 0.9883 - val_loss: 0.9303 - val_acc: 0.7812\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0320 - acc: 0.9850\n",
            "Epoch 00086: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0503 - acc: 0.9805 - val_loss: 0.8978 - val_acc: 0.8281\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0183 - acc: 0.9900\n",
            "Epoch 00087: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0156 - acc: 0.9922 - val_loss: 0.7666 - val_acc: 0.8438\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0130 - acc: 0.9950\n",
            "Epoch 00088: val_acc improved from 0.85938 to 0.87500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-88-0.87500-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0109 - acc: 0.9961 - val_loss: 0.7404 - val_acc: 0.8750\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0199 - acc: 0.9950\n",
            "Epoch 00089: val_acc improved from 0.87500 to 0.90625, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-89-0.90625-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 5ms/sample - loss: 0.0433 - acc: 0.9844 - val_loss: 0.7347 - val_acc: 0.9062\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0268 - acc: 0.9950\n",
            "Epoch 00090: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0326 - acc: 0.9883 - val_loss: 0.7310 - val_acc: 0.8750\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0468 - acc: 0.9900\n",
            "Epoch 00091: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0490 - acc: 0.9883 - val_loss: 0.7159 - val_acc: 0.8594\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0165 - acc: 0.9950\n",
            "Epoch 00092: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0151 - acc: 0.9961 - val_loss: 0.8482 - val_acc: 0.7656\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0355 - acc: 0.9800\n",
            "Epoch 00093: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0355 - acc: 0.9805 - val_loss: 0.8673 - val_acc: 0.7656\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0448 - acc: 0.9900\n",
            "Epoch 00094: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0388 - acc: 0.9922 - val_loss: 0.7165 - val_acc: 0.7656\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0565 - acc: 0.9750\n",
            "Epoch 00095: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0596 - acc: 0.9727 - val_loss: 0.5938 - val_acc: 0.8438\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0150 - acc: 0.9950\n",
            "Epoch 00096: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0222 - acc: 0.9922 - val_loss: 0.6240 - val_acc: 0.8438\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0354 - acc: 0.9850\n",
            "Epoch 00097: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0338 - acc: 0.9844 - val_loss: 0.5773 - val_acc: 0.8594\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0247 - acc: 0.9950\n",
            "Epoch 00098: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0747 - acc: 0.9883 - val_loss: 0.5912 - val_acc: 0.8750\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0414 - acc: 0.9750\n",
            "Epoch 00099: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0397 - acc: 0.9766 - val_loss: 0.6666 - val_acc: 0.8594\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0242 - acc: 0.9900\n",
            "Epoch 00100: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0221 - acc: 0.9883 - val_loss: 0.6920 - val_acc: 0.8750\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0280 - acc: 0.9900\n",
            "Epoch 00101: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0254 - acc: 0.9922 - val_loss: 0.7651 - val_acc: 0.8750\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0291 - acc: 0.9850\n",
            "Epoch 00102: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0235 - acc: 0.9883 - val_loss: 0.9226 - val_acc: 0.8281\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0438 - acc: 0.9700\n",
            "Epoch 00103: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0364 - acc: 0.9766 - val_loss: 0.9776 - val_acc: 0.8594\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0698 - acc: 0.9800\n",
            "Epoch 00104: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0565 - acc: 0.9844 - val_loss: 1.0055 - val_acc: 0.8594\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0108 - acc: 0.9950\n",
            "Epoch 00105: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0170 - acc: 0.9883 - val_loss: 1.0572 - val_acc: 0.8438\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0280 - acc: 0.9950\n",
            "Epoch 00106: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0385 - acc: 0.9883 - val_loss: 1.0936 - val_acc: 0.8594\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0223 - acc: 0.9850\n",
            "Epoch 00107: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0269 - acc: 0.9844 - val_loss: 1.0075 - val_acc: 0.8438\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0147 - acc: 0.9950\n",
            "Epoch 00108: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0146 - acc: 0.9961 - val_loss: 0.7974 - val_acc: 0.8281\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0459 - acc: 0.9700\n",
            "Epoch 00109: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0517 - acc: 0.9688 - val_loss: 0.6978 - val_acc: 0.8750\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0122 - acc: 1.0000\n",
            "Epoch 00110: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0104 - acc: 1.0000 - val_loss: 0.7310 - val_acc: 0.8750\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0076 - acc: 1.0000\n",
            "Epoch 00111: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0082 - acc: 1.0000 - val_loss: 0.8150 - val_acc: 0.8750\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0311 - acc: 0.9900\n",
            "Epoch 00112: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0499 - acc: 0.9883 - val_loss: 0.9463 - val_acc: 0.8438\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0113 - acc: 0.9950\n",
            "Epoch 00113: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0098 - acc: 0.9961 - val_loss: 1.0931 - val_acc: 0.8281\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0094 - acc: 1.0000\n",
            "Epoch 00114: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0078 - acc: 1.0000 - val_loss: 1.2220 - val_acc: 0.8125\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0487 - acc: 0.9850\n",
            "Epoch 00115: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0402 - acc: 0.9883 - val_loss: 1.1513 - val_acc: 0.7969\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0210 - acc: 0.9900\n",
            "Epoch 00116: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0403 - acc: 0.9844 - val_loss: 1.1344 - val_acc: 0.7969\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 00117: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0159 - acc: 0.9961 - val_loss: 1.1997 - val_acc: 0.8125\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0290 - acc: 0.9900\n",
            "Epoch 00118: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0288 - acc: 0.9883 - val_loss: 1.1246 - val_acc: 0.8281\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0254 - acc: 0.9900\n",
            "Epoch 00119: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0215 - acc: 0.9922 - val_loss: 1.0695 - val_acc: 0.8438\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0099 - acc: 0.9950\n",
            "Epoch 00120: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0104 - acc: 0.9961 - val_loss: 1.1413 - val_acc: 0.8438\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0149 - acc: 0.9950\n",
            "Epoch 00121: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0125 - acc: 0.9961 - val_loss: 1.1939 - val_acc: 0.8281\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0246 - acc: 0.9900\n",
            "Epoch 00122: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0314 - acc: 0.9883 - val_loss: 1.4703 - val_acc: 0.8125\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 00123: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0424 - acc: 0.9961 - val_loss: 1.4325 - val_acc: 0.7969\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0102 - acc: 0.9950\n",
            "Epoch 00124: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0223 - acc: 0.9922 - val_loss: 1.1424 - val_acc: 0.8125\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0735 - acc: 0.9800\n",
            "Epoch 00125: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0589 - acc: 0.9844 - val_loss: 1.0034 - val_acc: 0.8125\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0366 - acc: 0.9850\n",
            "Epoch 00126: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0394 - acc: 0.9805 - val_loss: 1.0823 - val_acc: 0.7812\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0203 - acc: 0.9900\n",
            "Epoch 00127: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0218 - acc: 0.9883 - val_loss: 1.2319 - val_acc: 0.7656\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0188 - acc: 0.9950\n",
            "Epoch 00128: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0168 - acc: 0.9961 - val_loss: 1.2459 - val_acc: 0.8281\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0182 - acc: 0.9950\n",
            "Epoch 00129: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0144 - acc: 0.9961 - val_loss: 1.2058 - val_acc: 0.8125\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0200 - acc: 0.9900\n",
            "Epoch 00130: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0205 - acc: 0.9922 - val_loss: 1.1738 - val_acc: 0.8125\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0293 - acc: 0.9900\n",
            "Epoch 00131: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0294 - acc: 0.9883 - val_loss: 1.0936 - val_acc: 0.8125\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0157 - acc: 0.9950\n",
            "Epoch 00132: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0189 - acc: 0.9922 - val_loss: 1.1852 - val_acc: 0.7969\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0153 - acc: 0.9950\n",
            "Epoch 00133: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0199 - acc: 0.9922 - val_loss: 1.2659 - val_acc: 0.7969\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0303 - acc: 0.9800\n",
            "Epoch 00134: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0308 - acc: 0.9805 - val_loss: 1.2401 - val_acc: 0.8438\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0528 - acc: 0.9850\n",
            "Epoch 00135: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0486 - acc: 0.9844 - val_loss: 1.1403 - val_acc: 0.8438\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0361 - acc: 0.9900\n",
            "Epoch 00136: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0372 - acc: 0.9883 - val_loss: 1.1440 - val_acc: 0.8281\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 00137: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0052 - acc: 1.0000 - val_loss: 1.2868 - val_acc: 0.8125\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0150 - acc: 0.9900\n",
            "Epoch 00138: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0263 - acc: 0.9883 - val_loss: 1.5564 - val_acc: 0.7812\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0068 - acc: 0.9950\n",
            "Epoch 00139: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0071 - acc: 0.9961 - val_loss: 1.5382 - val_acc: 0.7656\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0163 - acc: 0.9950\n",
            "Epoch 00140: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0131 - acc: 0.9961 - val_loss: 1.4329 - val_acc: 0.7656\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0078 - acc: 1.0000\n",
            "Epoch 00141: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0076 - acc: 1.0000 - val_loss: 1.4556 - val_acc: 0.7656\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 00142: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0092 - acc: 0.9961 - val_loss: 1.3476 - val_acc: 0.7812\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0160 - acc: 0.9900\n",
            "Epoch 00143: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0144 - acc: 0.9922 - val_loss: 1.1672 - val_acc: 0.7656\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0292 - acc: 0.9950\n",
            "Epoch 00144: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0230 - acc: 0.9961 - val_loss: 1.3391 - val_acc: 0.7969\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 00145: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0080 - acc: 0.9961 - val_loss: 1.5267 - val_acc: 0.7812\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0097 - acc: 0.9950\n",
            "Epoch 00146: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0085 - acc: 0.9961 - val_loss: 1.3623 - val_acc: 0.7969\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 00147: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 1.2881 - val_acc: 0.8281\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0165 - acc: 0.9950\n",
            "Epoch 00148: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0188 - acc: 0.9922 - val_loss: 1.2554 - val_acc: 0.8438\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0117 - acc: 0.9950\n",
            "Epoch 00149: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0099 - acc: 0.9961 - val_loss: 1.2443 - val_acc: 0.8281\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 00150: val_acc did not improve from 0.90625\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.0041 - acc: 1.0000 - val_loss: 1.3098 - val_acc: 0.8281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4yXJQVjjDFelhSMMcZ4/T8l4G6ajaK+bAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 1.0,validation accuracy: 0.828125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2xi_1AW1_6G",
        "colab_type": "code",
        "outputId": "2365a404-0d9e-46e2-92da-f4ab9683b50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxrTrYnX3slV",
        "colab_type": "code",
        "outputId": "578814ea-0a96-4ae0-8271-a5de5f7eb3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 08\n",
        "# recalled the initial number of neurons (64) in conv2d layers\n",
        "# with additional layers added\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('08'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.7289 - acc: 0.5750\n",
            "Epoch 00001: val_acc improved from -inf to 0.32812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.32812-0.55469.hdf5\n",
            "256/256 [==============================] - 6s 22ms/sample - loss: 1.6507 - acc: 0.5547 - val_loss: 0.7029 - val_acc: 0.3281\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.0209 - acc: 0.4850\n",
            "Epoch 00002: val_acc improved from 0.32812 to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-02-0.67188-0.51953.hdf5\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.9800 - acc: 0.5195 - val_loss: 0.6798 - val_acc: 0.6719\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7246 - acc: 0.6600\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7125 - acc: 0.6445 - val_loss: 0.7047 - val_acc: 0.3281\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6869 - acc: 0.5950\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6960 - acc: 0.5859 - val_loss: 0.7053 - val_acc: 0.3281\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6368 - acc: 0.6550\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6292 - acc: 0.6680 - val_loss: 0.6760 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6209 - acc: 0.6550\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6140 - acc: 0.6562 - val_loss: 0.6529 - val_acc: 0.6719\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5988 - acc: 0.6450\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5802 - acc: 0.6641 - val_loss: 0.6413 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5558 - acc: 0.6900\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5403 - acc: 0.7070 - val_loss: 0.6385 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5057 - acc: 0.7550\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5190 - acc: 0.7500 - val_loss: 0.6368 - val_acc: 0.6719\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4992 - acc: 0.7550\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5083 - acc: 0.7539 - val_loss: 0.6394 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4504 - acc: 0.8000\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4350 - acc: 0.8047 - val_loss: 0.6113 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4130 - acc: 0.8000\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4355 - acc: 0.7852 - val_loss: 0.6030 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3772 - acc: 0.8300\n",
            "Epoch 00013: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-13-0.68750-0.83984.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.3653 - acc: 0.8398 - val_loss: 0.5963 - val_acc: 0.6875\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3533 - acc: 0.8500\n",
            "Epoch 00014: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3343 - acc: 0.8555 - val_loss: 0.5935 - val_acc: 0.6875\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3264 - acc: 0.8900\n",
            "Epoch 00015: val_acc improved from 0.68750 to 0.70312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-15-0.70312-0.89844.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.3068 - acc: 0.8984 - val_loss: 0.6124 - val_acc: 0.7031\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2755 - acc: 0.9100\n",
            "Epoch 00016: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2656 - acc: 0.9180 - val_loss: 0.6162 - val_acc: 0.6875\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2402 - acc: 0.8850\n",
            "Epoch 00017: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2410 - acc: 0.8945 - val_loss: 0.6063 - val_acc: 0.7031\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2349 - acc: 0.9050\n",
            "Epoch 00018: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2523 - acc: 0.9102 - val_loss: 0.6151 - val_acc: 0.7031\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2062 - acc: 0.9100\n",
            "Epoch 00019: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2210 - acc: 0.8984 - val_loss: 0.6035 - val_acc: 0.7031\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1729 - acc: 0.9450\n",
            "Epoch 00020: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1676 - acc: 0.9414 - val_loss: 0.6229 - val_acc: 0.7031\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2442 - acc: 0.9100\n",
            "Epoch 00021: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2368 - acc: 0.9102 - val_loss: 0.6668 - val_acc: 0.6719\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1756 - acc: 0.9350\n",
            "Epoch 00022: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1693 - acc: 0.9375 - val_loss: 0.6500 - val_acc: 0.7031\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1901 - acc: 0.9150\n",
            "Epoch 00023: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1960 - acc: 0.9180 - val_loss: 0.7303 - val_acc: 0.6719\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1943 - acc: 0.9100\n",
            "Epoch 00024: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1780 - acc: 0.9219 - val_loss: 0.7016 - val_acc: 0.6719\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2097 - acc: 0.9200\n",
            "Epoch 00025: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1988 - acc: 0.9258 - val_loss: 0.8155 - val_acc: 0.6719\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1174 - acc: 0.9500\n",
            "Epoch 00026: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1326 - acc: 0.9414 - val_loss: 0.8995 - val_acc: 0.6719\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1168 - acc: 0.9550\n",
            "Epoch 00027: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1237 - acc: 0.9570 - val_loss: 0.8767 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1292 - acc: 0.9500\n",
            "Epoch 00028: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1303 - acc: 0.9531 - val_loss: 1.0109 - val_acc: 0.6719\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1339 - acc: 0.9450\n",
            "Epoch 00029: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1323 - acc: 0.9453 - val_loss: 1.0653 - val_acc: 0.6719\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1217 - acc: 0.9700\n",
            "Epoch 00030: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1182 - acc: 0.9688 - val_loss: 1.1074 - val_acc: 0.6719\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1096 - acc: 0.9500\n",
            "Epoch 00031: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1071 - acc: 0.9492 - val_loss: 1.0626 - val_acc: 0.6719\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1229 - acc: 0.9550\n",
            "Epoch 00032: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1253 - acc: 0.9492 - val_loss: 1.0164 - val_acc: 0.6719\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0839 - acc: 0.9650\n",
            "Epoch 00033: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0869 - acc: 0.9688 - val_loss: 1.0942 - val_acc: 0.6719\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1052 - acc: 0.9650\n",
            "Epoch 00034: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1018 - acc: 0.9609 - val_loss: 1.1025 - val_acc: 0.6719\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0736 - acc: 0.9700\n",
            "Epoch 00035: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0722 - acc: 0.9727 - val_loss: 1.0233 - val_acc: 0.6719\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0721 - acc: 0.9750\n",
            "Epoch 00036: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0652 - acc: 0.9766 - val_loss: 1.1890 - val_acc: 0.6719\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0871 - acc: 0.9800\n",
            "Epoch 00037: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0860 - acc: 0.9766 - val_loss: 1.1038 - val_acc: 0.6719\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0655 - acc: 0.9750\n",
            "Epoch 00038: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1041 - acc: 0.9531 - val_loss: 1.1584 - val_acc: 0.6719\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0435 - acc: 0.9950\n",
            "Epoch 00039: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0620 - acc: 0.9844 - val_loss: 1.5342 - val_acc: 0.6719\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0845 - acc: 0.9650\n",
            "Epoch 00040: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0817 - acc: 0.9688 - val_loss: 1.3541 - val_acc: 0.6719\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0627 - acc: 0.9800\n",
            "Epoch 00041: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0726 - acc: 0.9727 - val_loss: 1.4201 - val_acc: 0.6719\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0956 - acc: 0.9600\n",
            "Epoch 00042: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0983 - acc: 0.9648 - val_loss: 1.7514 - val_acc: 0.6719\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0516 - acc: 0.9800\n",
            "Epoch 00043: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0469 - acc: 0.9844 - val_loss: 1.6855 - val_acc: 0.6719\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0580 - acc: 0.9750\n",
            "Epoch 00044: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0609 - acc: 0.9766 - val_loss: 1.7720 - val_acc: 0.6719\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0644 - acc: 0.9750\n",
            "Epoch 00045: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0575 - acc: 0.9766 - val_loss: 2.3240 - val_acc: 0.6719\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0588 - acc: 0.9750\n",
            "Epoch 00046: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0603 - acc: 0.9688 - val_loss: 2.2147 - val_acc: 0.6719\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0346 - acc: 0.9900\n",
            "Epoch 00047: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0380 - acc: 0.9883 - val_loss: 1.9041 - val_acc: 0.6719\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0335 - acc: 0.9850\n",
            "Epoch 00048: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0505 - acc: 0.9844 - val_loss: 1.9159 - val_acc: 0.6719\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0479 - acc: 0.9900\n",
            "Epoch 00049: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0479 - acc: 0.9883 - val_loss: 2.1340 - val_acc: 0.6719\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0507 - acc: 0.9850\n",
            "Epoch 00050: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0447 - acc: 0.9883 - val_loss: 1.9597 - val_acc: 0.6719\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0312 - acc: 0.9900\n",
            "Epoch 00051: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0413 - acc: 0.9844 - val_loss: 1.5441 - val_acc: 0.6719\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0564 - acc: 0.9900\n",
            "Epoch 00052: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0727 - acc: 0.9766 - val_loss: 1.7371 - val_acc: 0.6719\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0474 - acc: 0.9850\n",
            "Epoch 00053: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0518 - acc: 0.9805 - val_loss: 1.9155 - val_acc: 0.6719\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0262 - acc: 0.9900\n",
            "Epoch 00054: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0247 - acc: 0.9922 - val_loss: 1.6355 - val_acc: 0.6719\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0514 - acc: 0.9750\n",
            "Epoch 00055: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0561 - acc: 0.9766 - val_loss: 1.9355 - val_acc: 0.6719\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0265 - acc: 0.9900\n",
            "Epoch 00056: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0716 - acc: 0.9805 - val_loss: 2.0988 - val_acc: 0.6719\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0697 - acc: 0.9800\n",
            "Epoch 00057: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0591 - acc: 0.9844 - val_loss: 1.3611 - val_acc: 0.6719\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0519 - acc: 0.9700\n",
            "Epoch 00058: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0565 - acc: 0.9727 - val_loss: 1.4181 - val_acc: 0.6875\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0360 - acc: 0.9800\n",
            "Epoch 00059: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0309 - acc: 0.9844 - val_loss: 1.9247 - val_acc: 0.6719\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0455 - acc: 0.9850\n",
            "Epoch 00060: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0455 - acc: 0.9844 - val_loss: 1.8702 - val_acc: 0.6719\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0178 - acc: 1.0000\n",
            "Epoch 00061: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0176 - acc: 1.0000 - val_loss: 1.5804 - val_acc: 0.6875\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0260 - acc: 0.9950\n",
            "Epoch 00062: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0307 - acc: 0.9922 - val_loss: 1.6404 - val_acc: 0.6875\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0160 - acc: 0.9950\n",
            "Epoch 00063: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0140 - acc: 0.9961 - val_loss: 2.2778 - val_acc: 0.6719\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0311 - acc: 0.9900\n",
            "Epoch 00064: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0501 - acc: 0.9766 - val_loss: 2.2956 - val_acc: 0.6719\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0248 - acc: 0.9950\n",
            "Epoch 00065: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0236 - acc: 0.9961 - val_loss: 1.8892 - val_acc: 0.6875\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0396 - acc: 0.9850\n",
            "Epoch 00066: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0426 - acc: 0.9805 - val_loss: 2.0773 - val_acc: 0.6875\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 00067: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0133 - acc: 0.9961 - val_loss: 2.5449 - val_acc: 0.6719\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0266 - acc: 0.9950\n",
            "Epoch 00068: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0253 - acc: 0.9961 - val_loss: 2.5077 - val_acc: 0.6719\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0227 - acc: 0.9900\n",
            "Epoch 00069: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0311 - acc: 0.9883 - val_loss: 1.8188 - val_acc: 0.6875\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0142 - acc: 0.9900\n",
            "Epoch 00070: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0147 - acc: 0.9922 - val_loss: 1.3659 - val_acc: 0.7031\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0440 - acc: 0.9900\n",
            "Epoch 00071: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0405 - acc: 0.9883 - val_loss: 1.7360 - val_acc: 0.6875\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0083 - acc: 1.0000\n",
            "Epoch 00072: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0081 - acc: 1.0000 - val_loss: 2.1729 - val_acc: 0.6719\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0340 - acc: 0.9900\n",
            "Epoch 00073: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0284 - acc: 0.9922 - val_loss: 1.9908 - val_acc: 0.6875\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0113 - acc: 0.9950\n",
            "Epoch 00074: val_acc improved from 0.70312 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-74-0.73438-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0104 - acc: 0.9961 - val_loss: 1.5645 - val_acc: 0.7344\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 00075: val_acc improved from 0.73438 to 0.75000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-75-0.75000-1.00000.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0079 - acc: 1.0000 - val_loss: 1.4374 - val_acc: 0.7500\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0262 - acc: 0.9900\n",
            "Epoch 00076: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0244 - acc: 0.9883 - val_loss: 1.8073 - val_acc: 0.7344\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0167 - acc: 0.9950\n",
            "Epoch 00077: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0141 - acc: 0.9961 - val_loss: 1.9527 - val_acc: 0.7344\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0184 - acc: 0.9950\n",
            "Epoch 00078: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0152 - acc: 0.9961 - val_loss: 1.8866 - val_acc: 0.7188\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0250 - acc: 0.9900\n",
            "Epoch 00079: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0202 - acc: 0.9922 - val_loss: 2.0182 - val_acc: 0.7188\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 00080: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 2.0763 - val_acc: 0.7188\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0134 - acc: 0.9950\n",
            "Epoch 00081: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0116 - acc: 0.9961 - val_loss: 1.9160 - val_acc: 0.7188\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0220 - acc: 0.9900\n",
            "Epoch 00082: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0210 - acc: 0.9922 - val_loss: 1.6549 - val_acc: 0.7500\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 00083: val_acc improved from 0.75000 to 0.76562, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-83-0.76562-1.00000.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 1.6692 - val_acc: 0.7656\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 00084: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 1.6873 - val_acc: 0.7656\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0123 - acc: 0.9950\n",
            "Epoch 00085: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0104 - acc: 0.9961 - val_loss: 1.9153 - val_acc: 0.7656\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0082 - acc: 0.9950\n",
            "Epoch 00086: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0095 - acc: 0.9961 - val_loss: 2.1579 - val_acc: 0.7344\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0141 - acc: 0.9950\n",
            "Epoch 00087: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0111 - acc: 0.9961 - val_loss: 2.0629 - val_acc: 0.7500\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0092 - acc: 0.9950\n",
            "Epoch 00088: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0108 - acc: 0.9922 - val_loss: 1.6942 - val_acc: 0.7656\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0169 - acc: 0.9950\n",
            "Epoch 00089: val_acc improved from 0.76562 to 0.79688, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-89-0.79688-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0136 - acc: 0.9961 - val_loss: 1.2841 - val_acc: 0.7969\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0145 - acc: 0.9950\n",
            "Epoch 00090: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0244 - acc: 0.9922 - val_loss: 1.5875 - val_acc: 0.7812\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0111 - acc: 0.9950\n",
            "Epoch 00091: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0089 - acc: 0.9961 - val_loss: 2.2416 - val_acc: 0.7188\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0616 - acc: 0.9800\n",
            "Epoch 00092: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0509 - acc: 0.9844 - val_loss: 1.4590 - val_acc: 0.7812\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 00093: val_acc improved from 0.79688 to 0.81250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-93-0.81250-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0069 - acc: 0.9961 - val_loss: 0.9949 - val_acc: 0.8125\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0264 - acc: 0.9950\n",
            "Epoch 00094: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0234 - acc: 0.9961 - val_loss: 1.0393 - val_acc: 0.8125\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0136 - acc: 0.9950\n",
            "Epoch 00095: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0144 - acc: 0.9922 - val_loss: 1.1568 - val_acc: 0.8125\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 00096: val_acc improved from 0.81250 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-96-0.82812-1.00000.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 1.0886 - val_acc: 0.8281\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 00097: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0098 - acc: 0.9961 - val_loss: 1.0426 - val_acc: 0.8125\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0109 - acc: 0.9950\n",
            "Epoch 00098: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0106 - acc: 0.9961 - val_loss: 1.0154 - val_acc: 0.8125\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0227 - acc: 0.9900\n",
            "Epoch 00099: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0220 - acc: 0.9883 - val_loss: 0.8616 - val_acc: 0.8281\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0207 - acc: 0.9850\n",
            "Epoch 00100: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0209 - acc: 0.9883 - val_loss: 0.9055 - val_acc: 0.8125\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0159 - acc: 0.9900\n",
            "Epoch 00101: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0153 - acc: 0.9922 - val_loss: 1.3256 - val_acc: 0.7500\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0132 - acc: 0.9900\n",
            "Epoch 00102: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0106 - acc: 0.9922 - val_loss: 1.2159 - val_acc: 0.7656\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0097 - acc: 0.9950\n",
            "Epoch 00103: val_acc improved from 0.82812 to 0.84375, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-103-0.84375-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0100 - acc: 0.9961 - val_loss: 0.9885 - val_acc: 0.8438\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0127 - acc: 0.9950\n",
            "Epoch 00104: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0110 - acc: 0.9961 - val_loss: 1.0068 - val_acc: 0.8281\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0102 - acc: 0.9950\n",
            "Epoch 00105: val_acc improved from 0.84375 to 0.85938, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-105-0.85938-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0102 - acc: 0.9961 - val_loss: 0.8989 - val_acc: 0.8594\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0123 - acc: 0.9950\n",
            "Epoch 00106: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0200 - acc: 0.9922 - val_loss: 0.8045 - val_acc: 0.8594\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0094 - acc: 0.9950\n",
            "Epoch 00107: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0196 - acc: 0.9883 - val_loss: 0.8164 - val_acc: 0.8594\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0080 - acc: 0.9950\n",
            "Epoch 00108: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0070 - acc: 0.9961 - val_loss: 0.7073 - val_acc: 0.8438\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0177 - acc: 0.9950\n",
            "Epoch 00109: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0269 - acc: 0.9883 - val_loss: 0.6254 - val_acc: 0.8281\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0463 - acc: 0.9800\n",
            "Epoch 00110: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0398 - acc: 0.9844 - val_loss: 0.6609 - val_acc: 0.8125\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 00111: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0250 - acc: 0.9961 - val_loss: 0.6888 - val_acc: 0.8125\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
            "Epoch 00112: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0099 - acc: 0.9961 - val_loss: 0.6154 - val_acc: 0.8281\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 00113: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0433 - acc: 0.9922 - val_loss: 0.5951 - val_acc: 0.7812\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0170 - acc: 0.9900\n",
            "Epoch 00114: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0201 - acc: 0.9883 - val_loss: 0.7210 - val_acc: 0.7812\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0568 - acc: 0.9800\n",
            "Epoch 00115: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0537 - acc: 0.9805 - val_loss: 0.6096 - val_acc: 0.8281\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0404 - acc: 0.9900\n",
            "Epoch 00116: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0363 - acc: 0.9922 - val_loss: 0.8568 - val_acc: 0.8281\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0382 - acc: 0.9800\n",
            "Epoch 00117: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0310 - acc: 0.9844 - val_loss: 0.8562 - val_acc: 0.8125\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0138 - acc: 1.0000\n",
            "Epoch 00118: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0123 - acc: 1.0000 - val_loss: 0.9224 - val_acc: 0.7969\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0196 - acc: 0.9900\n",
            "Epoch 00119: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0194 - acc: 0.9922 - val_loss: 0.8297 - val_acc: 0.8281\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0192 - acc: 0.9950\n",
            "Epoch 00120: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0167 - acc: 0.9961 - val_loss: 0.9207 - val_acc: 0.8438\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0124 - acc: 0.9950\n",
            "Epoch 00121: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0102 - acc: 0.9961 - val_loss: 1.1805 - val_acc: 0.8125\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 00122: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0097 - acc: 0.9961 - val_loss: 1.3981 - val_acc: 0.7812\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0102 - acc: 0.9950\n",
            "Epoch 00123: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0093 - acc: 0.9961 - val_loss: 1.6138 - val_acc: 0.7812\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 00124: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0119 - acc: 0.9961 - val_loss: 1.6676 - val_acc: 0.7344\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0077 - acc: 1.0000\n",
            "Epoch 00125: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0069 - acc: 1.0000 - val_loss: 1.4853 - val_acc: 0.8125\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0084 - acc: 0.9950\n",
            "Epoch 00126: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0110 - acc: 0.9961 - val_loss: 1.3877 - val_acc: 0.7969\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0330 - acc: 0.9800\n",
            "Epoch 00127: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0311 - acc: 0.9844 - val_loss: 1.3110 - val_acc: 0.7812\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 00128: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 1.2266 - val_acc: 0.7969\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0074 - acc: 1.0000\n",
            "Epoch 00129: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0065 - acc: 1.0000 - val_loss: 1.2069 - val_acc: 0.8125\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0128 - acc: 0.9900\n",
            "Epoch 00130: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0231 - acc: 0.9883 - val_loss: 1.0846 - val_acc: 0.8438\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 00131: val_acc improved from 0.85938 to 0.89062, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-131-0.89062-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0355 - acc: 0.9961 - val_loss: 1.0209 - val_acc: 0.8906\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0176 - acc: 0.9950\n",
            "Epoch 00132: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0182 - acc: 0.9961 - val_loss: 1.2694 - val_acc: 0.8281\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0507 - acc: 0.9850\n",
            "Epoch 00133: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0400 - acc: 0.9883 - val_loss: 1.1944 - val_acc: 0.7969\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 00134: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0048 - acc: 1.0000 - val_loss: 1.1250 - val_acc: 0.7969\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0116 - acc: 0.9950\n",
            "Epoch 00135: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0333 - acc: 0.9883 - val_loss: 1.0601 - val_acc: 0.8125\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0222 - acc: 0.9850\n",
            "Epoch 00136: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0195 - acc: 0.9883 - val_loss: 1.2025 - val_acc: 0.8125\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 00137: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0243 - acc: 0.9961 - val_loss: 1.5382 - val_acc: 0.8125\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0264 - acc: 0.9900\n",
            "Epoch 00138: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0744 - acc: 0.9844 - val_loss: 1.2263 - val_acc: 0.8125\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0262 - acc: 0.9900\n",
            "Epoch 00139: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0305 - acc: 0.9883 - val_loss: 1.1211 - val_acc: 0.8438\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0531 - acc: 0.9750\n",
            "Epoch 00140: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0433 - acc: 0.9805 - val_loss: 0.8781 - val_acc: 0.8438\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0235 - acc: 0.9950\n",
            "Epoch 00141: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0290 - acc: 0.9922 - val_loss: 0.9126 - val_acc: 0.8438\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0220 - acc: 0.9900\n",
            "Epoch 00142: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0217 - acc: 0.9883 - val_loss: 0.7656 - val_acc: 0.8438\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0116 - acc: 0.9950\n",
            "Epoch 00143: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0103 - acc: 0.9961 - val_loss: 0.8291 - val_acc: 0.8438\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0450 - acc: 0.9800\n",
            "Epoch 00144: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0385 - acc: 0.9805 - val_loss: 0.8153 - val_acc: 0.8438\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 00145: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0081 - acc: 0.9961 - val_loss: 0.8351 - val_acc: 0.8750\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0181 - acc: 0.9900\n",
            "Epoch 00146: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0147 - acc: 0.9922 - val_loss: 0.9546 - val_acc: 0.8281\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 00147: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0091 - acc: 0.9961 - val_loss: 0.9716 - val_acc: 0.8281\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0076 - acc: 1.0000\n",
            "Epoch 00148: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0093 - acc: 0.9961 - val_loss: 0.9605 - val_acc: 0.8750\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0180 - acc: 0.9900\n",
            "Epoch 00149: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0141 - acc: 0.9922 - val_loss: 1.0204 - val_acc: 0.8594\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 8.5370e-04 - acc: 1.0000\n",
            "Epoch 00150: val_acc did not improve from 0.89062\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 1.0618 - val_acc: 0.8594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 1.0,validation accuracy: 0.859375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdvyqHa43spi",
        "colab_type": "code",
        "outputId": "31e36f63-d2ab-418e-b0e3-8f6582f71684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3zkV91Y3yP1",
        "colab_type": "code",
        "outputId": "c77c2a7f-acf2-4ce6-b958-0a2158be1c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 09 \n",
        "# decreased learning rate of adam optimizer\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(lr=1e-1),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('09'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 487.1447 - acc: 0.5350\n",
            "Epoch 00001: val_acc improved from -inf to 0.32812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.32812-0.50391.hdf5\n",
            "256/256 [==============================] - 10s 38ms/sample - loss: 2821.2992 - acc: 0.5039 - val_loss: 1128295.2500 - val_acc: 0.3281\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1302.9964 - acc: 0.5450\n",
            "Epoch 00002: val_acc improved from 0.32812 to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-02-0.67188-0.49609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 1020.7452 - acc: 0.4961 - val_loss: 0.7885 - val_acc: 0.6719\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 4.0001 - acc: 0.5950\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 3.5458 - acc: 0.5938 - val_loss: 0.9647 - val_acc: 0.6719\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 2.6599 - acc: 0.6050\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 2.3867 - acc: 0.5742 - val_loss: 0.7637 - val_acc: 0.3281\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.7190 - acc: 0.4900\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 1.5329 - acc: 0.4805 - val_loss: 0.6329 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7986 - acc: 0.5250\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7868 - acc: 0.5312 - val_loss: 0.7079 - val_acc: 0.3281\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8225 - acc: 0.5700\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7884 - acc: 0.5820 - val_loss: 0.6384 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.0897 - acc: 0.5900\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 1.0235 - acc: 0.5547 - val_loss: 0.6758 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7289 - acc: 0.5400\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7446 - acc: 0.5273 - val_loss: 20.1857 - val_acc: 0.6250\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7447 - acc: 0.4550\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7340 - acc: 0.4531 - val_loss: 674.0499 - val_acc: 0.3281\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7042 - acc: 0.5750\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7014 - acc: 0.5898 - val_loss: 4759.3740 - val_acc: 0.3281\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6876 - acc: 0.5900\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6884 - acc: 0.5859 - val_loss: 5665.6377 - val_acc: 0.3281\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6821 - acc: 0.5300\n",
            "Epoch 00013: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.5508 - val_loss: 3719.7917 - val_acc: 0.3750\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6921 - acc: 0.5850\n",
            "Epoch 00014: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6941 - acc: 0.5781 - val_loss: 663.5321 - val_acc: 0.4688\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8318 - acc: 0.6000\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.8632 - acc: 0.6016 - val_loss: 478.5172 - val_acc: 0.5625\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8491 - acc: 0.6000\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.8099 - acc: 0.5977 - val_loss: 420.2496 - val_acc: 0.6094\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6781 - acc: 0.6000\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6759 - acc: 0.6016 - val_loss: 450.7206 - val_acc: 0.4844\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6620 - acc: 0.6350\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6748 - acc: 0.6016 - val_loss: 487.9659 - val_acc: 0.4688\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6832 - acc: 0.5950\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6802 - acc: 0.5977 - val_loss: 557.9921 - val_acc: 0.3125\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6831 - acc: 0.6000\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6791 - acc: 0.6016 - val_loss: 512.1260 - val_acc: 0.3594\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6629 - acc: 0.6150\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6701 - acc: 0.6016 - val_loss: 335.8664 - val_acc: 0.4375\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6746 - acc: 0.6000\n",
            "Epoch 00022: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6764 - acc: 0.6016 - val_loss: 157.1272 - val_acc: 0.5312\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6913 - acc: 0.5750\n",
            "Epoch 00023: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6807 - acc: 0.6016 - val_loss: 28.9771 - val_acc: 0.6094\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6685 - acc: 0.6200\n",
            "Epoch 00024: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6769 - acc: 0.6016 - val_loss: 1.7932 - val_acc: 0.6719\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6833 - acc: 0.5950\n",
            "Epoch 00025: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6801 - acc: 0.6055 - val_loss: 0.6505 - val_acc: 0.6719\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6832 - acc: 0.6050\n",
            "Epoch 00026: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6818 - acc: 0.6016 - val_loss: 0.6382 - val_acc: 0.6719\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6719 - acc: 0.6050\n",
            "Epoch 00027: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6757 - acc: 0.6016 - val_loss: 0.6378 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6723 - acc: 0.6050\n",
            "Epoch 00028: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6736 - acc: 0.6016 - val_loss: 0.6401 - val_acc: 0.6719\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6814 - acc: 0.5800\n",
            "Epoch 00029: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6752 - acc: 0.6016 - val_loss: 0.6474 - val_acc: 0.6719\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6688 - acc: 0.6050\n",
            "Epoch 00030: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6713 - acc: 0.6016 - val_loss: 0.6457 - val_acc: 0.6719\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6740 - acc: 0.6000\n",
            "Epoch 00031: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6734 - acc: 0.6016 - val_loss: 0.6446 - val_acc: 0.6719\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6788 - acc: 0.5900\n",
            "Epoch 00032: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6417 - val_acc: 0.6719\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6779 - acc: 0.5950\n",
            "Epoch 00033: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6738 - acc: 0.6016 - val_loss: 0.6406 - val_acc: 0.6719\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6738 - acc: 0.6000\n",
            "Epoch 00034: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6723 - acc: 0.6016 - val_loss: 0.6413 - val_acc: 0.6719\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6650 - acc: 0.6150\n",
            "Epoch 00035: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6415 - val_acc: 0.6719\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6670 - acc: 0.6100\n",
            "Epoch 00036: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6720 - acc: 0.6016 - val_loss: 0.6453 - val_acc: 0.6719\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6727 - acc: 0.6150\n",
            "Epoch 00037: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6752 - acc: 0.6016 - val_loss: 0.6461 - val_acc: 0.6719\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6787 - acc: 0.5950\n",
            "Epoch 00038: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6775 - acc: 0.6016 - val_loss: 0.6441 - val_acc: 0.6719\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6724 - acc: 0.6000\n",
            "Epoch 00039: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6420 - val_acc: 0.6719\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6772 - acc: 0.5900\n",
            "Epoch 00040: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6720 - acc: 0.6016 - val_loss: 0.6419 - val_acc: 0.6719\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6768 - acc: 0.5900\n",
            "Epoch 00041: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6718 - acc: 0.6016 - val_loss: 0.6417 - val_acc: 0.6719\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6749 - acc: 0.5950\n",
            "Epoch 00042: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6417 - val_acc: 0.6719\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6754 - acc: 0.5950\n",
            "Epoch 00043: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6710 - acc: 0.6016 - val_loss: 0.6410 - val_acc: 0.6719\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6660 - acc: 0.6250\n",
            "Epoch 00044: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6770 - acc: 0.6016 - val_loss: 0.6418 - val_acc: 0.6719\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6715 - acc: 0.6050\n",
            "Epoch 00045: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6736 - acc: 0.6016 - val_loss: 0.6468 - val_acc: 0.6719\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6639 - acc: 0.6250\n",
            "Epoch 00046: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6476 - val_acc: 0.6719\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6660 - acc: 0.6200\n",
            "Epoch 00047: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6486 - val_acc: 0.6719\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6700 - acc: 0.6100\n",
            "Epoch 00048: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6485 - val_acc: 0.6719\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6731 - acc: 0.6000\n",
            "Epoch 00049: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6478 - val_acc: 0.6719\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6709 - acc: 0.6050\n",
            "Epoch 00050: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6451 - val_acc: 0.6719\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6677 - acc: 0.6150\n",
            "Epoch 00051: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6426 - val_acc: 0.6719\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6698 - acc: 0.6100\n",
            "Epoch 00052: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6734 - acc: 0.6016 - val_loss: 0.6428 - val_acc: 0.6719\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6744 - acc: 0.5950\n",
            "Epoch 00053: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6717 - acc: 0.6016 - val_loss: 0.6433 - val_acc: 0.6719\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6770 - acc: 0.5900\n",
            "Epoch 00054: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6719 - acc: 0.6016 - val_loss: 0.6439 - val_acc: 0.6719\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6623 - acc: 0.6250\n",
            "Epoch 00055: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6425 - val_acc: 0.6719\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6823 - acc: 0.5800\n",
            "Epoch 00056: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6738 - acc: 0.6016 - val_loss: 0.6439 - val_acc: 0.6719\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6807 - acc: 0.5850\n",
            "Epoch 00057: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6737 - acc: 0.6016 - val_loss: 0.6427 - val_acc: 0.6719\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6804 - acc: 0.5850\n",
            "Epoch 00058: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6410 - val_acc: 0.6719\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6856 - acc: 0.5750\n",
            "Epoch 00059: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6731 - acc: 0.6016 - val_loss: 0.6393 - val_acc: 0.6719\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6664 - acc: 0.6150\n",
            "Epoch 00060: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6731 - acc: 0.6016 - val_loss: 0.6387 - val_acc: 0.6719\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6737 - acc: 0.6000\n",
            "Epoch 00061: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6402 - val_acc: 0.6719\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6665 - acc: 0.6150\n",
            "Epoch 00062: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6427 - val_acc: 0.6719\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6831 - acc: 0.5750\n",
            "Epoch 00063: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6462 - val_acc: 0.6719\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6693 - acc: 0.6100\n",
            "Epoch 00064: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6457 - val_acc: 0.6719\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6732 - acc: 0.6000\n",
            "Epoch 00065: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6455 - val_acc: 0.6719\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6776 - acc: 0.5900\n",
            "Epoch 00066: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6437 - val_acc: 0.6719\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6711 - acc: 0.6050\n",
            "Epoch 00067: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6428 - val_acc: 0.6719\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6622 - acc: 0.6250\n",
            "Epoch 00068: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6421 - val_acc: 0.6719\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6687 - acc: 0.6100\n",
            "Epoch 00069: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6723 - acc: 0.6016 - val_loss: 0.6430 - val_acc: 0.6719\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6604 - acc: 0.6300\n",
            "Epoch 00070: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6442 - val_acc: 0.6719\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6746 - acc: 0.5950\n",
            "Epoch 00071: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6723 - acc: 0.6016 - val_loss: 0.6472 - val_acc: 0.6719\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6754 - acc: 0.5950\n",
            "Epoch 00072: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6475 - val_acc: 0.6719\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6644 - acc: 0.6250\n",
            "Epoch 00073: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6461 - val_acc: 0.6719\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6600 - acc: 0.6350\n",
            "Epoch 00074: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6733 - acc: 0.6016 - val_loss: 0.6448 - val_acc: 0.6719\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6690 - acc: 0.6100\n",
            "Epoch 00075: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6723 - acc: 0.6016 - val_loss: 0.6459 - val_acc: 0.6719\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6750 - acc: 0.5950\n",
            "Epoch 00076: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6466 - val_acc: 0.6719\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6658 - acc: 0.6200\n",
            "Epoch 00077: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6458 - val_acc: 0.6719\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6654 - acc: 0.6200\n",
            "Epoch 00078: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6452 - val_acc: 0.6719\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6735 - acc: 0.6000\n",
            "Epoch 00079: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6459 - val_acc: 0.6719\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6845 - acc: 0.5700\n",
            "Epoch 00080: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6449 - val_acc: 0.6719\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6711 - acc: 0.6050\n",
            "Epoch 00081: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6412 - val_acc: 0.6719\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6664 - acc: 0.6150\n",
            "Epoch 00082: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6400 - val_acc: 0.6719\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6757 - acc: 0.5950\n",
            "Epoch 00083: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6408 - val_acc: 0.6719\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6867 - acc: 0.5700\n",
            "Epoch 00084: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6736 - acc: 0.6016 - val_loss: 0.6433 - val_acc: 0.6719\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6603 - acc: 0.6300\n",
            "Epoch 00085: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6422 - val_acc: 0.6719\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6752 - acc: 0.5950\n",
            "Epoch 00086: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6444 - val_acc: 0.6719\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6810 - acc: 0.5800\n",
            "Epoch 00087: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6456 - val_acc: 0.6719\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6674 - acc: 0.6150\n",
            "Epoch 00088: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6432 - val_acc: 0.6719\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6754 - acc: 0.5950\n",
            "Epoch 00089: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6424 - val_acc: 0.6719\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6711 - acc: 0.6050\n",
            "Epoch 00090: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6417 - val_acc: 0.6719\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6582 - acc: 0.6350\n",
            "Epoch 00091: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6425 - val_acc: 0.6719\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6689 - acc: 0.6100\n",
            "Epoch 00092: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6722 - acc: 0.6016 - val_loss: 0.6456 - val_acc: 0.6719\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6716 - acc: 0.6050\n",
            "Epoch 00093: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6482 - val_acc: 0.6719\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6769 - acc: 0.5900\n",
            "Epoch 00094: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6478 - val_acc: 0.6719\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6823 - acc: 0.5750\n",
            "Epoch 00095: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6451 - val_acc: 0.6719\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6687 - acc: 0.6100\n",
            "Epoch 00096: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6407 - val_acc: 0.6719\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6664 - acc: 0.6150\n",
            "Epoch 00097: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6390 - val_acc: 0.6719\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6712 - acc: 0.6050\n",
            "Epoch 00098: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6405 - val_acc: 0.6719\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6734 - acc: 0.6000\n",
            "Epoch 00099: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6430 - val_acc: 0.6719\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6732 - acc: 0.6000\n",
            "Epoch 00100: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6453 - val_acc: 0.6719\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6750 - acc: 0.5950\n",
            "Epoch 00101: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6459 - val_acc: 0.6719\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6737 - acc: 0.6000\n",
            "Epoch 00102: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6439 - val_acc: 0.6719\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6831 - acc: 0.5750\n",
            "Epoch 00103: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6437 - val_acc: 0.6719\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6648 - acc: 0.6200\n",
            "Epoch 00104: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6731 - acc: 0.6016 - val_loss: 0.6406 - val_acc: 0.6719\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6735 - acc: 0.6000\n",
            "Epoch 00105: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6410 - val_acc: 0.6719\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6690 - acc: 0.6100\n",
            "Epoch 00106: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6429 - val_acc: 0.6719\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6793 - acc: 0.5850\n",
            "Epoch 00107: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6444 - val_acc: 0.6719\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6770 - acc: 0.5900\n",
            "Epoch 00108: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6446 - val_acc: 0.6719\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6672 - acc: 0.6150\n",
            "Epoch 00109: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6426 - val_acc: 0.6719\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6730 - acc: 0.6000\n",
            "Epoch 00110: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6431 - val_acc: 0.6719\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6676 - acc: 0.6150\n",
            "Epoch 00111: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6733 - acc: 0.6016 - val_loss: 0.6423 - val_acc: 0.6719\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6774 - acc: 0.5900\n",
            "Epoch 00112: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6452 - val_acc: 0.6719\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6827 - acc: 0.5750\n",
            "Epoch 00113: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6452 - val_acc: 0.6719\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6866 - acc: 0.5650\n",
            "Epoch 00114: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6431 - val_acc: 0.6719\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6692 - acc: 0.6100\n",
            "Epoch 00115: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6732 - acc: 0.6016 - val_loss: 0.6384 - val_acc: 0.6719\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6740 - acc: 0.6000\n",
            "Epoch 00116: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6733 - acc: 0.6016 - val_loss: 0.6383 - val_acc: 0.6719\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6690 - acc: 0.6100\n",
            "Epoch 00117: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6731 - acc: 0.6016 - val_loss: 0.6400 - val_acc: 0.6719\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6640 - acc: 0.6200\n",
            "Epoch 00118: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6723 - acc: 0.6016 - val_loss: 0.6433 - val_acc: 0.6719\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6649 - acc: 0.6200\n",
            "Epoch 00119: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6719 - acc: 0.6016 - val_loss: 0.6486 - val_acc: 0.6719\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6768 - acc: 0.5900\n",
            "Epoch 00120: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6734 - acc: 0.6016 - val_loss: 0.6526 - val_acc: 0.6719\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6730 - acc: 0.6050\n",
            "Epoch 00121: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6739 - acc: 0.6016 - val_loss: 0.6501 - val_acc: 0.6719\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6775 - acc: 0.5900\n",
            "Epoch 00122: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6731 - acc: 0.6016 - val_loss: 0.6444 - val_acc: 0.6719\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6798 - acc: 0.5850\n",
            "Epoch 00123: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6400 - val_acc: 0.6719\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6735 - acc: 0.6000\n",
            "Epoch 00124: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6387 - val_acc: 0.6719\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6665 - acc: 0.6150\n",
            "Epoch 00125: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6732 - acc: 0.6016 - val_loss: 0.6387 - val_acc: 0.6719\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6857 - acc: 0.5750\n",
            "Epoch 00126: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6735 - acc: 0.6016 - val_loss: 0.6414 - val_acc: 0.6719\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6731 - acc: 0.6000\n",
            "Epoch 00127: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6430 - val_acc: 0.6719\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6688 - acc: 0.6100\n",
            "Epoch 00128: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6723 - acc: 0.6016 - val_loss: 0.6442 - val_acc: 0.6719\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6760 - acc: 0.5950\n",
            "Epoch 00129: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6734 - acc: 0.6016 - val_loss: 0.6469 - val_acc: 0.6719\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6681 - acc: 0.6150\n",
            "Epoch 00130: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6732 - acc: 0.6016 - val_loss: 0.6444 - val_acc: 0.6719\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6730 - acc: 0.6000\n",
            "Epoch 00131: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6445 - val_acc: 0.6719\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6691 - acc: 0.6100\n",
            "Epoch 00132: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6434 - val_acc: 0.6719\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6792 - acc: 0.5850\n",
            "Epoch 00133: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6441 - val_acc: 0.6719\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6672 - acc: 0.6150\n",
            "Epoch 00134: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6419 - val_acc: 0.6719\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6775 - acc: 0.5900\n",
            "Epoch 00135: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6426 - val_acc: 0.6719\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6814 - acc: 0.5800\n",
            "Epoch 00136: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6432 - val_acc: 0.6719\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6709 - acc: 0.6050\n",
            "Epoch 00137: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6418 - val_acc: 0.6719\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6861 - acc: 0.5700\n",
            "Epoch 00138: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6421 - val_acc: 0.6719\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6687 - acc: 0.6100\n",
            "Epoch 00139: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6403 - val_acc: 0.6719\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6874 - acc: 0.5700\n",
            "Epoch 00140: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6739 - acc: 0.6016 - val_loss: 0.6422 - val_acc: 0.6719\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6881 - acc: 0.5650\n",
            "Epoch 00141: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6016 - val_loss: 0.6414 - val_acc: 0.6719\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6578 - acc: 0.6350\n",
            "Epoch 00142: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6747 - acc: 0.6016 - val_loss: 0.6378 - val_acc: 0.6719\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6815 - acc: 0.5850\n",
            "Epoch 00143: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6736 - acc: 0.6016 - val_loss: 0.6410 - val_acc: 0.6719\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6709 - acc: 0.6050\n",
            "Epoch 00144: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6016 - val_loss: 0.6430 - val_acc: 0.6719\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6671 - acc: 0.6150\n",
            "Epoch 00145: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.6016 - val_loss: 0.6445 - val_acc: 0.6719\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6715 - acc: 0.6050\n",
            "Epoch 00146: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6016 - val_loss: 0.6482 - val_acc: 0.6719\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6628 - acc: 0.6300\n",
            "Epoch 00147: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6732 - acc: 0.6016 - val_loss: 0.6474 - val_acc: 0.6719\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6718 - acc: 0.6050\n",
            "Epoch 00148: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6488 - val_acc: 0.6719\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6735 - acc: 0.6000\n",
            "Epoch 00149: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6465 - val_acc: 0.6719\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6806 - acc: 0.5800\n",
            "Epoch 00150: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6725 - acc: 0.6016 - val_loss: 0.6440 - val_acc: 0.6719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nszAMwzCKYs7CMAzDKMr/B1vGvgHneG3NAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.6015625,validation accuracy: 0.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFxCueOl3yTZ",
        "colab_type": "code",
        "outputId": "390abdd1-b59c-47f6-b310-db4437fb3c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1cugYbE7nqv",
        "colab_type": "code",
        "outputId": "d7673c83-b090-445d-9cad-c73b4b9216d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 10 \n",
        "# applied sgd optimizer\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='sgd',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('10'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7913 - acc: 0.5800\n",
            "Epoch 00001: val_acc improved from -inf to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.67188-0.58203.hdf5\n",
            "256/256 [==============================] - 10s 38ms/sample - loss: 0.8062 - acc: 0.5820 - val_loss: 0.6839 - val_acc: 0.6719\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8355 - acc: 0.5750\n",
            "Epoch 00002: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.8649 - acc: 0.5625 - val_loss: 0.6950 - val_acc: 0.3281\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8858 - acc: 0.5250\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.8543 - acc: 0.5430 - val_loss: 0.6901 - val_acc: 0.6719\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8205 - acc: 0.6050\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.8454 - acc: 0.5820 - val_loss: 0.6778 - val_acc: 0.6719\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8167 - acc: 0.5700\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.8884 - acc: 0.5430 - val_loss: 0.6882 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8257 - acc: 0.5650\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7918 - acc: 0.5859 - val_loss: 0.6793 - val_acc: 0.6719\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7004 - acc: 0.6050\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6904 - acc: 0.6055 - val_loss: 0.6768 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7213 - acc: 0.6050\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7031 - acc: 0.5977 - val_loss: 0.6687 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6947 - acc: 0.6100\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7132 - acc: 0.5938 - val_loss: 0.6664 - val_acc: 0.6719\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6254 - acc: 0.6600\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6206 - acc: 0.6758 - val_loss: 0.6593 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7205 - acc: 0.5500\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7252 - acc: 0.5508 - val_loss: 0.6686 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6062 - acc: 0.6850\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6399 - acc: 0.6602 - val_loss: 0.6604 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6133 - acc: 0.6800\n",
            "Epoch 00013: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6062 - acc: 0.6758 - val_loss: 0.6478 - val_acc: 0.6719\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6882 - acc: 0.6150\n",
            "Epoch 00014: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6776 - acc: 0.6172 - val_loss: 0.6463 - val_acc: 0.6719\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5870 - acc: 0.6950\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5994 - acc: 0.6758 - val_loss: 0.6516 - val_acc: 0.6719\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6065 - acc: 0.6800\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6084 - acc: 0.6562 - val_loss: 0.6459 - val_acc: 0.6719\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5823 - acc: 0.7100\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5903 - acc: 0.7031 - val_loss: 0.6376 - val_acc: 0.6719\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6172 - acc: 0.6650\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5912 - acc: 0.6836 - val_loss: 0.6352 - val_acc: 0.6719\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5372 - acc: 0.7400\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5533 - acc: 0.7266 - val_loss: 0.6359 - val_acc: 0.6719\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5598 - acc: 0.7050\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5767 - acc: 0.6836 - val_loss: 0.6303 - val_acc: 0.6719\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6102 - acc: 0.6400\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5996 - acc: 0.6680 - val_loss: 0.6308 - val_acc: 0.6719\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5006 - acc: 0.7600\n",
            "Epoch 00022: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5080 - acc: 0.7578 - val_loss: 0.6303 - val_acc: 0.6719\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5094 - acc: 0.7250\n",
            "Epoch 00023: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5089 - acc: 0.7227 - val_loss: 0.6297 - val_acc: 0.6719\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5417 - acc: 0.7150\n",
            "Epoch 00024: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5413 - acc: 0.7266 - val_loss: 0.6308 - val_acc: 0.6719\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4894 - acc: 0.7600\n",
            "Epoch 00025: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5127 - acc: 0.7422 - val_loss: 0.6287 - val_acc: 0.6719\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5446 - acc: 0.7050\n",
            "Epoch 00026: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5436 - acc: 0.7031 - val_loss: 0.6307 - val_acc: 0.6719\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5180 - acc: 0.7200\n",
            "Epoch 00027: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5090 - acc: 0.7383 - val_loss: 0.6427 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4576 - acc: 0.7700\n",
            "Epoch 00028: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4663 - acc: 0.7656 - val_loss: 0.6379 - val_acc: 0.6719\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4814 - acc: 0.7350\n",
            "Epoch 00029: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4688 - acc: 0.7461 - val_loss: 0.6443 - val_acc: 0.6719\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4974 - acc: 0.7550\n",
            "Epoch 00030: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4929 - acc: 0.7500 - val_loss: 0.6321 - val_acc: 0.6719\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4865 - acc: 0.7700\n",
            "Epoch 00031: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4786 - acc: 0.7773 - val_loss: 0.6402 - val_acc: 0.6719\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3997 - acc: 0.8000\n",
            "Epoch 00032: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4305 - acc: 0.7930 - val_loss: 0.6430 - val_acc: 0.6719\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4385 - acc: 0.8100\n",
            "Epoch 00033: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4664 - acc: 0.7812 - val_loss: 0.6480 - val_acc: 0.6719\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4279 - acc: 0.8050\n",
            "Epoch 00034: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4378 - acc: 0.8008 - val_loss: 0.6566 - val_acc: 0.6719\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4135 - acc: 0.8000\n",
            "Epoch 00035: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4132 - acc: 0.8008 - val_loss: 0.6625 - val_acc: 0.6719\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4188 - acc: 0.8150\n",
            "Epoch 00036: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4300 - acc: 0.7930 - val_loss: 0.6690 - val_acc: 0.6719\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4186 - acc: 0.7950\n",
            "Epoch 00037: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4137 - acc: 0.7969 - val_loss: 0.6881 - val_acc: 0.6719\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4470 - acc: 0.7750\n",
            "Epoch 00038: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4162 - acc: 0.8047 - val_loss: 0.6733 - val_acc: 0.6719\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4147 - acc: 0.7950\n",
            "Epoch 00039: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4289 - acc: 0.7969 - val_loss: 0.7144 - val_acc: 0.6719\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3781 - acc: 0.8500\n",
            "Epoch 00040: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4087 - acc: 0.8281 - val_loss: 0.7056 - val_acc: 0.6719\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4002 - acc: 0.8200\n",
            "Epoch 00041: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4163 - acc: 0.8086 - val_loss: 0.6603 - val_acc: 0.6719\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3513 - acc: 0.8500\n",
            "Epoch 00042: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3919 - acc: 0.8281 - val_loss: 0.7039 - val_acc: 0.6719\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3955 - acc: 0.8350\n",
            "Epoch 00043: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4042 - acc: 0.8281 - val_loss: 0.7067 - val_acc: 0.6719\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3494 - acc: 0.8700\n",
            "Epoch 00044: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3671 - acc: 0.8438 - val_loss: 0.6918 - val_acc: 0.6719\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3448 - acc: 0.8550\n",
            "Epoch 00045: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3597 - acc: 0.8438 - val_loss: 0.6814 - val_acc: 0.6719\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3797 - acc: 0.8300\n",
            "Epoch 00046: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3763 - acc: 0.8320 - val_loss: 0.6958 - val_acc: 0.6719\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3781 - acc: 0.8300\n",
            "Epoch 00047: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3740 - acc: 0.8398 - val_loss: 0.7231 - val_acc: 0.6719\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3882 - acc: 0.8150\n",
            "Epoch 00048: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3728 - acc: 0.8242 - val_loss: 0.7124 - val_acc: 0.6719\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3922 - acc: 0.8200\n",
            "Epoch 00049: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3792 - acc: 0.8203 - val_loss: 0.7397 - val_acc: 0.6719\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3466 - acc: 0.8400\n",
            "Epoch 00050: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3407 - acc: 0.8477 - val_loss: 0.6723 - val_acc: 0.6719\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3558 - acc: 0.8600\n",
            "Epoch 00051: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3619 - acc: 0.8555 - val_loss: 0.6869 - val_acc: 0.6719\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3223 - acc: 0.8650\n",
            "Epoch 00052: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3658 - acc: 0.8516 - val_loss: 0.7027 - val_acc: 0.6719\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3481 - acc: 0.8250\n",
            "Epoch 00053: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3392 - acc: 0.8359 - val_loss: 0.7360 - val_acc: 0.6719\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3332 - acc: 0.8500\n",
            "Epoch 00054: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3188 - acc: 0.8633 - val_loss: 0.7172 - val_acc: 0.6719\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3374 - acc: 0.8700\n",
            "Epoch 00055: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3160 - acc: 0.8750 - val_loss: 0.7336 - val_acc: 0.6719\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3791 - acc: 0.8450\n",
            "Epoch 00056: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3609 - acc: 0.8555 - val_loss: 0.7377 - val_acc: 0.6719\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2876 - acc: 0.8750\n",
            "Epoch 00057: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2901 - acc: 0.8633 - val_loss: 0.7222 - val_acc: 0.6719\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3295 - acc: 0.8600\n",
            "Epoch 00058: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3096 - acc: 0.8750 - val_loss: 0.7873 - val_acc: 0.6719\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2859 - acc: 0.8750\n",
            "Epoch 00059: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2973 - acc: 0.8672 - val_loss: 0.7523 - val_acc: 0.6719\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2494 - acc: 0.9150\n",
            "Epoch 00060: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2691 - acc: 0.8984 - val_loss: 0.7710 - val_acc: 0.6719\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2919 - acc: 0.8700\n",
            "Epoch 00061: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2777 - acc: 0.8828 - val_loss: 0.7905 - val_acc: 0.6719\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3192 - acc: 0.8650\n",
            "Epoch 00062: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3039 - acc: 0.8672 - val_loss: 0.7643 - val_acc: 0.6719\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3412 - acc: 0.8600\n",
            "Epoch 00063: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3287 - acc: 0.8711 - val_loss: 0.7983 - val_acc: 0.6719\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3419 - acc: 0.8650\n",
            "Epoch 00064: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3360 - acc: 0.8594 - val_loss: 0.8034 - val_acc: 0.6719\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3155 - acc: 0.8550\n",
            "Epoch 00065: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3209 - acc: 0.8633 - val_loss: 0.7266 - val_acc: 0.6719\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2440 - acc: 0.9100\n",
            "Epoch 00066: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2592 - acc: 0.9023 - val_loss: 0.8329 - val_acc: 0.6719\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3493 - acc: 0.8600\n",
            "Epoch 00067: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3225 - acc: 0.8711 - val_loss: 0.8243 - val_acc: 0.6719\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2096 - acc: 0.9200\n",
            "Epoch 00068: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2370 - acc: 0.9062 - val_loss: 0.7550 - val_acc: 0.6719\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2997 - acc: 0.8600\n",
            "Epoch 00069: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2698 - acc: 0.8828 - val_loss: 0.7557 - val_acc: 0.6719\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2934 - acc: 0.8600\n",
            "Epoch 00070: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2698 - acc: 0.8711 - val_loss: 0.7457 - val_acc: 0.6719\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2927 - acc: 0.8850\n",
            "Epoch 00071: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2881 - acc: 0.8867 - val_loss: 0.7567 - val_acc: 0.6719\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2974 - acc: 0.8650\n",
            "Epoch 00072: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3000 - acc: 0.8750 - val_loss: 0.7533 - val_acc: 0.6719\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2560 - acc: 0.8950\n",
            "Epoch 00073: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2670 - acc: 0.8945 - val_loss: 0.7249 - val_acc: 0.6719\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2426 - acc: 0.9200\n",
            "Epoch 00074: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2542 - acc: 0.9102 - val_loss: 0.8765 - val_acc: 0.6719\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2821 - acc: 0.8750\n",
            "Epoch 00075: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2748 - acc: 0.8906 - val_loss: 0.8137 - val_acc: 0.6719\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2611 - acc: 0.8800\n",
            "Epoch 00076: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2530 - acc: 0.8867 - val_loss: 0.8247 - val_acc: 0.6719\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2440 - acc: 0.9100\n",
            "Epoch 00077: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2536 - acc: 0.8984 - val_loss: 0.7386 - val_acc: 0.6719\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2079 - acc: 0.9150\n",
            "Epoch 00078: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2240 - acc: 0.9141 - val_loss: 0.8076 - val_acc: 0.6719\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2448 - acc: 0.8950\n",
            "Epoch 00079: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2453 - acc: 0.8984 - val_loss: 0.7724 - val_acc: 0.6719\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2431 - acc: 0.8900\n",
            "Epoch 00080: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2556 - acc: 0.8828 - val_loss: 0.6786 - val_acc: 0.6719\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2702 - acc: 0.9050\n",
            "Epoch 00081: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2987 - acc: 0.8945 - val_loss: 0.7907 - val_acc: 0.6719\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2317 - acc: 0.9050\n",
            "Epoch 00082: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2523 - acc: 0.8984 - val_loss: 0.7454 - val_acc: 0.6719\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2511 - acc: 0.8900\n",
            "Epoch 00083: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2597 - acc: 0.8906 - val_loss: 0.6698 - val_acc: 0.6719\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2869 - acc: 0.8600\n",
            "Epoch 00084: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2586 - acc: 0.8867 - val_loss: 0.7142 - val_acc: 0.6719\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2142 - acc: 0.9250\n",
            "Epoch 00085: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2541 - acc: 0.9141 - val_loss: 0.7999 - val_acc: 0.6719\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2476 - acc: 0.8900\n",
            "Epoch 00086: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2327 - acc: 0.9023 - val_loss: 0.7295 - val_acc: 0.6719\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2144 - acc: 0.9200\n",
            "Epoch 00087: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2105 - acc: 0.9180 - val_loss: 0.7518 - val_acc: 0.6719\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2432 - acc: 0.8850\n",
            "Epoch 00088: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2366 - acc: 0.8945 - val_loss: 0.7535 - val_acc: 0.6719\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2195 - acc: 0.9250\n",
            "Epoch 00089: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2083 - acc: 0.9297 - val_loss: 0.7037 - val_acc: 0.6719\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2027 - acc: 0.9200\n",
            "Epoch 00090: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1919 - acc: 0.9258 - val_loss: 0.7217 - val_acc: 0.6719\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2234 - acc: 0.9150\n",
            "Epoch 00091: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2039 - acc: 0.9297 - val_loss: 0.7886 - val_acc: 0.6719\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1777 - acc: 0.9450\n",
            "Epoch 00092: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1969 - acc: 0.9375 - val_loss: 0.6883 - val_acc: 0.6719\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2173 - acc: 0.8900\n",
            "Epoch 00093: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2117 - acc: 0.8984 - val_loss: 0.7717 - val_acc: 0.6719\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2241 - acc: 0.9100\n",
            "Epoch 00094: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2298 - acc: 0.9062 - val_loss: 0.7388 - val_acc: 0.6719\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1922 - acc: 0.9300\n",
            "Epoch 00095: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1924 - acc: 0.9375 - val_loss: 0.8682 - val_acc: 0.6719\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1705 - acc: 0.9450\n",
            "Epoch 00096: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1834 - acc: 0.9336 - val_loss: 0.7798 - val_acc: 0.6719\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1782 - acc: 0.9200\n",
            "Epoch 00097: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1816 - acc: 0.9219 - val_loss: 0.8261 - val_acc: 0.6719\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2156 - acc: 0.9150\n",
            "Epoch 00098: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2369 - acc: 0.9141 - val_loss: 0.7820 - val_acc: 0.6719\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1667 - acc: 0.9450\n",
            "Epoch 00099: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1788 - acc: 0.9375 - val_loss: 0.8393 - val_acc: 0.6719\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1977 - acc: 0.9250\n",
            "Epoch 00100: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1982 - acc: 0.9297 - val_loss: 0.8022 - val_acc: 0.6719\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1438 - acc: 0.9400\n",
            "Epoch 00101: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1442 - acc: 0.9414 - val_loss: 0.8578 - val_acc: 0.6719\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1837 - acc: 0.9200\n",
            "Epoch 00102: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1986 - acc: 0.9062 - val_loss: 0.7333 - val_acc: 0.6719\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1656 - acc: 0.9450\n",
            "Epoch 00103: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1581 - acc: 0.9492 - val_loss: 0.7790 - val_acc: 0.6719\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1655 - acc: 0.9350\n",
            "Epoch 00104: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1722 - acc: 0.9336 - val_loss: 0.8451 - val_acc: 0.6719\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1572 - acc: 0.9250\n",
            "Epoch 00105: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1641 - acc: 0.9219 - val_loss: 0.8553 - val_acc: 0.6719\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1410 - acc: 0.9350\n",
            "Epoch 00106: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1583 - acc: 0.9375 - val_loss: 0.7269 - val_acc: 0.6719\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1972 - acc: 0.9100\n",
            "Epoch 00107: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1964 - acc: 0.9141 - val_loss: 0.7790 - val_acc: 0.6719\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1580 - acc: 0.9350\n",
            "Epoch 00108: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1561 - acc: 0.9414 - val_loss: 0.7724 - val_acc: 0.6719\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1096 - acc: 0.9550\n",
            "Epoch 00109: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1421 - acc: 0.9375 - val_loss: 0.7821 - val_acc: 0.6719\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1744 - acc: 0.9400\n",
            "Epoch 00110: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1636 - acc: 0.9453 - val_loss: 0.7603 - val_acc: 0.6719\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1421 - acc: 0.9350\n",
            "Epoch 00111: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1735 - acc: 0.9219 - val_loss: 0.7760 - val_acc: 0.6719\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1569 - acc: 0.9300\n",
            "Epoch 00112: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1411 - acc: 0.9414 - val_loss: 0.7684 - val_acc: 0.6719\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1724 - acc: 0.9350\n",
            "Epoch 00113: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-113-0.68750-0.93359.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.1715 - acc: 0.9336 - val_loss: 0.6749 - val_acc: 0.6875\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1643 - acc: 0.9450\n",
            "Epoch 00114: val_acc improved from 0.68750 to 0.71875, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-114-0.71875-0.94922.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.1528 - acc: 0.9492 - val_loss: 0.5697 - val_acc: 0.7188\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1246 - acc: 0.9600\n",
            "Epoch 00115: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1428 - acc: 0.9453 - val_loss: 0.7648 - val_acc: 0.6719\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1521 - acc: 0.9400\n",
            "Epoch 00116: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1450 - acc: 0.9453 - val_loss: 0.6917 - val_acc: 0.6719\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1159 - acc: 0.9600\n",
            "Epoch 00117: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1222 - acc: 0.9531 - val_loss: 0.7096 - val_acc: 0.7031\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1243 - acc: 0.9550\n",
            "Epoch 00118: val_acc improved from 0.71875 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-118-0.73438-0.95312.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.1358 - acc: 0.9531 - val_loss: 0.5678 - val_acc: 0.7344\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1563 - acc: 0.9350\n",
            "Epoch 00119: val_acc improved from 0.73438 to 0.75000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-119-0.75000-0.94922.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.1367 - acc: 0.9492 - val_loss: 0.5737 - val_acc: 0.7500\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1285 - acc: 0.9550\n",
            "Epoch 00120: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1220 - acc: 0.9570 - val_loss: 0.6507 - val_acc: 0.7188\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1331 - acc: 0.9400\n",
            "Epoch 00121: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1221 - acc: 0.9492 - val_loss: 0.6657 - val_acc: 0.7188\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1119 - acc: 0.9550\n",
            "Epoch 00122: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1061 - acc: 0.9648 - val_loss: 0.5916 - val_acc: 0.7344\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1408 - acc: 0.9550\n",
            "Epoch 00123: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1297 - acc: 0.9570 - val_loss: 0.6788 - val_acc: 0.7188\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1743 - acc: 0.9300\n",
            "Epoch 00124: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1585 - acc: 0.9375 - val_loss: 0.6501 - val_acc: 0.7188\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1363 - acc: 0.9500\n",
            "Epoch 00125: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1320 - acc: 0.9492 - val_loss: 0.6333 - val_acc: 0.7344\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1168 - acc: 0.9550\n",
            "Epoch 00126: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1274 - acc: 0.9531 - val_loss: 0.6171 - val_acc: 0.7344\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0957 - acc: 0.9650\n",
            "Epoch 00127: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1022 - acc: 0.9688 - val_loss: 0.6675 - val_acc: 0.7188\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1072 - acc: 0.9600\n",
            "Epoch 00128: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1177 - acc: 0.9570 - val_loss: 0.6429 - val_acc: 0.7188\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1283 - acc: 0.9600\n",
            "Epoch 00129: val_acc improved from 0.75000 to 0.76562, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-129-0.76562-0.96484.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.1231 - acc: 0.9648 - val_loss: 0.6067 - val_acc: 0.7656\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1151 - acc: 0.9700\n",
            "Epoch 00130: val_acc improved from 0.76562 to 0.79688, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-130-0.79688-0.96094.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.1252 - acc: 0.9609 - val_loss: 0.4624 - val_acc: 0.7969\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0815 - acc: 0.9800\n",
            "Epoch 00131: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0965 - acc: 0.9648 - val_loss: 0.5478 - val_acc: 0.7656\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1308 - acc: 0.9400\n",
            "Epoch 00132: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1365 - acc: 0.9414 - val_loss: 0.5646 - val_acc: 0.7656\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1204 - acc: 0.9750\n",
            "Epoch 00133: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1128 - acc: 0.9766 - val_loss: 0.5224 - val_acc: 0.7656\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1110 - acc: 0.9650\n",
            "Epoch 00134: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1057 - acc: 0.9688 - val_loss: 0.5622 - val_acc: 0.7500\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0811 - acc: 0.9800\n",
            "Epoch 00135: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0954 - acc: 0.9688 - val_loss: 0.6200 - val_acc: 0.7188\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1123 - acc: 0.9550\n",
            "Epoch 00136: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1008 - acc: 0.9609 - val_loss: 0.6046 - val_acc: 0.7344\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0769 - acc: 0.9850\n",
            "Epoch 00137: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0854 - acc: 0.9766 - val_loss: 0.5520 - val_acc: 0.7969\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0814 - acc: 0.9750\n",
            "Epoch 00138: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0753 - acc: 0.9766 - val_loss: 0.5011 - val_acc: 0.7969\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1001 - acc: 0.9600\n",
            "Epoch 00139: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1159 - acc: 0.9570 - val_loss: 0.6733 - val_acc: 0.7031\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0923 - acc: 0.9700\n",
            "Epoch 00140: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1014 - acc: 0.9609 - val_loss: 0.5813 - val_acc: 0.7500\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1308 - acc: 0.9650\n",
            "Epoch 00141: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1123 - acc: 0.9727 - val_loss: 0.5815 - val_acc: 0.7969\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0864 - acc: 0.9700\n",
            "Epoch 00142: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0950 - acc: 0.9609 - val_loss: 0.5381 - val_acc: 0.7812\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1015 - acc: 0.9750\n",
            "Epoch 00143: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0941 - acc: 0.9727 - val_loss: 0.6307 - val_acc: 0.7344\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0758 - acc: 0.9700\n",
            "Epoch 00144: val_acc improved from 0.79688 to 0.81250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-144-0.81250-0.97266.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0730 - acc: 0.9727 - val_loss: 0.5129 - val_acc: 0.8125\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0971 - acc: 0.9500\n",
            "Epoch 00145: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0971 - acc: 0.9531 - val_loss: 0.5134 - val_acc: 0.7969\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0967 - acc: 0.9700\n",
            "Epoch 00146: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0940 - acc: 0.9688 - val_loss: 0.6562 - val_acc: 0.7500\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0623 - acc: 0.9900\n",
            "Epoch 00147: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0647 - acc: 0.9844 - val_loss: 0.6283 - val_acc: 0.7812\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0732 - acc: 0.9850\n",
            "Epoch 00148: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0714 - acc: 0.9805 - val_loss: 0.6297 - val_acc: 0.7812\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1004 - acc: 0.9650\n",
            "Epoch 00149: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1000 - acc: 0.9648 - val_loss: 0.7124 - val_acc: 0.7344\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0710 - acc: 0.9850\n",
            "Epoch 00150: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0727 - acc: 0.9805 - val_loss: 0.5377 - val_acc: 0.7969\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.98046875,validation accuracy: 0.796875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR118s547pLH",
        "colab_type": "code",
        "outputId": "ff5fc90d-909f-418a-ae30-97cb2e755523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBEE-dwD7pIf",
        "colab_type": "code",
        "outputId": "9b9cd463-39ad-4146-93df-b6102ed3ccda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 11\n",
        "# applied rmsprop optimizer\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='rmsprop',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('11'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.9394 - acc: 0.4600\n",
            "Epoch 00001: val_acc improved from -inf to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.67188-0.52344.hdf5\n",
            "256/256 [==============================] - 11s 42ms/sample - loss: 3.0637 - acc: 0.5234 - val_loss: 0.6408 - val_acc: 0.6719\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.2044 - acc: 0.5900\n",
            "Epoch 00002: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 1.0771 - acc: 0.6055 - val_loss: 0.6838 - val_acc: 0.6719\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6652 - acc: 0.5900\n",
            "Epoch 00003: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-03-0.68750-0.60547.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6575 - acc: 0.6055 - val_loss: 0.6865 - val_acc: 0.6875\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6308 - acc: 0.6400\n",
            "Epoch 00004: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6326 - acc: 0.6406 - val_loss: 0.6794 - val_acc: 0.6719\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6247 - acc: 0.6650\n",
            "Epoch 00005: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6181 - acc: 0.6602 - val_loss: 0.6561 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5777 - acc: 0.7100\n",
            "Epoch 00006: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5591 - acc: 0.7305 - val_loss: 0.6671 - val_acc: 0.6719\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5208 - acc: 0.7300\n",
            "Epoch 00007: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5112 - acc: 0.7344 - val_loss: 0.6135 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8534 - acc: 0.6000\n",
            "Epoch 00008: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7781 - acc: 0.6367 - val_loss: 0.6679 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4642 - acc: 0.7850\n",
            "Epoch 00009: val_acc improved from 0.68750 to 0.70312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-09-0.70312-0.78906.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.4623 - acc: 0.7891 - val_loss: 0.6634 - val_acc: 0.7031\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6474 - acc: 0.6850\n",
            "Epoch 00010: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6406 - acc: 0.6797 - val_loss: 0.6402 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3913 - acc: 0.8800\n",
            "Epoch 00011: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3848 - acc: 0.8789 - val_loss: 0.6239 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3752 - acc: 0.8400\n",
            "Epoch 00012: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3978 - acc: 0.8164 - val_loss: 0.6251 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4246 - acc: 0.7700\n",
            "Epoch 00013: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4167 - acc: 0.7891 - val_loss: 0.6559 - val_acc: 0.7031\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3030 - acc: 0.8950\n",
            "Epoch 00014: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3245 - acc: 0.8906 - val_loss: 0.6628 - val_acc: 0.6875\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4006 - acc: 0.8150\n",
            "Epoch 00015: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4437 - acc: 0.7852 - val_loss: 0.6172 - val_acc: 0.6719\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3589 - acc: 0.8250\n",
            "Epoch 00016: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3335 - acc: 0.8438 - val_loss: 0.6170 - val_acc: 0.6875\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2598 - acc: 0.8950\n",
            "Epoch 00017: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2842 - acc: 0.8867 - val_loss: 0.6015 - val_acc: 0.6719\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3103 - acc: 0.8650\n",
            "Epoch 00018: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2940 - acc: 0.8672 - val_loss: 0.6068 - val_acc: 0.6719\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2472 - acc: 0.8800\n",
            "Epoch 00019: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2395 - acc: 0.8867 - val_loss: 0.6395 - val_acc: 0.6406\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2709 - acc: 0.8850\n",
            "Epoch 00020: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3495 - acc: 0.8438 - val_loss: 0.7049 - val_acc: 0.6719\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3490 - acc: 0.8700\n",
            "Epoch 00021: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3278 - acc: 0.8750 - val_loss: 0.6414 - val_acc: 0.6719\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2792 - acc: 0.9000\n",
            "Epoch 00022: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2614 - acc: 0.8945 - val_loss: 0.5913 - val_acc: 0.7031\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1986 - acc: 0.9250\n",
            "Epoch 00023: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1790 - acc: 0.9297 - val_loss: 0.6069 - val_acc: 0.7031\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1641 - acc: 0.9250\n",
            "Epoch 00024: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1626 - acc: 0.9258 - val_loss: 0.6627 - val_acc: 0.6875\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1309 - acc: 0.9500\n",
            "Epoch 00025: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1385 - acc: 0.9453 - val_loss: 0.6777 - val_acc: 0.6875\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2058 - acc: 0.9150\n",
            "Epoch 00026: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2402 - acc: 0.8984 - val_loss: 0.7578 - val_acc: 0.6719\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1777 - acc: 0.9250\n",
            "Epoch 00027: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1700 - acc: 0.9258 - val_loss: 0.7322 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0958 - acc: 0.9700\n",
            "Epoch 00028: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1187 - acc: 0.9688 - val_loss: 0.9965 - val_acc: 0.6719\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1947 - acc: 0.9300\n",
            "Epoch 00029: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2092 - acc: 0.9180 - val_loss: 0.6948 - val_acc: 0.6875\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1732 - acc: 0.9250\n",
            "Epoch 00030: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1901 - acc: 0.9258 - val_loss: 0.8554 - val_acc: 0.6719\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1379 - acc: 0.9300\n",
            "Epoch 00031: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1198 - acc: 0.9414 - val_loss: 1.0164 - val_acc: 0.6719\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0695 - acc: 0.9800\n",
            "Epoch 00032: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0613 - acc: 0.9844 - val_loss: 1.0269 - val_acc: 0.6719\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0713 - acc: 0.9750\n",
            "Epoch 00033: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0626 - acc: 0.9805 - val_loss: 1.5579 - val_acc: 0.6719\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0412 - acc: 0.9900\n",
            "Epoch 00034: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0533 - acc: 0.9805 - val_loss: 1.4745 - val_acc: 0.6719\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1997 - acc: 0.9350\n",
            "Epoch 00035: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2183 - acc: 0.9297 - val_loss: 0.6493 - val_acc: 0.7031\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4428 - acc: 0.8350\n",
            "Epoch 00036: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3869 - acc: 0.8594 - val_loss: 0.9141 - val_acc: 0.7031\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1275 - acc: 0.9550\n",
            "Epoch 00037: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1114 - acc: 0.9609 - val_loss: 1.2456 - val_acc: 0.6719\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0686 - acc: 0.9700\n",
            "Epoch 00038: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0713 - acc: 0.9688 - val_loss: 1.2549 - val_acc: 0.6875\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0645 - acc: 0.9700\n",
            "Epoch 00039: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0686 - acc: 0.9688 - val_loss: 1.8842 - val_acc: 0.6719\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0765 - acc: 0.9600\n",
            "Epoch 00040: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0756 - acc: 0.9648 - val_loss: 1.4348 - val_acc: 0.6719\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0606 - acc: 0.9850\n",
            "Epoch 00041: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0572 - acc: 0.9883 - val_loss: 1.2458 - val_acc: 0.7031\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0324 - acc: 0.9850\n",
            "Epoch 00042: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0435 - acc: 0.9766 - val_loss: 1.2294 - val_acc: 0.6875\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0548 - acc: 0.9800\n",
            "Epoch 00043: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0494 - acc: 0.9844 - val_loss: 1.3111 - val_acc: 0.7031\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0925 - acc: 0.9650\n",
            "Epoch 00044: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0981 - acc: 0.9648 - val_loss: 2.1412 - val_acc: 0.6719\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3168 - acc: 0.8550\n",
            "Epoch 00045: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3003 - acc: 0.8672 - val_loss: 0.7908 - val_acc: 0.6406\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0742 - acc: 0.9600\n",
            "Epoch 00046: val_acc improved from 0.70312 to 0.71875, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-46-0.71875-0.96875.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0619 - acc: 0.9688 - val_loss: 0.9358 - val_acc: 0.7188\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0276 - acc: 0.9900\n",
            "Epoch 00047: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0387 - acc: 0.9844 - val_loss: 1.0846 - val_acc: 0.7188\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0955 - acc: 0.9750\n",
            "Epoch 00048: val_acc improved from 0.71875 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-48-0.73438-0.97656.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0842 - acc: 0.9766 - val_loss: 0.8966 - val_acc: 0.7344\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0389 - acc: 0.9800\n",
            "Epoch 00049: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0458 - acc: 0.9766 - val_loss: 1.1672 - val_acc: 0.7188\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0353 - acc: 0.9850\n",
            "Epoch 00050: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0408 - acc: 0.9844 - val_loss: 1.3114 - val_acc: 0.7031\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0733 - acc: 0.9750\n",
            "Epoch 00051: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0635 - acc: 0.9766 - val_loss: 1.7977 - val_acc: 0.6875\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0223 - acc: 0.9900\n",
            "Epoch 00052: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0233 - acc: 0.9883 - val_loss: 1.7971 - val_acc: 0.6875\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0515 - acc: 0.9750\n",
            "Epoch 00053: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0437 - acc: 0.9805 - val_loss: 1.8091 - val_acc: 0.6875\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0512 - acc: 0.9800\n",
            "Epoch 00054: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0432 - acc: 0.9844 - val_loss: 2.7249 - val_acc: 0.6719\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1241 - acc: 0.9550\n",
            "Epoch 00055: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1313 - acc: 0.9492 - val_loss: 1.2124 - val_acc: 0.5312\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3364 - acc: 0.9100\n",
            "Epoch 00056: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2814 - acc: 0.9219 - val_loss: 1.6139 - val_acc: 0.7031\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0256 - acc: 0.9900\n",
            "Epoch 00057: val_acc did not improve from 0.73438\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0232 - acc: 0.9922 - val_loss: 2.0623 - val_acc: 0.6875\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0793 - acc: 0.9650\n",
            "Epoch 00058: val_acc improved from 0.73438 to 0.75000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-58-0.75000-0.96875.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0686 - acc: 0.9688 - val_loss: 1.4405 - val_acc: 0.7500\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0220 - acc: 0.9900\n",
            "Epoch 00059: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0230 - acc: 0.9922 - val_loss: 1.6592 - val_acc: 0.7188\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0446 - acc: 0.9850\n",
            "Epoch 00060: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0389 - acc: 0.9883 - val_loss: 1.4821 - val_acc: 0.7188\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0273 - acc: 0.9800\n",
            "Epoch 00061: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0242 - acc: 0.9844 - val_loss: 1.6317 - val_acc: 0.7188\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0679 - acc: 0.9750\n",
            "Epoch 00062: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0568 - acc: 0.9805 - val_loss: 1.4166 - val_acc: 0.7188\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0249 - acc: 0.9850\n",
            "Epoch 00063: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0299 - acc: 0.9844 - val_loss: 2.1893 - val_acc: 0.7031\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0491 - acc: 0.9800\n",
            "Epoch 00064: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0878 - acc: 0.9609 - val_loss: 5.9957 - val_acc: 0.6719\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2706 - acc: 0.9350\n",
            "Epoch 00065: val_acc improved from 0.75000 to 0.81250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-65-0.81250-0.94922.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.2194 - acc: 0.9492 - val_loss: 1.8899 - val_acc: 0.8125\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1490 - acc: 0.9500\n",
            "Epoch 00066: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1177 - acc: 0.9609 - val_loss: 0.8421 - val_acc: 0.7812\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0193 - acc: 1.0000\n",
            "Epoch 00067: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0156 - acc: 1.0000 - val_loss: 0.7264 - val_acc: 0.8125\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0451 - acc: 0.9900\n",
            "Epoch 00068: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0368 - acc: 0.9922 - val_loss: 0.6550 - val_acc: 0.7812\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0171 - acc: 0.9950\n",
            "Epoch 00069: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0148 - acc: 0.9961 - val_loss: 0.7360 - val_acc: 0.7812\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0230 - acc: 0.9900\n",
            "Epoch 00070: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0244 - acc: 0.9883 - val_loss: 0.7186 - val_acc: 0.7969\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0175 - acc: 0.9950\n",
            "Epoch 00071: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0143 - acc: 0.9961 - val_loss: 0.7457 - val_acc: 0.7969\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 00072: val_acc improved from 0.81250 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-72-0.82812-1.00000.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0052 - acc: 1.0000 - val_loss: 0.8826 - val_acc: 0.8281\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 00073: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.9177 - val_acc: 0.8125\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0107 - acc: 0.9950\n",
            "Epoch 00074: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0085 - acc: 0.9961 - val_loss: 1.0757 - val_acc: 0.8281\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 00075: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0048 - acc: 1.0000 - val_loss: 1.0446 - val_acc: 0.8125\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0716 - acc: 0.9900\n",
            "Epoch 00076: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0568 - acc: 0.9922 - val_loss: 0.7771 - val_acc: 0.8281\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0067 - acc: 0.9950\n",
            "Epoch 00077: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0216 - acc: 0.9922 - val_loss: 0.7481 - val_acc: 0.8281\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1057 - acc: 0.9750\n",
            "Epoch 00078: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1591 - acc: 0.9531 - val_loss: 2.0544 - val_acc: 0.7188\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1678 - acc: 0.9300\n",
            "Epoch 00079: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1414 - acc: 0.9375 - val_loss: 1.0706 - val_acc: 0.7656\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0180 - acc: 0.9900\n",
            "Epoch 00080: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0170 - acc: 0.9922 - val_loss: 0.6905 - val_acc: 0.8281\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0172 - acc: 0.9950\n",
            "Epoch 00081: val_acc improved from 0.82812 to 0.84375, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-81-0.84375-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0139 - acc: 0.9961 - val_loss: 0.9693 - val_acc: 0.8438\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0116 - acc: 0.9950\n",
            "Epoch 00082: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0130 - acc: 0.9961 - val_loss: 0.9474 - val_acc: 0.8281\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0558 - acc: 0.9850\n",
            "Epoch 00083: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0497 - acc: 0.9844 - val_loss: 1.0342 - val_acc: 0.8438\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0433 - acc: 0.9850\n",
            "Epoch 00084: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0358 - acc: 0.9883 - val_loss: 1.5838 - val_acc: 0.8281\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0094 - acc: 0.9950\n",
            "Epoch 00085: val_acc did not improve from 0.84375\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0149 - acc: 0.9883 - val_loss: 0.9801 - val_acc: 0.8438\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0279 - acc: 0.9800\n",
            "Epoch 00086: val_acc improved from 0.84375 to 0.85938, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-86-0.85938-0.98438.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0226 - acc: 0.9844 - val_loss: 1.3309 - val_acc: 0.8594\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 00087: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0047 - acc: 1.0000 - val_loss: 1.4520 - val_acc: 0.8594\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 00088: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 2.0179 - val_acc: 0.8281\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 00089: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 1.2478 - val_acc: 0.8594\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0012 - acc: 1.0000    \n",
            "Epoch 00090: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 1.3891 - val_acc: 0.8594\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0279 - acc: 0.9900\n",
            "Epoch 00091: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0440 - acc: 0.9883 - val_loss: 3.1651 - val_acc: 0.7344\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2218 - acc: 0.9350\n",
            "Epoch 00092: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2568 - acc: 0.9297 - val_loss: 0.9682 - val_acc: 0.8281\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0577 - acc: 0.9900\n",
            "Epoch 00093: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0458 - acc: 0.9922 - val_loss: 0.7212 - val_acc: 0.8438\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0223 - acc: 0.9900\n",
            "Epoch 00094: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0181 - acc: 0.9922 - val_loss: 0.9203 - val_acc: 0.8594\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 00095: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 1.0290 - val_acc: 0.8125\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 00096: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 1.0119 - val_acc: 0.8125\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0159 - acc: 0.9950\n",
            "Epoch 00097: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0125 - acc: 0.9961 - val_loss: 1.0782 - val_acc: 0.8125\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0494 - acc: 0.9900\n",
            "Epoch 00098: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0564 - acc: 0.9844 - val_loss: 1.2959 - val_acc: 0.7500\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0877 - acc: 0.9700\n",
            "Epoch 00099: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0938 - acc: 0.9688 - val_loss: 1.0941 - val_acc: 0.8594\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0836 - acc: 0.9750\n",
            "Epoch 00100: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0676 - acc: 0.9805 - val_loss: 0.9952 - val_acc: 0.8438\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0119 - acc: 0.9950\n",
            "Epoch 00101: val_acc did not improve from 0.85938\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0097 - acc: 0.9961 - val_loss: 0.7983 - val_acc: 0.8594\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0074 - acc: 0.9950\n",
            "Epoch 00102: val_acc improved from 0.85938 to 0.87500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-102-0.87500-0.99609.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.0059 - acc: 0.9961 - val_loss: 0.8305 - val_acc: 0.8750\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 00103: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 1.5552 - val_acc: 0.8594\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 00104: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 1.0226 - val_acc: 0.8594\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 00105: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 9.4423e-04 - acc: 1.0000 - val_loss: 0.9649 - val_acc: 0.8594\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 00106: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 1.2830 - val_acc: 0.8594\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 00107: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 1.5247 - val_acc: 0.8594\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 00108: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 1.6454 - val_acc: 0.8438\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3317 - acc: 0.9300\n",
            "Epoch 00109: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3957 - acc: 0.9062 - val_loss: 1.5522 - val_acc: 0.7344\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0767 - acc: 0.9650\n",
            "Epoch 00110: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0716 - acc: 0.9688 - val_loss: 1.1294 - val_acc: 0.7812\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0141 - acc: 0.9950\n",
            "Epoch 00111: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0117 - acc: 0.9961 - val_loss: 0.9816 - val_acc: 0.7500\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0172 - acc: 0.9900\n",
            "Epoch 00112: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0142 - acc: 0.9922 - val_loss: 0.9004 - val_acc: 0.7812\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 00113: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0057 - acc: 1.0000 - val_loss: 0.9058 - val_acc: 0.7969\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0067 - acc: 0.9950\n",
            "Epoch 00114: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0124 - acc: 0.9922 - val_loss: 1.1645 - val_acc: 0.8125\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0125 - acc: 0.9950\n",
            "Epoch 00115: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0118 - acc: 0.9961 - val_loss: 1.2308 - val_acc: 0.7812\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0286 - acc: 0.9900\n",
            "Epoch 00116: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0229 - acc: 0.9922 - val_loss: 1.1961 - val_acc: 0.7969\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0260 - acc: 0.9900\n",
            "Epoch 00117: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0229 - acc: 0.9922 - val_loss: 1.1669 - val_acc: 0.8125\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0094 - acc: 1.0000\n",
            "Epoch 00118: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0087 - acc: 1.0000 - val_loss: 0.9339 - val_acc: 0.8438\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0062 - acc: 0.9950    \n",
            "Epoch 00119: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0089 - acc: 0.9922 - val_loss: 0.7740 - val_acc: 0.8281\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0399 - acc: 0.9900\n",
            "Epoch 00120: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0478 - acc: 0.9883 - val_loss: 1.1077 - val_acc: 0.8438\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0267 - acc: 0.9900\n",
            "Epoch 00121: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0222 - acc: 0.9922 - val_loss: 0.8295 - val_acc: 0.8281\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0164 - acc: 0.9950\n",
            "Epoch 00122: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0132 - acc: 0.9961 - val_loss: 0.9260 - val_acc: 0.8438\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0094 - acc: 0.9950    \n",
            "Epoch 00123: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0090 - acc: 0.9961 - val_loss: 0.9267 - val_acc: 0.8438\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 6.6494e-04 - acc: 1.0000\n",
            "Epoch 00124: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 5.2169e-04 - acc: 1.0000 - val_loss: 0.9483 - val_acc: 0.8281\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0028 - acc: 1.0000    \n",
            "Epoch 00125: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 1.0028 - val_acc: 0.8438\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 5.6618e-04 - acc: 1.0000\n",
            "Epoch 00126: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 1.4098 - val_acc: 0.8281\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0131 - acc: 0.9900\n",
            "Epoch 00127: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0163 - acc: 0.9883 - val_loss: 1.5596 - val_acc: 0.8438\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1828 - acc: 0.9700\n",
            "Epoch 00128: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1828 - acc: 0.9688 - val_loss: 1.2344 - val_acc: 0.8281\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0249 - acc: 0.9850\n",
            "Epoch 00129: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0197 - acc: 0.9883 - val_loss: 1.1607 - val_acc: 0.8125\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0416 - acc: 0.9850\n",
            "Epoch 00130: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0608 - acc: 0.9766 - val_loss: 1.3654 - val_acc: 0.8281\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1622 - acc: 0.9750\n",
            "Epoch 00131: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1313 - acc: 0.9766 - val_loss: 1.0315 - val_acc: 0.8281\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0191 - acc: 0.9900\n",
            "Epoch 00132: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0192 - acc: 0.9883 - val_loss: 0.9053 - val_acc: 0.8438\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0251 - acc: 0.9950\n",
            "Epoch 00133: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0197 - acc: 0.9961 - val_loss: 1.0784 - val_acc: 0.8594\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 00134: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 1.0658 - val_acc: 0.8594\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0143 - acc: 0.9950\n",
            "Epoch 00135: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0119 - acc: 0.9961 - val_loss: 1.2585 - val_acc: 0.8438\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 00136: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0088 - acc: 0.9961 - val_loss: 1.4075 - val_acc: 0.8594\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1635 - acc: 0.9500\n",
            "Epoch 00137: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.1303 - acc: 0.9609 - val_loss: 0.9858 - val_acc: 0.8438\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 00138: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 1.1439 - val_acc: 0.8281\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0063 - acc: 0.9950\n",
            "Epoch 00139: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0051 - acc: 0.9961 - val_loss: 1.1934 - val_acc: 0.8281\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 9.9886e-04 - acc: 1.0000\n",
            "Epoch 00140: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 7.8998e-04 - acc: 1.0000 - val_loss: 1.1208 - val_acc: 0.8281\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 00141: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 1.1392 - val_acc: 0.8281\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.5329e-04 - acc: 1.0000\n",
            "Epoch 00142: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 2.8541e-04 - acc: 1.0000 - val_loss: 1.1300 - val_acc: 0.8281\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0037 - acc: 1.0000    \n",
            "Epoch 00143: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0212 - acc: 0.9922 - val_loss: 1.7715 - val_acc: 0.8125\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.1012 - acc: 0.9950\n",
            "Epoch 00144: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0792 - acc: 0.9961 - val_loss: 1.1315 - val_acc: 0.8438\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 00145: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 1.0374 - val_acc: 0.8438\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 00146: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 9.6254e-04 - acc: 1.0000 - val_loss: 1.0273 - val_acc: 0.8438\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 00147: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 9.7626e-04 - acc: 1.0000 - val_loss: 1.0402 - val_acc: 0.8438\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.4446e-04 - acc: 1.0000\n",
            "Epoch 00148: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 2.7343e-04 - acc: 1.0000 - val_loss: 1.0885 - val_acc: 0.8438\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 6.0328e-04 - acc: 1.0000\n",
            "Epoch 00149: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 7.0072e-04 - acc: 1.0000 - val_loss: 1.1211 - val_acc: 0.8438\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.5268e-04 - acc: 1.0000\n",
            "Epoch 00150: val_acc did not improve from 0.87500\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 9.3664e-04 - acc: 1.0000 - val_loss: 1.3202 - val_acc: 0.8281\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4W9Xd+D9H07a8V+LEibPjBDLI\nIAkEwp4BymqhpZMySul+25cOun6le1NaSnkLhZZCSykjhL1HGAkkhGBnO4ljJ95LkjXP749zr4Yt\ny1e25JHcz/Poke7VHUeSfb7nu4WUEhMTExMTEwDLaA/AxMTExGTsYAoFExMTE5MIplAwMTExMYlg\nCgUTExMTkwimUDAxMTExiWAKBRMTExOTCKZQMDExMTGJYAoFExMTE5MIplAwMTExMYlgG+0BpEpp\naamcNm3aaA/DxMTEZFyxadOmFill2WDHjTuhMG3aNDZu3DjawzAxMTEZVwgh9hk5zjQfmZiYmJhE\nMIWCiYmJiUkEUyiYmJiYmEQwhYKJiYmJSQRTKJiYmJiYRMiYUBBC/FUI0SSEeH+A94UQ4vdCiF1C\niPeEEEsyNRYTExMTE2NkUlO4GzgnyfvnArO1x7XAnzI4FhMTExMTA2QsT0FK+bIQYlqSQy4C7pGq\nH+gbQohCIUSFlLIxU2MyMUnE9kPdNHR4ObW6POlxB9o8PPTOQULhMIU5Dj66YipZdiutPT6e2naY\nS5dOxmmzZmSMr+9q4Y09rQnfWzy1kNOqJyQ9v7nbx/1v7ScQClPkUmNPZay+YIj73txPu9tv6PgL\nFk1i9oS8pMd80NBFh8fPCbNKkx7X1NXLG3vbuGBhBUKIQe/d4wvyjzf24fYFI/tsVgsfWzGVklyn\nofH35YXaJhw2CycOMNa6FjfbD3dz1vwJCCHYcbibx99rJLbdcWVxDpcvrYz7DC09Ph54+wC+QMjQ\nOE6fN4FFUwqH9BmMMprJa5OBAzHb9dq+fkJBCHEtSptg6tSpIzI4k6OH3zyzg2dqDrPuC6uZV5Gf\n8JgXtzfxpfs30+kNIARICY9sPsgXTpvNdx95n4bOXg519fLVM+ekfXzhsOTLD2ymqdtH3zlRSsi2\nW3njW6dTkG1PeL6Uki/+81027GmNjP3RLQ386WNLmViQNej9Gzu9fO7v77D5QEe/+ye+HzR09vLL\nyxcNeEyHx88n/vomXd4g6790ErPKcxMeFw5LbrzvXd6qa0OghE0ydjf3cO09G9nd7I4bq5TQ6Q1w\n89r5g3+AGIKhMD97spa/vLIXgC+ePpsvnz4biyX+i/jtszt4eHMDH1o8iRNnlXLzI+/TGwhHxqDL\nBptFcMmSSgC2HOjg+r9vorGz19D3ClCen3VECwXDSCnvAO4AWLZsmRzkcBOTlDjQ7iEUlnzzoa38\n67pV/PbZHWw/1M2frlqKw2bh0S0NfOn+d6memM9jN65makkOT207xNf+tYXP3rORSQVZnDS7lD+9\nuIsLF1Uwqzz5CrmmsYub/vMeTd0+LELwzfOqWbtw4Mluc30HTd0+fvuRxXzouMlx771/sJO1t77K\nfzbV85nV0xOe/+CmejbsaeWWi4/lYyuqeGJrI1/79xYu+MOrPPmlkxKunpu6e7nqzjfp7g0qQQjc\nftUSzjm2YtDv8+zfvEx3b6Df/t89u5P9bR6+f+F8fvpELe2eADl2K9/+71buvXoFP3+yltpD3fzs\nsoVMLswG4F8bD/BWXRuFOXZ+8NgHnDy7jIKceOG37r0GfvX0DnoDIdrcflxOG/dds4ITZkZX9Tfe\n9w7/3niAr501hxyHsWlPSskN/3iHpz84zCdXVeHxh/j9czu578192K0Wzj5mIt+/8BgA9rd5KMi2\n88iWBh7e3MDyaUXc9rEllOcpoRsOSy67/XV+9HgNp8wt59kPDvOdR96nPM/Jui+s5tjJBYbGNBKM\nplA4CEyJ2a7U9pmYJGVfq5vJhdnYrMol1tDhpSjHQbZjaKab+nYvlUXZbD7Qwem/fpEDbV4A/vzS\nbq5cMZXvPvI+S6YW8ferV0TucfYxE5n5+Vweeqeeq1dPRwKn/+olbvrPVr58xhyyHRaOm1LUb0X5\n6JYGvvHgFvKz7Jwyt4zXd7fyu2d3cv6CgU0jT207hM0iEpq3jp1cwJKphdz7xj4+dcK0yP28/hDv\n7m/HFwpzy/oallUVceVypWWfu6CCSYXZXHTba9z/9gE+f+qsftdd/14jOw73cPFxk8lxWPnUCdMG\nNQfpuJxWemJMNzr3v72fxs5e3tzbSn27l+tOnsH0Uhc3PbQ18r1n2S1ccOurfHftfAqy7fx4fQ0r\nphdz89r5XHTba/z0yVp+cskCQK3if/7Udu54eQ/HTs5n+bQisu1Wrl0zMyJUdD6xahrr3mvkkc0N\nXHm8MWvDw5sP8vQHh7np3GquXzMTKSUrZ5Tw5t5W3tnfweNbGyNCob7dy1nzJ3Dh4klsPdjJZ1fP\nwGGLumwtFsFPLlnI+b9/hYtue5UDbV5Wzyrl1iuPo8jlMDSekWI0hcKjwI1CiPuBFUCn6U8wGYzu\n3gBn/uZlvn3ePD55wjR6AyHO/s3LlOc7+fPHlw1ohhiIrt4And4AN5wyk9d2t/LGnlZ+cdlCXtze\nzK0v7OKtujbcviA/u3RBP6EzqzyXb5xTHdn+zvnz+PqD73HV/70JwN+vXsHq2dHVaofHz1ce2Myi\nygJu//hSyvOy+PfGA3z9wffYsKc1bmWrI6Xk6W2HWTWzZEDz0CdWTePLD2zm1V0tnDxH1Tv788u7\n+e2zOwFw2Cz85JIFcQJq0ZRCTpxVwj/e2Md1J8+ICFidp7YdZnZ5Lr/5yOJUvk4AcrPsdHrjNYUO\nj5/Gzl7OX1DBm3tbmVqcw5fOmE2Wzcp/3z3IlvoOfnfFYo6dXMB1927iyw9sBsBps/DjSxYwsyyX\nq1dP546X95DrtHLNyTP4ygObeW1XKx9fWcXNa+fHTcJ9WT6tiOqJedyzYR9XLJ8yqG+i3e3n/62r\n4biphVx70gwAhBBcurSSS5dWctsLu/jFU9tx+4JYLYKmbh9TinM4aXYZJ81OXHNu7sQ8rl8zkz+8\nsIvr18zk62fPxWoxaDcaQTImFIQQ/wROAUqFEPXA9wA7gJTydmA9cB6wC/AAn87UWEzSx5PvNxIK\nw/kLBzcjpEK7289tL+ziC6fN7mceiKW1x48/GOatvW188oRp1DR20e0L4m0N8aHbXuNXH17E2cdM\nNHzfek0rmFKcwx0nTKPLG6A8P4s1c8p4eWczr+xs4YunzRrUJARw+bIpLJpSSH27h8/cvZE9LT1x\nQqGxs5dQWPLZk2ZEzAoXLJrELetruOf1fSyrKuaPL+5ib4sbgDPmTaB6Yh57W9wDmoYAzl0wkR89\n7uCeDXURodDY0UuJy8HtH1/K5MJsJvVZOQN8fOU0rv/7Jp6taeKcY6PfWbvbz1t1bVy/ZsbgX2AC\ncp1WGjq8cftqD3UD8OHlU/jJpQsIh2XEjHPXp5fj9oUoy1NmrHVfWM37BzuRQGVRNhUFauxfP3su\nvkCIv7yyl7+9vg8E/OKyhVy+bAqDIYTgkydM45sPbWXjvnaWTysG4E8v7uak2aX9zDc/eaKGLm+g\nnzDVmVbiAmBfqwenXQmjyqL+33FfvnbWHD6yfApTinMGPXa0yGT00ZWDvC+Bz2fq/iaZ4WdPbqel\nx8ep1WXkOGxIKQ1FhCQi9ty/vraXO1/dS48vyE8vXTjgOR3aCnRLfQcA79V3AvCv61fxg8c+4Lp7\nN/H5U2fy1TONrcLq2z2A+ofOslvJsittoDw/i59fupBHtzRwQwLzykDMmZDH7PJcnDYLB9o8ce+1\n9PgAKI2x4WfZrXxk+RTufGUvl9/+OlvqO6kqycHrD/HI5oaI5nPmvIGji5w2K2fOn8CT7x+K7Gvz\n+CnPz4pMfok4Y145kwqyuPeNujih8FxtE6GwTEm4xpLrtNHTG28+qm3sAmDexDzys+KFfo7DFmfn\nz7JbWZZg3HarhR9cdCwLKwt54O0DfGftPBZWGne6XrR4Et97dBtPbzvE8mnF1Ld7+NmTtby8o4R/\nXrsyclxDh5cHN9XzqROmUz0xceBBVYma1Pe1uslxqrFXFg0+0QshxrRAADOj+aig0xPgpJ8/z0+f\nqCUYCg/5Oh0eP3tb3HT3Bnn43QYAvvHge3zyr28NeM79b+1n9c+ep1WbEHUef6+R1T97gV1N3fiC\nIf751n6y7Bbuf/sAb+5p5b4397P8lmf5oKGr3xhA2XBbenxsOdBBeZ6T46YU8sC1K7li+RRue2E3\nn7n7bTo8fnY393DBra9yy+MfJBxffbta0Sb6hz53QQV/umppRFAYRQhBZVF25No6UaEQb0O+akUV\nYSnZ2dTDbR9dwktfP5XXbzqNq1dPZ1dTD4unFA4aJVSWl0WHNxD5fdvcfopdA2tcoIVprqzitV2t\nEe0ElA+joiCLBUN0frqctrhwUFCaQrHLEdEGhsOlSyv51/WrUhIIoITPcVMK2aCF9r65pw2ADXta\n2Xm4O3LcfW/uRwKfPnHagNfShUJdqyeysJhSPLimMB4whcJRwM6mbg60ebn9pd188q63+Psb+7j/\nrf10evpHiCRDX5XnOKzcs6GOF2qb+Pemel7e2TzgtR7YeID6di+3PF4T2dfm9nPzI+9zsMPLtx56\nnye2HqKlx89vP7KYyqJsPnP323zrv1tp7vbxdl1b3PVibdXv1Xewub6DhZWFCCHIslv56aUL+fHF\nC3h9dwtrb32Vi/7wGlsPdvJEzCo6lgPtHlwOK0VJTFZDYUpxTn+h0K0EWmmfiXFKcQ53fWo5j31h\ndcQsZ7NauHntfP7x2RX88vKBNSed0lwHUkK79ju0u/0U5QzuwLxYi2bStQyPP8jLO5oj8fZDIc9p\no8cfjIvRr2nsYl5F3pCvmS5WzihhW0MXnd4AG/a0kpdlw2G1cM8G1WpAX6CcXj0h6Yo+L8tOaa6D\n/W1uDrR5sVtFxCQ43jGFwlFAQ2cvANetmcHbde185+H3uemhrdz/9v6UrrPlgDLZfOWMOdQe6uaL\n979LfpYNKeGtPpM3qKSjd/d3MKkgi4fePcirO1sAuOVxZa+95qTpvFXXxs2PvM/0UhdnzZ/Ijy9e\nQCAsueGUmeQ4rNS1uuOu2REjfF7Z2cKeZjeLp8SvaD+6Yir3X7uKYEgyo8zFp06YRn27t5+2Anrk\nUU7aJ6vKomwOtPc3HzlsFvKc/a22p8wtZ2ZZfyf5ibNKDfkzSlxK0LRpyWVtHj/FBqJaJhVms2By\nAU9tU0LhqW2H8AXDhkJPB8LlVH8THr9KyAqFJdsPdw9oihlJVs0sUX+ve9t4Y08rJ84sZe2iCh56\np57u3gBPbD1Eq9vPJ1ZVDXqtqhIXdS1KU5hUmD0mncZDwRQKRwG60+/GU2fx7s1n8ta3TyfPaaNR\nExZG2VLfyYwyF1etrKIg2053b5Dbr1qK02Zhw+7+2bZPf3AYgNs/vpTppS5u/Oc7XPiHV/nPO/Vc\nt2YG3zpvHiumF9PdG+SqlVVYLIKT55Tx/vfP5hvnVFNV4mJfa/zEqmsKM8tc/GdTPUDCZJ6lVUW8\n/I1TefiGEyP2cl3TiUUPR003lUU5dHgCcfH6zT0+ynKdGVkt6wKgtcdHMBSm0xswJBQAzj5mApsP\ndHC4q5e/vb6PGWUuVs4Y2BcxGLlZSujpYan7Wt30BsJUTzQW0ppJFk8pxGGz8G9Ng105o5hPrJqG\n268CFX647gNmlLpYPUiWNUBVcQ77Wt0Z+xsaLUyhcBTQ2OElL8tGXpYdl9NGeV4WZflOmrv7r5wH\nQkrJ5gMdLK4sJNth5XsXzOfmtfM5YVYpS6uKEpZgeGrbIaaV5LBgcgG3Xnkcy6qKKcpxcOXxU/nC\nabMRQvDzyxby4WWVfHhZZeQ8PbRwWklOQk0h12ljWVUxXZozc+HkxLZlh82CxSJYMLkAi4DNmqYT\nS327J0NCQV3zYEwUTkuPv58/IV3o1211++n0BpCSFISCEpq/eno7mw908PGVVcMSXLnOeKGgRx4N\nlC0+kmTZrSydWhRZsKyaWcriKYVcc9J0KovU3+q3z5+XMOKoL1UlLho6e9nT3MMUA07m8cK4yGg2\nGR4Nnb1MKoif+MrznDR1G9cUGjt7aenxRVbleqo+wKoZJfz62R10ePwUanbsTm+ADbtbuXr1dIQQ\nHDu5gDs/uazfdatKXPz8ssTlEKaW5PBcjYqE0VXzDq+fgmw7i6YU8sDGA0wvdSUNYQVlzphdnheJ\nWNLp9ATo7g1mJBpEnyTq27wRs0lLt48KA2UlhkKspqCbkIz4FEDlW0wvdfGvjfXkOKxcurRy8JOS\n4NIiiXRnc21jFxZByjkkmWLljBI27Gml2OVgtjamb5+fWvkLgGml6jfu6g2amoLJ2KI3EOJvr9cR\nCieuANLQ4WVSYfxkVJ6XRVMSTeHJ9w/x4/U1/Hh9Da/ubIn4ExKZalZqdto390b9Ci9ubyIYlpx1\nTPJCbcmYVuLCHwrT2BldbXd6AhTm2FlYqfwIiyqNRcgsmlLAe/Wdcc7PAzHhqOlGv2asX6GlxxcX\njppOCnMcWITyKehCwaimIISI/E4XHze5X8hoqkTMR5omV3OomxlluSlHcWWKVTNLAFg5o9iQRjAQ\nVVquAhgLRx0vmELhCOCZDw7zvUe3JTSPgFrlV/RJXirLc9LU5YubJHXe3d/O5/6xibtfq+Pu1+q4\n6v/e5JdPb8duFcyr6G8XXlRZSJY93q+wtb6TLLsq9TBUorHg0Ym1w6uEwtyJeRw3tdCwQ3TRlELa\n3P64iKBk4ajDpdjlINtujdwjHJa0uv2U5mXGfGS1CIpyHLS4/bR7UtMUAC5dUsn0UlfSJDmj9DUf\n1bW4mVnmSnbKiLJoSgGLKgu4aPHkwQ9OwrSS6N/NkaQpmOajIwDdkdzU1d8c5PWrImGTCvpqCk68\ngRBufyjyTwwQCIX55kNbmZCXxTNfPRm71cK3/ruVh945yMLKgoTllh02C8uqiuM0Bbc/SF6WfVgr\nMT1rtK7VHSlZ3OHxUz0xH7vVwn9vONHwtRZpMe2bD3REzEWR+PIMCIVoroK6R7vHTygsM6YpAJTk\nOjTzkXJuG9UUQCXdvfA/p6RlHH2FQrvHT0nu0B3X6cZps/LIjauHfZ3CHAcF2aqkx1hPSEsFUygc\nAUSEQgJzkG56qejrU8hXk1NTVy+5MaGQd76yl9pD3fz540vJ08wIv7p8EafOLU+aQDW1JIeaxmii\nWY8vhGuIBep0JuZn4bBZ4jSFTm9gUB9CIuZOzMNhs7DlQEek/HJ9u5c8p4387Mz8G1QWZUeK67X0\naDkKGRQKxS4HbbGawiDJa5nC5Yz6FMJhSbsnQHEKWst4YlpJDjWN3ZRl8HcdaUyhcASg5yEkchzr\nYad9a9/oiTZN3T5mxAiFv762lzVzyuJKHAghBq1jn+u0xVXG9PiCkclhqFgsgqriHOq0bFspJR2e\nAIUDFIZLht1qYVFlAa/sbImU19hS30FVafpzFHSmFOewaV87kLjERbopyXVS09BFa4+fXKctYw1/\nBiNP8yl0+4J09wYJheWYqwSaLo6ZXICEYWnEYw1TKBwB6NpAohBTPSSyr6NZLzfQV7vo9ASGFDro\nctjwBcMEQ2FsVgtufzAShTIcqkpc7NdqCLn9IYJhSeEQs48vXVLJTQ9t5e26dnIcVt7d38F3zp83\n7DEORGVRNl1aPwJdKKSjzMNAlLgctGqawmhpCaAqm1otArcvSJtHd3qP3ngyyXfXzicwjNIxYxHT\n0XwE0NChawoJzEfae31NP+V5UfORjj8Yxh8Kk+tMfYXp0s5x+0KR55whXKcveq6C0hLUBFOYPbRV\n50WLJ5OfZeNvG+q4d8M+su1WLl86eIXNoaI7sA+2eyMCO5NmhhKXk05vgKbu3lE11wghcDmsuH2h\nlMNjxxtZdmvEzHqkYAqFcY7ebQqgqSuxT6E019nPlFCQbcdhs8RpFx6/Mv8Y7UwVS8S5qF3D7R++\n+QhUBFJvIExTty9S4mIoPgWAbIeqSPrU+4d4ePNBPnTc5CFfywi6A3tPSw8tPX4cVkvG/BcAxVoC\n2+4m96iba/KyVMZ7quGxJqOPKRTGObqTOdtuTagpHEyQowBqNVeWG5/V7NZq1eQOYTKPdS7qz8N1\nNEM0FryuxR0pcTEUn4LOVSurCEmJLxg2VN9mOFRX5JGfZeOF2mZaenyU5DoyWhCuVJt4D3X1jvok\n7HJacfuCtJtCYdxhCoVxju5IXlBZQKvb1680dmNn74BZtOX5zjhBok/oQzH75PYRCh5fKC2awvRS\nJRR2N7uHrSmAEjLnLahgzZyyjJddsFstnD5vAs/VHuZwV29GncxAXK/l0Y720QMPoj4FUyiMF0yh\nMI54d387N/xjU9zEr2sKi6cUIqWqfaMjpaSxw5uw6xYo+3ZsxJI+oQ/FQRzVFEJIKdPmaJ5cmE2u\n08b2Q110eIfnU9C57aNL+Ntnjh/22Ixw9jET6PAEeGNPa8bqHunETryjbT5yaUKh3e3HabOQPUay\nmU0GxxQK44gH3j7A+q2H4qqb6k5mvexDrDmoyxvE7Q/1q3uk019TUOajoazwdUdzjy9IbyBMWA5N\n4+iLxSKYOzGPmkPdEU1hqNFHo8HJc8pw2iwEQplNXIP45j2jvTLPy9I0Bbcq4T3afRRMjGMKhXGE\nXon0UEzEkO5InqxpA7Er/90tPQBMHiAFvzwviw5PAF9QixiKOJqHEH0UUwRNv85QfBOJqJ6YR21j\nFx0eteocKzV0jJDjsEUaufdtrpNu8rPskcKBox3t43Ko7mvtHmPNfkzGDqZQGCc0dnqp0zJ7D8Vo\nCgc7vEwuzIrmHcREID1f04RFqCqmidDDUvVsW918NCxHsz8Y9U2kwXwEUF2RT1dvkNpD3eNKS9A5\nWys2l2lNwWIREQ1htDWFXE1TaHUba/ZjMnYwhcI4IbZfweE4TaGXioLshMloT207xPHTiwe0L5f1\nyVXQo4+G42ju8QUjZqih5DskYp7WnGVjXfuw/QmjwVnzJ7J4SiHLpw29OKBRSsaKUND6NLeZQmHc\nYQqFccKG3a0UZNvJslsimoLuSK4ozMJps1KYY4+Yj/Y097CzqSeuXEVfYktdgCpNAUNzNGfZLVhE\nvPkoXZrCHE0oeAOhjOYVZIqCHDsPf/7ElBvND4WS3LEhFFxOG2GpkidHeywmqWEKhTGM2xfkgwZV\nZO6NPW2smF7MxPysiE9BdyTr/oRyrRw2RFthnpVMKOTHaxduXxAhGFKkiBACl9OG2xeKRjGlyaeQ\nn2WPlCYeTo7C0UCxy4kQKjlxNNE1R38obPoUxhlm7aMxzO0v7ebW53fx4WWV7G/z8KkTptHpDUTM\nRw19KqCW52XRrNXYeWrbIRZMLogIjETopgY9YsntD5Fjtw65uJcem643bHelyXwEUD0xn/p277j0\nKYwks7UuaqPdRD7WL3Wk1j06UjE1hTHMjsPd2K2Cf21UDepXzSxhYkFUU9BzFCq0jGVdU2jo8PLu\n/g7Omp+865nNaiE/y0aXlinsHmZlU5dmR+4ZhhlqIPTmPoXmqjMpN5wyk8e/cNJoDyNOKIx2zoRJ\napiawhhmX6uHNXPKOG9BBZv2tTN3Qh4T87M4rHVM26uVlK7SGnyU5auyFbesr8FhtRjqLFWY44gU\nmnP7h5eF7HLacPtDUd9EmsxHQKTP8WibRcY6NquFUaqYHUfsbz/a2dUmqWEKhTGKlDLSceySJZVc\nskQ1Uy/Pz8IfDNPhCfBBYxflec5IeYOyXCf+UJjH32vkK2fMYWrJ4N2gCnPsdGiagscXHFKOgk6u\nVu8mEsWUhtpHOsdMUkLhSGpmciSj91SAaKE+k/GBKRTGKE3dPnoD4bg+sKC6kYFKYKtt7KY6pn5P\nufberPJcrj9lhqH7FGTbI5nCPcM1HzlstPZ4cPuC2CwCpy191slppS7uu2YFS6ZmPqzTZPiYmsL4\nxfQpjFH0bmN6lVCdiQVqpXyw3cuupp5IDD/A/Ip8SlwOfnrJAsNdtwpzHJHqox7/8Fpo6o5mt6Zx\npLu0wQkzS8dVNvPRTGyQgekHGl+YmsIYRe9LXNVHU5igaQOv727FHwrHVfqcVZ7Lxu+ckdJkXJht\nj/oUfMF+90sF3dHs9ofSVuLCZHyS57RrzzYcadQYTTKP+WuNUfa1ubFZRL+QUj3h7MUdTYCq2R9L\nqqvzwhw7nd4A4fDwK5vmOFW3LY8/SI4pFI5q9GRGM/Jo/GEKhTFKXauHyqJsbNb4n8hhs1Ca62BP\nsxu7VTCjNHdY9ynIthOWqsm6e5g9EHIdNvyhMO3uQFoa7JiMX4QQ5DptplAYhwwqFITiKiHEd7Xt\nqUIIQ8XohRDnCCG2CyF2CSFuSvB+lRDiOSHEe0KIF4UQlal/hCOTfa3ufv4EHd2ENLMsd9iquR7i\n2eHxay00hz6Z6wKlqbs3reGoJuOTXKctkiBpMn4wMqP8EVgFXKltdwO3DXaSEMKqHXcuMB+4Uggx\nv89hvwTukVIuBH4I/MTguI9opJTsa/H0izzS0SOQ0tE5THcCHursRcrh1SvKjQgFX9rqHpmMX85f\nWMEZ85InUJqMPYz8566QUi4RQrwLIKVsF0IYEf/HA7uklHsAhBD3AxcBH8QcMx/4qvb6BeBhwyM/\ngmlz++n2BQfWFLT2mtUT8xK+nwp62YiDWnb0cCqb6tpBd28wbRVSTcYv3z6/7xrQZDxgRFMIaKt+\nCSCEKAPCyU8BYDJwIGa7XtsXyxbgEu31xUCeECJx8f+jiH1tiSOPdHRNoTodmoJmPtJLZgxnhR9r\nejIdzSYm4xMjQuH3wH+BciHELcCrwI/TdP//AdZoWsga4CAQ6nuQEOJaIcRGIcTG5ubmNN167LKv\nNXGOgs78inxcDisLJhcM+14FfTSFYTmaY841Hc0mJuOTQWcAKeU/hBCbgNMBAXxISllj4NoHgSkx\n25XavthrN6BpCkKIXOBSKWVHgjHcAdwBsGzZMmng3uOavS0ehIApxYkrnJ4+r5x3vnum4QS1ZOiO\n5oNar+d0OJr7vjYxMRk/JP1slB5MAAAgAElEQVTP1cxG26SU1UBtitd+G5gthJiOEgZXAB/tc/1S\noE1KGQa+Cfw1xXuMSfzBMFaLGHL54i0HOphZljvgpC+ESItAAHDarOQ4rDSmwXwUrymYQsHEZDyS\n1HwkpQwB24UQU1O9sJQyCNwIPAXUAP+SUm4TQvxQCHGhdtgp2vV3ABOAW1K9z1jk4j++xu+f2zmk\ncwOhMBvr2gbsq5wJCrPtMY7m4VVJTfTaxMRk/GDkP7cI2CaEeAtw6zullBcOfErkmPXA+j77vhvz\n+kHgQcOjHSfUtbgjfoFYNta1YbUIjtOKur1/sJMeX5CVMQJg68FO3P5Q3L5MU5DjoEFr8Tmcyqax\npqd0NtgxMTEZOYwIhZszPoojCFUuIhQpHx3Ldx/ZRlhKnvzyyQB85+H36eoN8PzXTokc88aeVgBW\nzigekfFCfIvL4WgKDqsFm0UQDEvTfGRiMk4x4mh+aSQGcqTgDShh4NGa18dysMNLpzdAm9uPw2Zh\n68FObBaBlDJSs2jD7lbmTsiL9EgYCWJbXOYMY4Wv92nu9AaGdR0TE5PRw0iZi5VCiLeFED1CCL8Q\nIiSE6BqJwY119rd68AXjNQK3Jgw8fTQFjz8YKVH91t5W3q5rIxSW+IJh2rV+Bsqf0D6iWgJEhYLN\nInBYh1c2Q9c0TE3BxGR8YmQG+AOqxMVOIBv4LAbKXBzp+INhzv7ty/zzzf1x+90+TVPwxQuFBi3k\nE5Q28Mbu1pj3lJP3vfoOvIEQq2aObP5eQbZKUHc5bcPugaD7EkxHs4nJ+MTQslBKuQuwSilDUsq7\ngHMyO6yxT28whDcQor7dG7ffrfUndvcxH+kTf16WjTf2tPHGntZIy0L9vTf2tAFw/PSRFQq6ppCO\nhDNdGJiO5iOEoA+e/g50NYz2SExGCCNCwaPVOtoshPi5EOIrBs87ovEHVaWPNrc/br8uFPqajxo7\n1cS/duEkth/uZuvBTtYunKS9p7SI9+o7mFHqoniEK0vqjuZ0rO4j5iNTUzgy2PkMvH4rvH3naI/E\nZIQwMrl/XDvuRlRI6hTg0kwOajwQCGlCwRMvFHRhoAsHnYaOXoSAi49T5Z/CEi5YWIHDaqFBExi7\nm93MKh9ef4ShoGsK6ahXpPsScsy2mUcGtevUc8260R2HyYhhJPpon/ayF/hBZoczfggEVbWN9r6a\ngmY28gXDBEPhSJOchg4vZblOjptaiMthJRCWLKkqYmJBFo0dvQRCYfa1ujlz/siXGs7PTq/5KMtu\n6dccyGQcEgrC9ifAkQst26FlJ5TOHu1RmWQYI9FHJwohnhFC7BBC7NEfIzG4sYw/pDSCvppCrIbg\nCURNSI2dvUwqzMZutXD6vAmcMqeMLLuVioIsGjq87G/zEAhJZpWNgqYQ42geLnMm5DK7fPglvU3G\nAPteg94OOE1LVao1tYWjASOzwP8BXwE2kaCC6dGKP6IpBOL2u2Oijjy+EPlZWmnqTm+k/8HvrliM\n1Mr6TS7M5s29bexu6gFg5iiaj9KhKVy3ZibXrZk57OuYjAFq14EtC5Z8HN67X5mQVn9ltEdlkmGM\nCIVOKeUTGR/JOEP3KfT4gviCoUiBulhNQTclSSlp7Ojl1LnlgEry0iM/KwqzONTVy05dKJQlLped\nSSJCwXQOjz28HdClFRcumg4OrcdGwAttmsJeUAlZA5RRD4fB1wnZRYPfy9MGOVqOjJRQ+zjMPB0c\nLqg+H57/EdS9BtmFA1xAQMkssB1BLTh7O6GzXr0umqa+C1DffzgEzpFfxGWaAWcBIcQS7eULQohf\nAA8BPv19KeU7GR7bmEYXCqC0hYkFmlDwx2sKAB2eAN5AiEmF/UthVxRkEwpLNuxuZUK+k7wse79j\nMk223Up+lo3SEcyiNjHIXedB0zb1et6F8JF71euHb4BtD6nXk5fBNc8lPv/138Orv4Gv7wJrkr+t\nhs1wxynw2Wehchk0vKuE0Wnfid77+R/B3eclH+/KG+CcI6ir7l3nw+Gt6vWsM+EqrVTbQ9cqYXHt\nC6M3tgyRbGn4qz7by2JeS+C09A9n/KCHpIIKS52otciMLW+hawp6dNEk7ZhYJmuC4q26NpZVGVjN\nZQAhBA9//kTK8kyhMKZo3q4EwvHXQlMNNG6Ovte4GaaeABYrtNcNfI2t/1Z+ge5DUDhl4OM6DwAS\n3v+PEgq160BYYY6WklQ2Fz79BLiTNLl68w7Y9l846xawHAGBBi07lUBYfo36Dt9/SGlTFhvseBJC\nfqWtFc8Y7ZGmlQGFgpTy1JEcyHjDH6spxDibe2IdzZpQaNSymSsSaQqFSlD4g+FRCUfVmTEKDm6T\nQdAdu6u/Au/cCy++Cn5PVBAsuBw8rdD0QeLz2/bC4ffV6+7G5ELB74ne8+wfK/9B1QlRcxKo7WQE\nvPDf66DhHSVYxjuR7//L0H1YCdgdTynzWEj7n69ZByd+cfTGmAGMRB99SQiRLxR3CiHeEUKcNRKD\nG8sEQtEGcLEJbB5fKNJcR3c6RzSFwv6aQkVBVFDMNCdmk1hq1sHkpZA/SQsFldC2G1p3gwxD6Ryw\nZ6vJOBG1j0dfD5aR7Fc+LTr2q9V+y3aoXpvaeOecrbSLIyVKqWYdTDpO+WwmHQd5k9Rnq30cckph\nwrHx3/ERghEd7zNSyi7gLKAElcz204yOahwQaz6K1RTc/iClucrRpmsKDR292K2CUld/80x+li2S\nBWwKBZMInQfVirv6fLVdNlc9N2+Hlh3qdelssOdAwEMknC2W2nVQoGkH3Y3J7xfwRF8/eZN61u9t\nlOwimLb6yEh062qAgxuj34HFol7veg52PA3V5yk/y4E3oadpdMeaZowIBb1C2nnAPVLKbTH7jlpi\nHc2xmoLbF6Q8L0t7rTSFxk4vEwuysCRozymEoELzNYym+chkjLFd601VfYF6Lp4JCGXn1oVCiSYU\nAIK98ef3NMH+N2Dxx8DqMKApaE2hpqyAnsNQsTi5uWkg5l0ArTuV8BrP9P3+QQmFoBf83UqLmrcW\nkEectmAkBnGTEOJpYDrwTSFEHhAe5JwjHv8AQsHjD0Um+aim4GVSQX9/gk5FYTaNnb1MyDcdvUcF\n4TA8+z1Y+BGYeKza9+LPYPrJULVKbdc8psxDZXPUtj0LiqqUQLBYoWCqCk/VhYLfo0xJOtufAKSa\npLfcpxzNyfD3qJyE+Rep1W+qpiOduefB+v9RWoqu3QyXwx+oPInTv69W7O11sOGPcNb/A5tTfbYn\n/lcV77Nnw3m/BFeJMqs9/R1Y/VUomKwytJ/5Liz95OBjq1mnwmtjj5u2GrIKIRyE6WvUvYumqc+6\n7NOpfaaO/WpsQf/gx1rtcOYPRsyhbURTuBq4CVgupfQADiDFb+DIQzcf5TptcUKhxxekMMeBw2qJ\nhKc2dfuYkN/fn6Bz6ZLJXL16+rDLVpuMEw5uUqGir/9ebbftgRd/DC//XG1726Hu1f7mm9K5Sii0\n7IiWm9AFQaz5B9REVVgFE45RtvDBzEd+j4rBX3C5ijhafOXQPlvBZCWw0qkpbPknvPY7ZU4DFeX0\n1p9h9/Nqe/N98MHD0FWvwnS3/lvt375eFfLb+Fe1vf91eOM22DBI5X9vO9S9or7/2P9Jqx3W/C+c\n/HUlpIVQwnPPS9CbYouZ2vXwwSMqrLXrYPJH7eOw8a7Urj8MjNQ+CgPvxGy3Aq0Dn3F0oJuPyvOd\ncT4Fjz+Ey2kl22HFo0UitfX4KckdOKHnosWTMztYk7FF7WPqeceTEApEzQ97X1bJajueAhmKN12A\nEgR7XwJhgaUnqn0RoRDjbO7tgj0vqlBWISC/AhrfSz4mv1sJhdxy+OgDw/t8DlfUHJUOdHNZzWPK\n8a5/fzXrYO65SgBOOg6ufRFuW6G2V14f9W3UroPTb45ub18P4d8ojSsRO59R2kDf7x9g1Q3x2/Mu\ngA1/gJ1Pw4LLUvhM21XC4fWvxAueRNx7sfoMZ/5w8GPTwBEQTDw66EJhQl4WbTGlLnp8QVwOGy6H\nFbc/hC8YotsXpGSEy2GbjFGkVJOTs0Bly9a9Et0OB9XkUvMY5FWoiS6WsrnKdxDwxGgKeoZzjKaw\n61kVMqmbgHRNIZEzWifgBnuasukdrv6ay3DQhULtOji0VZlenAVqcu/YrzQv/bNWr1U1m7oa1OTu\nLIDmWuWLqX1cbbubof7tge9X8xjkTlQCaDAql4OrLPWIq5adSvMzMslXr1XaZFNNavcYIqZQGCK6\n+WhiQVakUmogFMYfDONy2shx2vD4gxHT0kj2XDYZwzRvV2Gla76hJvSNdykb/srrIXeCMn3sek6Z\nLvomgJXO6f86kaZQu06FTE45Xm3nV6hJurdz4HHpmkI6cOSkT1MI+pQPIa8CWnfBq79WmtIZ3wVv\nm/IRgFqxg3L+yjA89S3lED7z+2r/C7co89Kp3wKLXU38iQh4lVCtPs9YAp7FqvwoO59RYzVK8/b4\n3zMZ1ecDYsQc2kbyFGYKIZza61OEEF8UQgxU/OSowa/lKZTnOWnz+JFSRspa5DisSlPwhWjtUUJh\npBvnmIxRdNPHsZfCrNOh5lGUQ/hCbXJ5WkW4JHL0xgkFzQHaV1MI+qIhk7p5JK9CPSfzK/g90bpK\nw8WRG02GGy56TsZKzWyz7b8wZSUsvAKsTrVdMjvqEK5YDPmVar8jV0VfVSxW28ICCz8MM9YowZlI\nc9rzovouU3G0z7tAOer3vGTseG87uJuiQQSDkTdRaSS1AwiyNGNEU/gPEBJCzALuQDXZuS+joxoH\nBEJh7FZBscuBPxjG7Q9FylrkOm3kOJSm0KppCqVJfAom44RwKD6KR8posTSdw9ugfqN6TjTp1KxT\n/+D5FdGJR3cIz9O2swpUpEtfcoqVBpBVCK5Sta+vprD3ZS1kMsYenq86/CUNS/W71SSaDhyuaDLc\ncNFNRzPWqBpPoFbOzlyYeWp0W0eI6PbsM1WEkP49V52ovsPqtUr7OLwtel7nQfW7bb5PmZimnWR8\njNNPBkcebP6HusZAj7a92mfapZ6Nagr6Z2zcosxlGcaIUAhLKYPAxcCtUsqvAxWZHdbYxx8M47Ba\nIhpAu9sfCUHNcdpwOa14/CFae5RKWZwgcc1knLH5Pvjtwujkuu0h+O0CFTIJapX5pxPgztPV8/4N\n8ed72lTNIr2e0JyzVRjoMR9Sk9m0k1UCWPXagYvXVSyEiQuitmh7TNVUfQy2LDVR6eRNVM/JNIWA\nO3qt4aIn1KWD2JyM+RepjGldeM7/kPZ8Ufw5x3yoz/sXAiJ63NzziDPHBHqjv1vNo8p5nUqlV5sT\n5p6jIqDuPH3gxx+Wq0VFixaZlYpQ0M1jI2BCMpKnEBBCXAl8EtCXHyNfynOMEQiFsduiQqHN7Udf\nF7ocVk1TCMX4FExNYdzT8C6EfOof8/hrVIE0GVahhRPmR00WZ/4QHv+qSgKLRc98LZ6unrOL4HOv\nR1fyNgdc+1LyMtcX3xG/3Tck1dOmHJ/2mBBo3XzUlcx8lE6fQm76fAotO6I5GSs/B7PPUrkBoPI8\nKhYqLSuWqhPU91o+X22XzYUbNkQn4bwJyt9S+xic8r8qoqu3Q/1u5ccMrW7Teb9Q4xmIniZ45Ab1\nt9OxTyUUFlYZv37JTPjwPalpMEPEiFD4NHA9cIuUcq8QYjpwb2aHNfZR5iMLRbpQ8Phxai0oXZqm\n4PYFaenxY7cK8sxeBeOf2CiYxR9TDmF9e803VOz5rDOUrwD6T4weLZI7pyS6r6RPQ6KiQSaK3LL4\n7b7mI18XOPP6H5NdBN3JzEee9DuapRx+CGVsTobVDuXV0fcslv4CQafv/vJ58dvVa+GZm5UZqXad\nMv+suF6t+odCdpEyVw2ElPDKL5VQsDlVhro1xTmhr0aUIQY1H0kpP5BSflFK+U9te6+U8meZH9rY\nxh+UynyUEzUf6RVSXQ5bjKbgo8TlNBPTjgRadgBCJZZte0g5hGefrSqRbn1QOQ/nXRC1zfd1tiYS\nCsOlr6PZ35PYN5A3aWBNQUrtvDSGpCIHLtRnlHBYhW6mKzM6Ft3vUPOYyv6ec9bQBYIR9ES3vS8r\njdOok3kUGEqP5r1mj2ZV5sJhi9EU3H48Wgazy6lFH/mVpmCajo4AvB3KHDTvApVP8Mz3lMP37B+r\n95/SQh1nnxlTeqKPs9XTop7TKRRsTkDEaArd/TUFUI7tgXwKAS8g0+hT0LuTDdOv0HUwPicjnZTM\nVOalV3+r8hZSLf43FKrXQjigfodU/AkjjBFH8/8BvwZWA8tRzXaWZ3JQ44FAUEUf5WfZcNgsHOrs\njUQf6XkKUsLBdq8Zjnok0LJTPS+6QtnoPS3KYVw6S5VQ9rQo525WgWbSEf0nxUxoCkLEO3Z9PYlb\nROYlEwrauemMPoLhRyBFqsFmaAKtXqt+N6tDdVXLNJXLwaVa8o53odAppXxCStkkpWzVHxkf2Rgn\noGkKQghml+ey/XB3pD+zy6kymgEOtHvMNpdjja0Pwjv3RLdr18PrtyY/R5+gyqq16BWiUTB6yKO+\nLUTiUg+eNmW7TreZwpETryk4EmkKk5Sz828XRh///rTKa9An73T6FMB4rkLt46q2UV8iQiED5iOI\nagfT10BWfmbuEYvFovJHYEwLBSOeDrNHcwL8mqMZoHpiPi/vbGbJVBU1km1X0UegaiGZmsIYY9Pd\nKr9gySfU9lt3qDjyVTcO7Bht2R6NGDn+WjX5zjpDvbfk42oCO+bi6PEJhUJrfCezdGHPjvcpJDIf\nzTkH6l6LZt1621XUzUlfVWGekN7kNTAegfT8j6B9H5zwxfjvf89Lqh+EnpORbioWwbKr43+3TLPi\nehUCq0dGjUGMCIUV2rPZozkGfzAqFOZV5PGfd+o50OYh227FahHkOKLFtkyfwhgj4FVhgYFeFbrZ\nslMlfHUfUrb3RLTsjEaMlFfDpX+JvldQCZf3qWJpT1DqwdOaXtNR7L30Rju+7sTmo8lL4NMxMe51\nr8Ld56sx6T6AdJmPIs5vA0KhdXe0nWhXg6qyCsoMtvt5WPaZzBWBEwLW/joz1x6I8nlwyZ9H9p4p\nYqRK6pB7NQshzgF+B1iBO6WUP+3z/lTgb0ChdsxNUsr1Q73fSBIIqRpHAPMqlOq5aX97ZF9OTAiq\nWQxvjBH0qfyCtt1q5d+lZSW3bB9YKDRvHzj8MRGJYvU9rSojOd3oLTn9bkAm1hT6ogsnTyvoCkK6\nHM0Rn4IBoRCbjNWyPSoUdj+nckJGwgFsEoeR6KMCIcSvhRAbtcevhBAFBs6zArcB5wLzgSuFEH11\npu8A/5JSHgdcAfwx9Y8wOsSbj9Q/4b5WDy6n0hBcsZqCmc08tghq9veWHapLmE7LzgGO14qypRIa\n6XD1XylnVFPwxvgGDKz4deHkaYtO3mkNScWYT6F2napVBPHff806yC6GqavSMyYTwxhxNP8V6AY+\nrD26ACMdH44Hdkkp90gp/cD9QN/sCwnoHp4CYJCegWOHQFBityq1tiTXSVmemvhdmi9B9ykAFJvm\no7FFQGtd2bwjfiIaqDFM217V3yAV52CiSqHu1szYx3Wfgq9bbTsNOE31rGlPa3TyTrtQGCT6qPsw\nHHhL+XacBVHHciigekrMPTf1BC+TYWPkG58ppbw0ZvsHQojNBs6bDByI2a4n6p/Q+T7wtBDiC4AL\nOCPRhYQQ1wLXAkydOtXArTOPij6KagPVE/No7vZFNQVn9L1SU1MYW8RqCuGAcrROmB+dlHQCXlWr\nX69hlEq8vMMVnywW8CrNIVOO5q6GGKFgQFOw2lSehadVlezWx5yW8STo8ZCI7etRFWLXwq5nokK5\n7hXwdQ69JajJsDCiKXiFEJGSjUKIE4FhpipGuBK4W0pZCZwH3CuE6DcmKeUdUsplUsplZWVl/S4y\nGvi0PAUd3a8Q8SmYmsLYRdcUWrariah4uso16Gs+evEnyhn7/I9UmeaSFISCvU/0kadNPWfS0RwR\nCgZ8CvpYPK3RyXukfQq166BouorEKZ0T/f5r1qmxzByyO9NkGBjRFD4H/E3zIwigDfiUgfMOosps\n61Rq+2K5GjgHQEq5QQiRBZQCTQauP6oEQqpKqo7uV9DNR7qm4LRZ4vwLJqOMlDGawi7VOL10jnps\n+adqZZmVr47b9rAqt3zqt1QnLiMrcJ2+PoVMJK7pRBzNKfgU9LG4W9LvU7DaVfhuMqHQ26VCTldc\np6KASmer0tPeDqVBzDo9WtfJZEQxUvtos5RyEbAQWCClPE5KucXAtd8GZgshpgshHChH8qN9jtkP\nnA4ghJgHZAHNqXyA0UJPXtOpnqg0BT0UNctmRQgVeWTWPRpDhAIq8qhgqhIOLdvVhKT7C3TH8+Ft\nKmx14YdVb4PSWandp69PIaNCQXM0D0lTaFPCxOocuFz3UBisT/POp5XpTi8JrSeovfeAyrxO1B/Z\nZEQYUCgIIa7Snr8qhPgq8FngszHbSdF6MNwIPAXUoKKMtgkhfiiEuFA77GvANUKILcA/gU9JmayR\n7Ojh9gVZ84sXeLtOmQFi8xQAZpa7VNmLbPWPZbEIcuxWsw3nWEPXEioWRveVzo0KBd2EUbsOENHs\n5VRx5Kp+ymFVDyvzmsIQzEeukqijOV2JazqO3OQ+hdrHVcmHSq1ijv79v/Z7sNhUgTqTUSGZ+UjX\nJRP9hRmauLWcg/V99n035vUHwIlGrjXaNHf72Nfqoaaxi+XTigmEZJxQcNqs3PnJ5cwsi6rgOU6b\nmc081tD9CRMXRputl85RfgWLLersrF0HU1ZAbvnQ7hMpiudW5qiM+hSyVZE+b7vaTsV85GlNb9e1\nyJhyBo4+CvpUT+NjL4m2DC2apgoKdtXDjFOS95QwySgDCgUppZ5296yU8rXY9zRn81GFLxgGVNkK\nKWWkSmosa+bEO8Gnl7qorjC4ajMZGXRNoaBSTTzedmU+stqheIaKQGqvg0Nb4awfDf0+jphKoVn5\nWoVUkZnJThdAPU0qksqoLT6nRCWIuZvS52TWcbgGzlPY85LKIJ8XYyKy2lTl0uZaM+polDHiaL4V\nWGJg3xFNb0CZATy+IIGQUpQc1uS+gvuvWZnxcZmkiK4p2LOUhtBeB9mFal/pHNXO8u+Xqe3hZNP2\njcDxtCqBYMlA0IEuBHoOK2e4UR+WrrV07E+/phDrU3jjTyr8dfGValtvahPbMhSUcG6uNbOYR5kB\nhYIQYhVwAlDWx4eQjypJcVShawpuf4hASL2ONR8lwmIxHcxjDl1TsGXDyhtULX2dpZ+KloqYc7bS\nHIZKIqGQCdMRRGsXuZuNJa7pRITCgaG1oEyGwxUt1f3m7epeulBoeAemruhfLXbZZ5RZT29PajIq\nJNMUHECudkysDaQLuCyTgxqL+IKaphAjFPqaj0zGAbGaQnUfJ/LsM5O3VEyFWJ8CZFgoxGgKqaz4\n9fEEvekLR42MKaYlZ1ejyuaWUj1adqly1X2ZeZp6mIwqyXwKLwEvCSHullLuG8ExjUl6A7pPIYjf\noKZgMgaJ1RQyiT4567kKnrbUGrWnQqxPoW8v4mTECqlM+RS87cpvEfKpKrQhv/oNMtFNzSQtGJnV\n7hRCFOobQogiIcRTGRzTmCRWU/BrpiSHKRTGH7GaQiZxJNIUMlDiAqKawkC9FAYiViikW1PQfQpd\nMeXMWrZHQ37HcJOZox0jjuZSKWWHviGlbBdCDDFOb/zii9EUIo5m03w0/hgxTSHGpyClEgqZahYT\nG22Uivkoq0BFK8lQZoRCwB3fArRlp9IUIHPd1EyGjZFZLaz1PQBACFGFwTyFI4nYkFSjjmaTMYiu\nKaS7JWZfYruP+brVZJgxn0KM6ScVR7MQ0TFlwqcQDqroLnUzlQPSskOVxHZl6LswGTZGNIVvA68K\nIV5C1T46Ca1i6dFENCQ1aj6yDxKSajIG0TWFTNfViXU0e1rU60w7miG1+kyg1T9qyoCmoI2jdZd6\nnnisVpU2mFpfCpMRx0jntSeFEEsAPej+y1LKlswOa+wR0RQCUUezaT4ah0Q0hQz7FGLLR+sltPMm\nZvZekJpPAaKCyp5uoaCNqXWXaugz4ViVAxIK9I/6MhlTJKt9VK09LwGmohrgNABTtX1HFRFHsy9E\nwHQ0j19GSlOwWKJhmbpdPS9D8fdD9SlA1IyTCZ8CKD9CfoVyLHc3Kq3JdDKPaZJpCl8DrgF+leA9\nCRxVAcXR5LWYkFRTUxh/BHoBoUo7Z5q+ETgD9X8eLsM1H0H6C+LpmkfHfph9VrwgMJ3MY5pkeQrX\naM9mpwuiPoXeQDiSs2BqCuOQYK+aREeinHlEUzikJslUnMCpYLGq0tchX+r3iAiFDJS5AEBGNQUd\nM0dhTJOszMUlyU6UUj6U/uGMXXRNAaDLGwDM6KNxScCbeX+Cjl4+urtBTYyZFET2bCUUUp3cIz6F\nDCSv6eRNilahFVYoHBstdU0Sk8x8pJcwLEfVQHpe2z4VeB04uoRCICoUOjShYDqaRxEp4ZHPq6bv\nU1MoPKhrCiOBQysfHeiFvAyZjnTsOdDbMXRHc9r7KcQIhfyKaBVaqyMzRQFN0kYy89GnAYQQTwPz\npZSN2nYFcPeIjG4M0as5mgE6PSoBxzQfjSLedtW+0eFKTSiMqKaglXroOQRTV2X4XtqknqpPYdYZ\nsOJzMGFBmsfTR1MAWPO/0L8Fu8kYw0iewhRdIGgcRkUjHVXEagqduvnIZuYpjBp605qWHamdN5Ka\ngt2l6hF1H8pcOGrkXtpnStmnUAzn/jQD44nRPHQH+4Kjro7muMSIUHhOq3X0T237I8CzmRvS2MQX\noym0ezTzkakpjB56e8vmFIXCSGsKHQdUNnOmwlF19Ek43Q7joRI7jkybzkzSipHktRuFEBcDekeM\nO6SU/83ssMYevmAYh9WCPxSO+BTMkNRRRBcK3Q2qjIRRW/qI+xS0vsmZCkfViWgKY0Qo2BxRx7LZ\nWnNcYURTAHgH6JZSPhSA5FEAACAASURBVCuEyBFC5EkpuzM5sLGGLxCiyGXncJcvYj4yNYVRxBOT\nVN+yEyYbzKcMeMFVNvhx6SButTwCmoKwpD+KaDg4XEogjET4r0naGHRWE0JcAzwI6D2bJwMPZ3JQ\nY4FwWPJ2XRtBLVHNFwxTlKMSnnRHsxmSmgIBLzRuSd/1dE0BouWYjRDszXzZbJ1EdvWM3Stbtbgc\nSxOw3ZV5YWiSdozMap8HTkR1XENKuRMVpnrE0ukN8Nl7NnL57Rt4fbeafGKFQoc3gNUisJrtNo3z\n6m/hjlOUjT0deFpVwpbFpur0GyXYm/my2TqRCBwBuRMye6/CqVA8LbP3SJWiKphwzGiPwiRFjJiP\nfFJKv9BWIEIIG0dw6exOb4CL/vAqda0eANo1rcAXCFHsckSOcZr+hNT44BGQYdi+HlZcN/zredqU\nGcienVoEUmAENQVdKOSWqzj9THLqt+Hkr2f2Hqly1UNmTsI4xMjM9pIQ4ltAthDiTODfwGOZHdbo\n8e7+dupaPXzvgvkA9PiCAPQGwxTmqH9sKU1/Qkq07obmGvW6Jk1/Onons7K5KZqPvCOvKYxE9I3V\nnv6idsPFkZP5vhUmacfIzHYT0AxsBa4D1gPfyeSgRhPdibysSrVOdGtCwRcIkW23km1XKx8zmzkF\ndEGw8COw7/VojsFw8LSqbNzS2UrohILGzhsNTcEMyTQZRySd2YQQVuBeKeVfpJSXSykv014fseaj\nDi0HYWKBmjh6fCo/wRcM47RbyHEooWA6mVOg9nGoWAQrrletH3c8Ofxruls0oTAXwoGYDl9JCIdV\nfaCR0hT0SqGZdjKbmKSRpDOblDIEVAkhRqDO8NhAFwpFOXZcDituX5BgKEwwLMmyWclxmppCSnQf\ngvq3oPoCmHQc5E+GmnXDv66nTfU81qtvGvErBLUGOyOuKZgROCbjByOO5j3Aa0KIRwG3vlNK+euM\njWoU6fQGyHPasFktuJw23L5gpEKq024hx66+MlNTGITa9fDs98DXo7arz1fhktXnwzv3qJLSQ7WB\nhwLg69Q0hVlq32NfhGdu7n/siuvh+GvUa10ojLRPwdQUTMYRRma23cA67di8mMcRSYfXT362cijn\nOm30xAqFGE3BFAqDsPt5aN8HVavgpP+B8nlqf/VaNTnvfj75+cnQfRI5xZBVoCJvpp+sTFSxj3AQ\nXv+9igwAlSsBI6cplM+H1V+FuWb7SZPxg5EyFz8AEELkq80jO5O50xOIRBlFNQXlV3Daoj4F03w0\nCL2daoV82V/j91edCFmFyoQ074LE5w6Gnriml31e843Ex71zDzz6BTi0FSoWjrymYLXBGd8bmXuZ\nmKQJIxnNy4QQW4H3gK1CiC1CiKWZH9ro0OGNFQpW3L5QpNNalt1KjkPJUYfVTFxLSm+nWsX3xWqD\nuefCjieUGWgo9BUKAzHnXFX6oVbzYYy0pmBiMg4xstz9K3CDlHKalHIaKsP5royOahTp8PgpzFZ+\n9aj5qL+mYJqPBmEgoQDKhNTbCXWvDu3aet2jwYRCbhlMWamin2DkNQUTk3GIkZktJKV8Rd+QUr4K\nGAoKF0KcI4TYLoTYJYS4KcH7vxFCbNYeO4QQHcaHnhk6vQEKYs1H/mCkl4IKSdU0BdN8lJxkQmHm\naWpi1ifrVIloCqWDHztvLRx+H9r2mpqCiYkBjGY0/1kIcYoQYo0Q4o/Ai0KIJUKIAUtTajkOtwHn\nAvOBK4UQ82OPkVJ+RUq5WEq5GLiVUW7xKaWkwxOgMDvep9AbUJpCWdu75NqUgEioKTRuSU9ilrsF\nttwPm/8J+zYM/3rJqHtN3WfLA+BuHfz49jo1wep0NSTuadDboXwHiXDkwKzTlVAIhxMfk4h9GyDo\nj3c0D0b1WvVcu87UFExMDGAkJHWR9tzXY3YcqgbSaQOcdzywS0q5B0AIcT9wEfDBAMdfmeAeI4rb\nHyIYlhGfQmz0UaVoYsHTX+bQzG/xF47tX+YiHIK7zof5F8KH/ji8gbz0M3jrDvXaYoP/2WlsAkwV\nvxvuuVBF6QCsuhHOviX5Of/9nNICbnhdbT/8Oeishy9sij8umaYAarKuXQcN70KlARdVTxPcdQ6c\n+UOlKTgLjNUTKqpSUUC7X4DCKrXP1BRMTAZkUE1BSnlqksdAAgFUie3Ykpj12r5+CCGqgOnAMOIU\nh0+HVvxO9ynkOKz0BsJ4/EGqhfooE3v3AAnMRx37VUOV7euNl1wYiKYaqFgMH/2XmrDTkQGciNZd\n6vrn/womHKvumwwp4fA2aNqmSkt42mDvK+p1oDd6XNAPAc/AmgLAnLNVA5Zag4lsPi3o7YNHo3WP\njDLhGFUfydQUTEwGZawYxq8AHtQyqPshhLhWCLFRCLGxubk5Y4PQs5kLYjQFgDZ3gJmiAYCS3joA\n7H2jj/SibN522Pfa8AbSvB0mHguzz1IZwEO1vQ96H83sM/UEtZoeLCu4p0kljYEa044nVdkKpBIw\nOr4u9ZxMU8gphmmrjQuFoE89H9wIh94f3MkcS+lc6NwfNTuZmoKJyYBkUigcBKbEbFdq+xJxBdEe\n0P2QUt4hpVwmpVxWVpa5rll6MbxYnwJAm9sXEQqF7joggU9Bn1CtjuFN4t52cDep8g16BvCu58Dv\nGfo1B6JlhwrZLJkJZXOg84AyKSU7HrTPuE7lGlgd8e+BMh1BcqEAyoTUssNYn+VgjCbSXJOiUJit\nng9tVc8j1aPZxGQcYiRPoV/t20T7EvA2MFsIMV2rnXQF8GiCa1UDRUCGPaqDo2sKhVozHV0otPT4\nmWVR8iyn9xA59PY3H7VsV9Ews89SQmGoNQNbtBW3XtOneq0q97z7uaFdL+m9dkDRNFXeOFJDKEkZ\nar2ZzeKPwoG3YNezqvIpIv68Xi2IbFChcL56rjVQTjvkj992GYg80tE/2yGt85spFExMBsSIppBo\nsh50ApdSBoEbgaeAGuBfUsptQogfCiEujDn0CuD+sVB5tcOr+RQi5iOVk9DWozSFsEs1nJshGvo7\nmlt2qsmn+nzoqlcO1KGgT7z6RFZ1grLNZ8KE1LIjeh9DQmGnqvy5/BpAqoqjCy5TXb9iu58Z1RQK\nJsOkJcY+m24+qlyunlPxKZTMVBpRU63atps+BROTgRhQKAghJmqZy9lCiOP0EFQhxCmAoe7gUsr1\nUso5UsqZUspbtH3flVI+GnPM96WU/XIYMs7BTfB/Z4M3mhoR8Sno5iMtJyHU00SB8BCeq1a2M0VD\nYvNR6WyYc05/B+r6b6h2lEZo2aFMMnqkjNWuMoC3/ht+PR/+cno03r4vbXvV+0ZaXoZDyg+gC4Pi\nGWrcuhno4Rvg7TsTf8YJx0DRdCWsqk5U14g1H3kNagqg8ggOblKfTX/culQ5r2PRhcIxl6jnVMxH\nNqfSiMIBsNjNbmAmJklIpimcDfwS5Qv4VczjK8C3Mj+0DLPxLjjwRlxkT6c3QJbdQpbWSEc3H+V2\nqQnKOvccpLAyy9IQbz5yt6qImLK5mgP1xOjq19OmJtcNf1AT8WC07ITimaochM6JX4bFH1Olpw9u\nVOGVidhyv3p/678Gv0/HPmWS0YWCPnG2bFeRVJv/Aa//Id4M1rxDfUYh4PxfwgW/U0KrdI4ye+k5\nB7qmkJ0k+kjnuE/Asqth5qnqMeNUaNsD7/X5DCFNKFSdoMJSj7108GvHon9OU0swMUnKgEJBSvk3\nKeWpwKeklKfFhKFeJKUc1SSzYRMOwfYn1OuY9pAdHn9ES4Bo9FGRtw4AMfFYAgVV/TUFfZUc6wdo\nrlUTpR6h425WdvjBaN6unL6xlFfDhb+Hy+9W8fkDRezo+430K2juM2b9dcvOqEBr3wtNWlqJr0eZ\nxXSn7awz4JgPqddlc5Tfo1PTUIyaj0CVolj7a7joNvX40G1aaYo+n0HXFOzZcOKXlMkqFfRxm/4E\nE5OkGPEpLBVCRJZ8QogiIcSPMjimzLP/DVU/p2BqXGRPpzcQyVGAqKZQ4d+PByfkTSJcModZ4mB8\nSGrED6BNPLEO1Jp1kDshGrGTjKBPZQyXzkn8vtWu4vu3P9E/F6JtryrnUDAVGt6BzoECvfQx60Jh\ndnRf2RxlUvrgUcivBERUQLRqvoZEY+vrj+jtVEl3dkNWxv5Unx8tTaGjCwWrI/E5g1E6Vz2b4agm\nJkkxIhTOlVJGDO9SynZgfBeIr30crE445ydqhbtHmWM6PNG6RxDVFKZzkH1MBosFx4S5zLAc5viq\n/Oj1WnaqFWiBtnotqFSmnq3/UX0D5l8E09cooZDMn962V2kV+gSWiHlrwdsG+/v4+vXJe63W+2j7\n+uTfQcsOcJXFO2xL5yiT0v7XYdH/b+/Mw6usrsX9LkIggUQgJIgaZbAIZQhJQMByBR5QVOpwRblX\npQhYq7R1aK+V4gz9gcrV38Ve9BaHq4LFClKr1KKiICIik4ooQUUgaKwyBCFMgQzr/rG/c3IyH5Mz\nkvU+z/fkfMPZ3zo75+z1rWGvfRWcPqDCkvIN+DXJVnX1M99sZmlgJdkf+0pTBASgfe6jhj7p+91k\n5j4yjLoIRikkBKagikgyEExKamyi6p7guw5zT92+2v74LIUKpZCU2IxmAl2bfctXCZkANMvoTnNK\nyWp9oKLNvV9A+27QLKA7e/wUdn3ilE6Pi91A932+mxFcGzU9vVflzBFOoVXN2PnsVTi5D3Q73w2A\nW+pJ8/RlSwUSuP/ji53c321yMYa9X7hAdFqX6m21TofktAqLqb4SF/XRrrP7LIGWVamXktq8gV89\nX5+apWAYdRJM7aP5wDIR8ZXLngjMDZ9IYeLbTS4Ie2SfG+SG3O65Yy50T9UbnmbYwc/o1ioFNmwC\nQIDrWnxKpuxlaYI3D883cK57wgVdwRXC6zS48v16XALLp1dk6HT4Mfz9N7DyIeg6FE4f6LJ4wCmL\nbcsrAsh1KYWWKa7KaN4rFbGHslLnEhvmJXH1uBje+yOse7L2TJvdW6D35ZWP+e57UqYrs9HyJLfE\n5dsPuAHfN6ehJnzxCGi8UgCnlFY86GZRp3SomLzWUPdRqzQ3j8QsBcOok2BWXpspIh8D53mH/p+q\nvhFescLA9rfhzXvd68TWFUsk9hkDm16AV3/LFIB/epvH3Z4HZFtLbznJDj2gRSqsnVO5/dMHVN7P\n6O4G1syzXSZRSgeXXZP3sttOyYYb33HXLr0HtnhZuif3rn/t4qwxbpGaV39bcUwSoKcX+O09Gt57\nBJb8ru52MqvInNwOMnq4FdFEXH7/af3g4+fd+b5X195WereKTK5QKIUzh8OKB6BgA/QYVTF5rTGB\n4s6DoUVK4+QyjBOcYCwFcJPPSlX1LRFpJSKpcbcsZ/+fe7NvcYNuS2+Z6W7nweQdFBcf5dz/fJtf\nDTuTiYMrXCRj5qxma+FxunbwLIWWqXDbZ3D8UEXb0sz55wMRgeuXuXM+rlnoUldXPOBSLsvLnctp\nz2duJvSls93AXB+9r4Auw1zevY/E5IqBuGMfmLyjcmmIqjRrXvOs4F+uxtlIHhNfdzEMqP4ZA8no\nDh895yyx4v2NX6zeV0yvxCvvUXrM9WVCsF/ZGrjy2YbHOQyjiVDvL0xEfgHcAKQBZ+Iqnc4BRoRX\ntBDTMsVtNdEqjQOlxeyhHS3anQqpHf2njid3YD8HaNk8wA1TV1uBVB3AEhJd26f0hQ+ehaJv3P6+\n7c7lE3Dfemldz+StYOYI1ERVd1PzFsHJFZiBFApLwef7903UKy12sZTG0CxW6j8aRuwSzK/k18Bg\noAhAVbcCHcIpVDTw1z1Kruyz9qWltkwM4YASmK3zfb4rX11bGmq84ItH7P3CUwoNVEo+fL5/n7VT\ndrzhQWbDMIImGFv8mKoeF8/sFpHmuMV1Tij8aykEpKRChVJIah7C0gi+tM69X1Q8CVedsBZvtO3k\nnuS/+8QN5KGyFHxKofSYKQXDiADBKIV3ROROXA2k84FfAUGUtYwv8gtdyeiObSoHMlPCYSm0TndP\n0oFKoX0dGUfxQLMEaP8jKFjv9hurFHyWQkmAUmis+8gwjHoJZqSbAuwBPgFuBJYAd4dTqGjw/rZC\n0lNa0DW9cuZPa69SasuqpbIbg0hFCuferZB6CiSdVP/7Yp30bm5eAzTefZTQ3AXDSz2lWWaWgmFE\ngjotBRFJAOap6ljgyciIFHlUlTXb9zGwa3ukSnaK332UGOLKmulnwdalLrumrnkJ8UT6WRXrPTfW\nUgBnLfgtBYspGEYkqPPx11ses5O3SM4JS37hEb4rKuacrtUzelK88tkhtRTAxRAO73YznOsqaxFP\nZAR8jlAohcSkCkuhtLjhE9cMwwiaYGIK24H3RGQx4F+rUVX/K2xSRZg12wsBGFSDUvBnH4Uy0AwV\n2UalxfGfeeQj0OIJtaVQdtwqnBpGBAhGKWzztmZAanjFiQ7vbyskI7UlZ2ZUn0kclpgCVClZfYK4\nj9r/qOJ1yC2FY9CigVVXDcMImmBiCqmqWk+9hPjFxRMKGVRDPAHCGFNo28m5Q8qOV3a7xDMtWrtK\nsQe+avjkuUCaJwXEFIqDm+1tGEajCCamMLiua+KdHXsPs/vgsRrjCRCmyWvgsmvSznS1eFIbWRIi\nlkjv5pRdKFw9ickB2UcWaDaMSBCM+2ijF094kcoxhfhefc1j49duqYj+nWt+CvXPUwi1+wigyxC3\nWtmJVI+nyxA3ozkUn6mSpWApqYYRCYJRCklAITA84JgCJ4RSyC88QjOBTu1r9le3bhGmQDPAqP8M\nfZvR5l9+47ZQkJjsiuuBKQXDiBDBlM6eGAlBosXOwsOc2ja51kH/rJNTuHFIV87tVkNFUSO8BFoK\nZTaj2TAiQb0+ERHJFJG/ichub/uriGRGQrhIkF94hM7ta1+/oHlCM+4Y9WPap9iAFHECYwo2ec0w\nIkIwjvJngMXAqd72d+/YCcHOwsO1uo6MKFM1+8iUgmGEnWCUQoaqPqOqpd72LFDHaivxw4EjJew/\nUlKnpWBEkcRkpwzKy92CQuY+MoywE4xSKBSRn4lIgrf9DBd4jnt27nPJVGeYpRCbNE9yVWT9S3Fa\nmQvDCDfBKIXrgH8DvgO+Ba4ETojgc36hW+rRLIUYJTHZWQi+JTmtzIVhhJ1gso92ApdGQJaIs3Ov\nZymkmaUQk/iUQPEB99cK4hlG2Akm+2iuiLQN2G8nIk+HV6zIkF94hI4nJZHcIgxzEIzG41cK3lwF\nCzQbRtgJxn2Upar7fTuq+j2QEz6RIodlHsU4iVUsBXMfGUbYCUYpNBMRfw0IEUkjuJnQMU99cxSM\nKONbktPcR4YRMYIZ3P8/8L6IvOjtjwFmhE+kyHDoWCl7Dx2jU7pZCjGLz1I4au4jw4gU9VoKqjoP\nGA3s8rbRqvpcMI2LyIUi8rmIfCkiU2q55t9EJE9ENovI8z9E+Maws9AFmc1SiGGqWgqmFAwj7ATl\nBlLVPCDvhzTsrcXwGHA+UACsF5HFXlu+a7oBdwCDVfV7EenwQ+7RED7Y+T2TF33MwWK3lrBlHsUw\nVWMKNnnNMMJOOGMDA4AvVXU7gIi8AFxGZeXyC+AxL3iNqu4OozwAbMjfx7Y9h7mk76lkpLSkR8cT\ncjG5EwO/pWDuI8OIFOFUCqcBXwfsFwADq1xzFoCIvAckAFNV9fUwysTB4lKaCfz3Vdk1rrRmxBDV\nso9MKRhGuIl2FlFzoBswDMgEVopIn8AUWAARuQG4AeCMM85o1A2LiktITUo0hRAPVJu8ZkrBMMJN\nGJYT8/MNcHrAfqZ3LJACYLGqlqjqDuALnJKohKo+oar9VbV/RkbjavEdLC4lNSnautAIikTPfeTP\nPrKUVMMIN+FUCuuBbiLSRURaAFfhSnAH8jLOSkBE0nHupO1hlImDxSWclJQYzlsYoaKqpWCT1wwj\n7IRNKahqKXAT8AawBVioqptF5A8i4qul9AauCmse8DZwu6qGtQJr0VGzFOKGRJu8ZhiRJqyjo6ou\nAZZUOXZvwGsF/sPbIkJRcQmZ7SwNNS6I8dpHJSUlFBQUUFxcHG1RDMNPUlISmZmZJCY2zCPS5B6Z\nDxaXclJyk/vY8YmIUwyl3qAbY4HmgoICUlNT6dy5syUuGDGBqlJYWEhBQQFdunRpUBvhjCnEJEUW\nU4gvfNaCJEBCbCnz4uJi2rdvbwrBiBlEhPbt2zfKem1SSqG8XDl0zGIKcYUvrhBjriMfphCMWKOx\n38kmpRQOHy9FFbMU4gmfpRCjSqEp8+yzz/LPf/4zIveaMGECixYtAuD6668nL6/2qjsrVqxg9erV\n/v05c+Ywb968sMpXVFTEPffcQ05ODjk5OVx11VVs3ry50jX3339/g9qu7/OGmialFIq8ekdmKcQR\nPkshxuIJ8U5paWmd+8HQWKXQkHsCPPXUU/Ts2bPW81WVwqRJk7j22msbdK9g2LdvH+eddx6nnXYa\nq1ev5qOPPuL222/n+uuvZ82aNf7ralMKqkp5eXmt7df3eUNNk1IKB4tLADgp2SyFuMFvKVg6ak3M\nmzePrKws+vbty7hx4wDIz89n+PDhZGVlMWLECL766ivAPW1PmjSJgQMHMnnyZKZOncq4ceMYPHgw\n48aNo6ysjNtvv52zzz6brKwsHn/8cf99Zs6cSZ8+fejbty9Tpkxh0aJFbNiwgbFjx5Kdnc3Ro0cr\nyTVs2DBuvfVWsrOz6d27N+vWrQMI+p6qyk033UT37t0577zz2L17d6W2N2zYAMDrr79Obm4uffv2\nZcSIEeTn5zNnzhxmzZpFdnY27777LlOnTuXhhx8GYOPGjQwaNIisrCwuv/xyvv/+e3+bv//97xkw\nYABnnXUW7777LgCbN29mwIABZGdnk5WVxdatW6v9D2677TamTZvGpEmTSE52DzH9+vVj8eLFTJ48\nGYApU6Zw9OhRsrOzGTt2LPn5+XTv3p1rr72W3r178/XXX/PLX/6S/v3706tXL+67774aP29KSgp3\n3XUXffv2ZdCgQezateuHf2nqoUk9Mh80SyH+8McUYnvi2rS/bybvn0UhbbPnqSdx3yW9aj2/efNm\npk+fzurVq0lPT2ffvn0A3HzzzYwfP57x48fz9NNPc8stt/Dyyy8DLmNq9erVJCQkMHXqVPLy8li1\nahXJyck88cQTtGnThvXr13Ps2DEGDx7MyJEj+eyzz3jllVdYu3YtrVq1Yt++faSlpfHoo4/y8MMP\n079//xrlO3LkCBs3bmTlypVcd911fPrppwBB3fOjjz7i888/Jy8vj127dtGzZ0+uu+66Su3v2bOH\nX/ziF6xcuZIuXbr45Zo0aRIpKSn87ne/A2DZsmX+91x77bXMnj2boUOHcu+99zJt2jQeeeQRwFku\n69atY8mSJUybNo233nqLOXPmcOuttzJ27FiOHz9OWVlZJRkOHTrEjh07uOiii1i7di033XQT6enp\nnHLKKUybNo3c3Fw+/PBDHnzwQR599FE2btwIOMW9detW5s6dy6BBgwCYMWMGaWlplJWVMWLECDZt\n2kRWVlal+x0+fJhBgwYxY8YMJk+ezJNPPsndd99dx7foh9OkLIWio85SSLWYQvzgUwbmPqrG8uXL\nGTNmDOnp6QCkpaUB8P7773PNNdcAMG7cOFatWuV/z5gxY0hIqFiT/NJLL/U/3S5dupR58+aRnZ3N\nwIEDKSwsZOvWrbz11ltMnDiRVq1aVbpPfVx99dUADBkyhKKiIvbv3x/0PVeuXMnVV19NQkICp556\nKsOHD6/W/po1axgyZIg/9bI+uQ4cOMD+/fsZOnQoAOPHj2flypX+86NHjwbcU35+fj4A55xzDvff\nfz8zZ85k586dfrl9bNmyhX79+gEwefJk/vrXvzJ//nyWL19OWVkZ3bt3Z9u2bTXK06lTJ79CAFi4\ncCG5ubnk5OSwefPmGuMILVq04OKLL64mZyhpUo/MPkvhJLMU4ocYzz7yUdcTfSzRunXrWvdVldmz\nZ3PBBRdUuuaNN95o0L2qZsH49oO555Illea8RoSWLd13LCEhwR/vuOaaaxg4cCD/+Mc/GDVqFI8/\n/ng1BeVTss2aNfMX7Bw40BWE3r17d63xgMB+2LFjBw8//DDr16+nXbt2TJgwoca00sTEimKegXKG\nkqZlKRSbpRB3WPZRrQwfPpwXX3yRwkJXGcbnPvrJT37CCy+8AMD8+fM599xzg2rvggsu4E9/+hMl\nJe538sUXX3D48GHOP/98nnnmGY4cOVLpPqmpqRw8eLDW9hYsWADAqlWraNOmDW3atAn6nkOGDGHB\nggWUlZXx7bff8vbbb1d776BBg1i5ciU7duwISq42bdrQrl07f7zgueee81sNtbF9+3a6du3KLbfc\nwmWXXcamTZsqne/RowcffvghAGVlZRQUFLB//37Wrl1LQUEBK1as4JxzzgHcgO77nFUpKiqidevW\ntGnThl27dvHaa6/VKVc4aVKPzBZTiEN8aypY3aNq9OrVi7vuuouhQ4eSkJBATk4Ozz77LLNnz2bi\nxIk89NBDZGRk8MwzzwTV3vXXX09+fj65ubmoKhkZGbz88stceOGFbNy4kf79+9OiRQtGjRrF/fff\n7w9cJycn8/7771dzrSQlJZGTk0NJSQlPP/30D7rn5ZdfzvLly+nZsydnnHGGf2ANJCMjgyeeeILR\no0dTXl5Ohw4dePPNN7nkkku48soreeWVV5g9e3al98ydO5dJkyZx5MgRunbtWm/fLFy4kOeee47E\nxEQ6duzInXfeWel8amoqHTp0YNmyZcycOZPLL7+c9PR0LrroImbNmsWTTz5Jixbuu3vDDTeQlZVF\nbm4uM2ZUXua+b9++5OTk0KNHD04//XQGDx5cp1xhRVXjauvXr582lPuX5Gm3u5Y0+P1GFHj1NtX7\nTlKd/+/RlqQaeXl50RYhZhk6dKiuX78+2mJEhO+++0779eunCxYs0JKSElVV3bJliz7//PNRk6mm\n7yawQYMYY5uWzs0GcwAACp5JREFU++hoqcUT4o1ES0k1YpuTTz6ZpUuXsn79egYOHEifPn2YOnUq\nvXv3jrZoDaJJjZC2lkIc0twmr8UjK1asiLYIESUtLY2HHnoo2mKEhCZlKdiqa3FIogWaDSOSNCml\n4Fuf2YgjLPvIMCJKk1IKtpZCHGKT1wwjojQxpVBCakuzFOKKOJm8ZhgnCk1KKRQdNUsh7jD3Ucxi\npbMrCGfpbIhsXzcZpVBSVs7RkjKLKcQb/tLZlpIaSqx0duhobOnsYDClEAYO2Wzm+MRvKcR2ldRo\nYaWz47N0NsCf//xnf9s33ngjZWVllJWVMWHCBHr37k2fPn2YNWtWvX0daprMCOmre2TzFOIMf0wh\nxi2F16bAd5+Ets2OfeCiB2s9baWz47d09pYtW1iwYAHvvfceiYmJ/OpXv2L+/Pn06tWLb775xt9X\n+/fvp23btvX2dShpMkrB6h7FKZZ9VCt1lc5+6aWXAFc62/e0CvWXzt60aZPfd3/gwIGIlM6u6Z6R\nKp09ZswY//naSmfPmDGDgoICRo8eTbdu3Sq1WVPp7JSUFHJzc7n33nv9pbNzc3MrvW/ZsmV88MEH\nnH322QAcPXqUDh06cMkll7B9+3ZuvvlmfvrTnzJy5Mg6P1M4aDIjpK2lEKckeZU1W6ZGV476qOOJ\nPpaw0tm1E8nS2arK+PHjeeCBB6qd+/jjj3njjTeYM2cOCxcurLWYYLhoMjEF3/rMln0UZ6R1gWte\nhO6joi1JzGGls+O3dPaIESNYtGiRP1ayb98+du7cyd69eykvL+eKK65g+vTp/rbr6+tQ0mRGyIMW\nU4hfzoq8CR0PWOns+C2dPX/+fKZPn87IkSMpLy8nMTGRxx57jOTkZCZOnEh5eTmA35Kor69DSjCl\nVGNpa2jp7Kfe3a6dfv+q7j98vEHvN4yqWOns2rHS2VY6O+Y5vV0yF/Q6mRQLNBuGEUKsdHacMrJX\nR0b26hhtMQyjSWCls+OXJmMpGIZhGPVjSsEwGoFz1RpG7NDY76QpBcNoIElJSRQWFppiMGIGVaWw\nsJCkpIaXhWkyMQXDCDWZmZkUFBSwZ8+eaItiGH6SkpLIzMxs8PvDqhRE5ELgj0AC8JSqPljl/ATg\nIeAb79CjqvpUOGUyjFCRmJjoL7FgGCcKYVMKIpIAPAacDxQA60VksapWLYS+QFVvCpcchmEYRvCE\nM6YwAPhSVber6nHgBeCyMN7PMAzDaCThVAqnAV8H7Bd4x6pyhYhsEpFFInJ6GOUxDMMw6iHagea/\nA39R1WMiciMwF6hWI1dEbgBu8HYPicjnDbxfOrC3ge+NFCZjaDAZQ0Osyxjr8kHsyNgpmIskXOl0\nInIOMFVVL/D27wBQ1eq1YvHHIPapavVSiqGTaYOqhn+VikZgMoYGkzE0xLqMsS4fxIeMgYTTfbQe\n6CYiXUSkBXAVsDjwAhE5JWD3UmBLGOUxDMMw6iFs7iNVLRWRm4A3cCmpT6vqZhH5A65a32LgFhG5\nFCgF9gETwiWPYRiGUT9hjSmo6hJgSZVj9wa8vgO4I5wyVOGJCN6roZiMocFkDA2xLmOsywfxIaOf\nsMUUDMMwjPjDah8ZhmEYfpqMUhCRC0XkcxH5UkSmRFseABE5XUTeFpE8EdksIrd6x9NE5E0R2er9\nbRdlORNE5CMRedXb7yIia72+XOAlEkRTvrbePJfPRGSLiJwTg334W+9//KmI/EVEkqLdjyLytIjs\nFpFPA47V2G/i+G9P1k0ikhtFGR/y/tebRORvItI24Nwdnoyfi8gF0ZIx4NxtIqIiku7tR6UffwhN\nQikElNy4COgJXC0iPaMrFeAC7Lepak9gEPBrT64pwDJV7QYs8/ajya1UzgybCcxS1R8B3wM/j4pU\nFfwReF1VewB9cbLGTB+KyGnALUB/Ve2NS7y4iuj347PAhVWO1dZvFwHdvO0G4E9RlPFNoLeqZgFf\n4MUlvd/OVUAv7z3/4/32oyEj3mTckcBXAYej1Y9B0ySUAjFackNVv1XVD73XB3GD2Wk42eZ6l80F\n/jU6EoKIZAI/BZ7y9gU3wXCRd0m05WsDDAH+F0BVj6vqfmKoDz2aA8ki0hxoBXxLlPtRVVfisv4C\nqa3fLgPmecv9rgHaVkkpj5iMqrpUVUu93TWAryToZcALqnpMVXcAX+J++xGX0WMWMBkIDNxGpR9/\nCE1FKQRbciNqiEhnIAdYC5ysqt96p74DTo6SWACP4L7Y5d5+e2B/wI8y2n3ZBdgDPOO5uJ4SkdbE\nUB+q6jfAw7gnxm+BA8AHxFY/+qit32L1N3Qd8Jr3OmZkFJHLgG9U9eMqp2JGxtpoKkohphGRFOCv\nwG9UtSjwnLr0sKikiInIxcBuVf0gGvcPkuZALvAnVc0BDlPFVRTNPgTw/PKX4RTYqUBranA3xBrR\n7rf6EJG7cC7Y+dGWJRARaQXcCdxb37WxSFNRCt8AgcX2MqlYwyGqiEgiTiHMV9WXvMO7fCal93d3\nlMQbDFwqIvk4l9twnP++recGgej3ZQFQoKprvf1FOCURK30IcB6wQ1X3qGoJ8BKub2OpH33U1m8x\n9RsStxbLxcBYrcirjxUZz8Q9AHzs/XYygQ9FpCOxI2OtNBWlUG/JjWjg+ef/F9iiqv8VcGoxMN57\nPR54JdKygZtcqKqZqtoZ12fLVXUs8DZwZbTlA1DV74CvRaS7d2gEkEeM9KHHV8AgEWnl/c99MsZM\nPwZQW78tBq71smcGAQcC3EwRRdziXZOBS1X1SMCpxcBVItJSRLrggrnrIi2fqn6iqh1UtbP32ykA\ncr3vasz0Y62oapPYgFG4TIVtwF3RlseT6V9w5vkmYKO3jcL57ZcBW4G3gLQYkHUY8Kr3uivux/Yl\n8CLQMsqyZQMbvH58GWgXa30ITAM+Az4FngNaRrsfgb/gYhwluIHr57X1GyC4DL5twCe4TKpoyfgl\nzi/v+83MCbj+Lk/Gz4GLoiVjlfP5QHo0+/GHbDaj2TAMw/DTVNxHhmEYRhCYUjAMwzD8mFIwDMMw\n/JhSMAzDMPyYUjAMwzD8mFIwjAgiIsPEqzZrGLGIKQXDMAzDjykFw6gBEfmZiKwTkY0i8ri4NSUO\nicgsb12EZSKS4V2bLSJrAur7+9Yg+JGIvCUiH4vIhyJyptd8ilSs/zDfm+VsGDGBKQXDqIKI/Bj4\nd2CwqmYDZcBYXCG7DaraC3gHuM97yzzg9+rq+38ScHw+8Jiq9gV+gpv1Cq4a7m9wa3t0xdVBMoyY\noHn9lxhGk2ME0A9Y7z3EJ+MKw5UDC7xr/gy85K3n0FZV3/GOzwVeFJFU4DRV/RuAqhYDeO2tU9UC\nb38j0BlYFf6PZRj1Y0rBMKojwFxVvaPSQZF7qlzX0BoxxwJel2G/QyOGMPeRYVRnGXCliHQA/7rF\nnXC/F19V02uAVap6APheRM71jo8D3lG3kl6BiPyr10ZLr86+YcQ09oRiGFVQ1TwRuRtYKiLNcNUv\nf41bwGeAd243Lu4ArsT0HG/Q3w5M9I6PAx4XkT94bYyJ4McwjAZhVVINI0hE5JCqpkRbDsMIJ+Y+\nMgzDMPyYpWAYhmH4MUvBMAzD8GNKwTAMw/BjSsEwDMPwY0rBMAzD8GNKwTAMw/BjSsEwDMPw83//\n24n7Ju/euAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 1.0,validation accuracy: 0.828125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rctSnIGM7pGE",
        "colab_type": "code",
        "outputId": "8f3a9ac7-0fd5-4536-d153-9becd66e9140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpcIpjq_7pDt",
        "colab_type": "code",
        "outputId": "1dd7b517-cff2-41e5-f8ca-bf4490a7f5ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 12\n",
        "# increased dropout (0.25 -> 0.5)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('12'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 5.9621 - acc: 0.5350\n",
            "Epoch 00001: val_acc improved from -inf to 0.32812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.32812-0.53906.hdf5\n",
            "256/256 [==============================] - 12s 48ms/sample - loss: 5.6771 - acc: 0.5391 - val_loss: 0.7228 - val_acc: 0.3281\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7529 - acc: 0.3950\n",
            "Epoch 00002: val_acc did not improve from 0.32812\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.7638 - acc: 0.3906 - val_loss: 0.7895 - val_acc: 0.3281\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7371 - acc: 0.4100\n",
            "Epoch 00003: val_acc did not improve from 0.32812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7304 - acc: 0.4258 - val_loss: 0.7036 - val_acc: 0.3281\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6864 - acc: 0.5150\n",
            "Epoch 00004: val_acc improved from 0.32812 to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-04-0.67188-0.53125.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6863 - acc: 0.5312 - val_loss: 0.6582 - val_acc: 0.6719\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6875 - acc: 0.5750\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6792 - acc: 0.5977 - val_loss: 0.6419 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6821 - acc: 0.5900\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6771 - acc: 0.6016 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6838 - acc: 0.5950\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6803 - acc: 0.6016 - val_loss: 0.6370 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6773 - acc: 0.6100\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6780 - acc: 0.6016 - val_loss: 0.6385 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6772 - acc: 0.5950\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6016 - val_loss: 0.6412 - val_acc: 0.6719\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6840 - acc: 0.5850\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6752 - acc: 0.6016 - val_loss: 0.6437 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6778 - acc: 0.5800\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6684 - acc: 0.6016 - val_loss: 0.6450 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6775 - acc: 0.5800\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6713 - acc: 0.6016 - val_loss: 0.6452 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6597 - acc: 0.6350\n",
            "Epoch 00013: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6748 - acc: 0.6016 - val_loss: 0.6441 - val_acc: 0.6719\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6801 - acc: 0.5800\n",
            "Epoch 00014: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6700 - acc: 0.6016 - val_loss: 0.6443 - val_acc: 0.6719\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6672 - acc: 0.6050\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6705 - acc: 0.6016 - val_loss: 0.6432 - val_acc: 0.6719\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6663 - acc: 0.6150\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6702 - acc: 0.6016 - val_loss: 0.6420 - val_acc: 0.6719\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6817 - acc: 0.5850\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6738 - acc: 0.6016 - val_loss: 0.6408 - val_acc: 0.6719\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6666 - acc: 0.6100\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6709 - acc: 0.6016 - val_loss: 0.6392 - val_acc: 0.6719\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6695 - acc: 0.6100\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6751 - acc: 0.6016 - val_loss: 0.6384 - val_acc: 0.6719\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6705 - acc: 0.5950\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6688 - acc: 0.6016 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6591 - acc: 0.6350\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6700 - acc: 0.6016 - val_loss: 0.6366 - val_acc: 0.6719\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6671 - acc: 0.6150\n",
            "Epoch 00022: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6742 - acc: 0.6016 - val_loss: 0.6368 - val_acc: 0.6719\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6728 - acc: 0.6100\n",
            "Epoch 00023: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6741 - acc: 0.6016 - val_loss: 0.6377 - val_acc: 0.6719\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6683 - acc: 0.6100\n",
            "Epoch 00024: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6712 - acc: 0.6016 - val_loss: 0.6383 - val_acc: 0.6719\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6663 - acc: 0.5900\n",
            "Epoch 00025: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6646 - acc: 0.6016 - val_loss: 0.6380 - val_acc: 0.6719\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6750 - acc: 0.6000\n",
            "Epoch 00026: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6746 - acc: 0.6016 - val_loss: 0.6370 - val_acc: 0.6719\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6611 - acc: 0.6150\n",
            "Epoch 00027: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6644 - acc: 0.6055 - val_loss: 0.6354 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6700 - acc: 0.5950\n",
            "Epoch 00028: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6655 - acc: 0.6016 - val_loss: 0.6342 - val_acc: 0.6719\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6590 - acc: 0.5950\n",
            "Epoch 00029: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6577 - acc: 0.6016 - val_loss: 0.6327 - val_acc: 0.6719\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6731 - acc: 0.5800\n",
            "Epoch 00030: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6634 - acc: 0.6016 - val_loss: 0.6317 - val_acc: 0.6719\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6690 - acc: 0.5800\n",
            "Epoch 00031: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6581 - acc: 0.6016 - val_loss: 0.6309 - val_acc: 0.6719\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6628 - acc: 0.6100\n",
            "Epoch 00032: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6674 - acc: 0.6016 - val_loss: 0.6305 - val_acc: 0.6719\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6587 - acc: 0.6050\n",
            "Epoch 00033: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6614 - acc: 0.6016 - val_loss: 0.6303 - val_acc: 0.6719\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6665 - acc: 0.5850\n",
            "Epoch 00034: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6615 - acc: 0.5977 - val_loss: 0.6301 - val_acc: 0.6719\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6573 - acc: 0.6050\n",
            "Epoch 00035: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6608 - acc: 0.6016 - val_loss: 0.6298 - val_acc: 0.6719\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6561 - acc: 0.6150\n",
            "Epoch 00036: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6609 - acc: 0.6016 - val_loss: 0.6295 - val_acc: 0.6719\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6457 - acc: 0.6200\n",
            "Epoch 00037: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6563 - acc: 0.5977 - val_loss: 0.6291 - val_acc: 0.6719\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6701 - acc: 0.6050\n",
            "Epoch 00038: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6674 - acc: 0.6094 - val_loss: 0.6287 - val_acc: 0.6719\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6531 - acc: 0.5950\n",
            "Epoch 00039: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6496 - acc: 0.6094 - val_loss: 0.6285 - val_acc: 0.6719\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6563 - acc: 0.6050\n",
            "Epoch 00040: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6541 - acc: 0.6094 - val_loss: 0.6292 - val_acc: 0.6719\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6468 - acc: 0.6300\n",
            "Epoch 00041: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6480 - acc: 0.6172 - val_loss: 0.6312 - val_acc: 0.6719\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6583 - acc: 0.6150\n",
            "Epoch 00042: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6573 - acc: 0.6133 - val_loss: 0.6327 - val_acc: 0.6719\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6611 - acc: 0.6000\n",
            "Epoch 00043: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6461 - acc: 0.6211 - val_loss: 0.6353 - val_acc: 0.6719\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6357 - acc: 0.6100\n",
            "Epoch 00044: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6451 - acc: 0.6016 - val_loss: 0.6437 - val_acc: 0.6719\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6524 - acc: 0.5950\n",
            "Epoch 00045: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6422 - acc: 0.6250 - val_loss: 0.6489 - val_acc: 0.6719\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6373 - acc: 0.6300\n",
            "Epoch 00046: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6430 - acc: 0.6211 - val_loss: 0.6672 - val_acc: 0.6719\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6505 - acc: 0.6200\n",
            "Epoch 00047: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6445 - acc: 0.6367 - val_loss: 0.6903 - val_acc: 0.6719\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6219 - acc: 0.6700\n",
            "Epoch 00048: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6222 - acc: 0.6719 - val_loss: 0.7427 - val_acc: 0.6719\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6303 - acc: 0.6250\n",
            "Epoch 00049: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6211 - acc: 0.6367 - val_loss: 0.8169 - val_acc: 0.6719\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6369 - acc: 0.6350\n",
            "Epoch 00050: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6461 - acc: 0.6289 - val_loss: 1.3201 - val_acc: 0.6719\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6849 - acc: 0.6850\n",
            "Epoch 00051: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6703 - acc: 0.6758 - val_loss: 4.4578 - val_acc: 0.6719\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6106 - acc: 0.6600\n",
            "Epoch 00052: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6134 - acc: 0.6484 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6778 - acc: 0.6550\n",
            "Epoch 00053: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6476 - acc: 0.6680 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6537 - acc: 0.7150\n",
            "Epoch 00054: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6554 - acc: 0.6875 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5776 - acc: 0.6850\n",
            "Epoch 00055: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6473 - acc: 0.6875 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6815 - acc: 0.6700\n",
            "Epoch 00056: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6595 - acc: 0.6641 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5718 - acc: 0.7000\n",
            "Epoch 00057: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5552 - acc: 0.7148 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.8796 - acc: 0.6850\n",
            "Epoch 00058: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.8066 - acc: 0.6953 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5633 - acc: 0.7050\n",
            "Epoch 00059: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5558 - acc: 0.7031 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5745 - acc: 0.6800\n",
            "Epoch 00060: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5569 - acc: 0.7070 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6293 - acc: 0.7150\n",
            "Epoch 00061: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6181 - acc: 0.7070 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5523 - acc: 0.6800\n",
            "Epoch 00062: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5502 - acc: 0.6914 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5888 - acc: 0.6600\n",
            "Epoch 00063: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5871 - acc: 0.6680 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5583 - acc: 0.7300\n",
            "Epoch 00064: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5579 - acc: 0.7305 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5983 - acc: 0.7300\n",
            "Epoch 00065: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5757 - acc: 0.7344 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5303 - acc: 0.7450\n",
            "Epoch 00066: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5194 - acc: 0.7500 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6676 - acc: 0.7250\n",
            "Epoch 00067: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6668 - acc: 0.7500 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7052 - acc: 0.7700\n",
            "Epoch 00068: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7325 - acc: 0.7578 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7419 - acc: 0.7650\n",
            "Epoch 00069: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6883 - acc: 0.7617 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5008 - acc: 0.7450\n",
            "Epoch 00070: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5074 - acc: 0.7461 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5852 - acc: 0.7400\n",
            "Epoch 00071: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5575 - acc: 0.7617 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5861 - acc: 0.7250\n",
            "Epoch 00072: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5479 - acc: 0.7539 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4678 - acc: 0.7800\n",
            "Epoch 00073: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4880 - acc: 0.7656 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6224 - acc: 0.7800\n",
            "Epoch 00074: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6497 - acc: 0.7812 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4635 - acc: 0.7850\n",
            "Epoch 00075: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4660 - acc: 0.7852 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4261 - acc: 0.7900\n",
            "Epoch 00076: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4373 - acc: 0.7852 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5109 - acc: 0.8100\n",
            "Epoch 00077: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5026 - acc: 0.7812 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3997 - acc: 0.8400\n",
            "Epoch 00078: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5226 - acc: 0.8281 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5583 - acc: 0.7900\n",
            "Epoch 00079: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5182 - acc: 0.7891 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5431 - acc: 0.7950\n",
            "Epoch 00080: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5111 - acc: 0.8008 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4698 - acc: 0.7650\n",
            "Epoch 00081: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4411 - acc: 0.7930 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4701 - acc: 0.7350\n",
            "Epoch 00082: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4967 - acc: 0.7305 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4852 - acc: 0.7800\n",
            "Epoch 00083: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4951 - acc: 0.7812 - val_loss: 1.8255 - val_acc: 0.6719\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5631 - acc: 0.7450\n",
            "Epoch 00084: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5695 - acc: 0.7305 - val_loss: 0.9975 - val_acc: 0.6094\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5932 - acc: 0.6700\n",
            "Epoch 00085: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5913 - acc: 0.6680 - val_loss: 0.7763 - val_acc: 0.6094\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5851 - acc: 0.6700\n",
            "Epoch 00086: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5794 - acc: 0.6992 - val_loss: 1.5524 - val_acc: 0.6094\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5625 - acc: 0.7150\n",
            "Epoch 00087: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5572 - acc: 0.7109 - val_loss: 1.5684 - val_acc: 0.6562\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5614 - acc: 0.6900\n",
            "Epoch 00088: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5627 - acc: 0.6875 - val_loss: 1.7809 - val_acc: 0.6562\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5452 - acc: 0.6850\n",
            "Epoch 00089: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5453 - acc: 0.6914 - val_loss: 1.9511 - val_acc: 0.6562\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5558 - acc: 0.7250\n",
            "Epoch 00090: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5389 - acc: 0.7344 - val_loss: 1.4057 - val_acc: 0.6406\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5166 - acc: 0.7050\n",
            "Epoch 00091: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5111 - acc: 0.7305 - val_loss: 1.2544 - val_acc: 0.6562\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4914 - acc: 0.7650\n",
            "Epoch 00092: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4951 - acc: 0.7500 - val_loss: 1.2095 - val_acc: 0.6719\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4727 - acc: 0.7350\n",
            "Epoch 00093: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4733 - acc: 0.7461 - val_loss: 1.2035 - val_acc: 0.6719\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4461 - acc: 0.8000\n",
            "Epoch 00094: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-94-0.68750-0.78906.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.5023 - acc: 0.7891 - val_loss: 1.6604 - val_acc: 0.6875\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5224 - acc: 0.7600\n",
            "Epoch 00095: val_acc improved from 0.68750 to 0.70312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-95-0.70312-0.76562.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.5042 - acc: 0.7656 - val_loss: 2.0998 - val_acc: 0.7031\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5073 - acc: 0.8050\n",
            "Epoch 00096: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5519 - acc: 0.8047 - val_loss: 2.3128 - val_acc: 0.6875\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5737 - acc: 0.7900\n",
            "Epoch 00097: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5426 - acc: 0.7812 - val_loss: 2.1130 - val_acc: 0.6875\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4287 - acc: 0.7750\n",
            "Epoch 00098: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4064 - acc: 0.8047 - val_loss: 2.0684 - val_acc: 0.7031\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4690 - acc: 0.8050\n",
            "Epoch 00099: val_acc did not improve from 0.70312\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4610 - acc: 0.7891 - val_loss: 2.0644 - val_acc: 0.7031\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3372 - acc: 0.8650\n",
            "Epoch 00100: val_acc improved from 0.70312 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-100-0.73438-0.82812.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.4214 - acc: 0.8281 - val_loss: 2.2823 - val_acc: 0.7344\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3289 - acc: 0.8600\n",
            "Epoch 00101: val_acc improved from 0.73438 to 0.75000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-101-0.75000-0.85938.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.3324 - acc: 0.8594 - val_loss: 2.7013 - val_acc: 0.7500\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4287 - acc: 0.8500\n",
            "Epoch 00102: val_acc did not improve from 0.75000\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4733 - acc: 0.8438 - val_loss: 2.9066 - val_acc: 0.7500\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6146 - acc: 0.8250\n",
            "Epoch 00103: val_acc improved from 0.75000 to 0.79688, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-103-0.79688-0.83984.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.5324 - acc: 0.8398 - val_loss: 1.7823 - val_acc: 0.7969\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4824 - acc: 0.7950\n",
            "Epoch 00104: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4513 - acc: 0.8047 - val_loss: 1.7666 - val_acc: 0.7812\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4956 - acc: 0.7950\n",
            "Epoch 00105: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4844 - acc: 0.7930 - val_loss: 2.5175 - val_acc: 0.7500\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3901 - acc: 0.8250\n",
            "Epoch 00106: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3841 - acc: 0.8359 - val_loss: 2.9625 - val_acc: 0.6875\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4674 - acc: 0.7750\n",
            "Epoch 00107: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4297 - acc: 0.8008 - val_loss: 2.9837 - val_acc: 0.6719\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3926 - acc: 0.8450\n",
            "Epoch 00108: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4177 - acc: 0.8164 - val_loss: 1.5037 - val_acc: 0.6562\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6247 - acc: 0.6700\n",
            "Epoch 00109: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6289 - acc: 0.6641 - val_loss: 1.4148 - val_acc: 0.5781\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5885 - acc: 0.6450\n",
            "Epoch 00110: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5850 - acc: 0.6445 - val_loss: 1.5603 - val_acc: 0.6719\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5346 - acc: 0.7450\n",
            "Epoch 00111: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5329 - acc: 0.7461 - val_loss: 1.5101 - val_acc: 0.6875\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5193 - acc: 0.7650\n",
            "Epoch 00112: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5243 - acc: 0.7617 - val_loss: 1.2847 - val_acc: 0.7031\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5037 - acc: 0.7100\n",
            "Epoch 00113: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5035 - acc: 0.6992 - val_loss: 0.8789 - val_acc: 0.6719\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5203 - acc: 0.6650\n",
            "Epoch 00114: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5076 - acc: 0.6914 - val_loss: 0.7996 - val_acc: 0.6875\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4920 - acc: 0.7200\n",
            "Epoch 00115: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4884 - acc: 0.7188 - val_loss: 0.5645 - val_acc: 0.7344\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5133 - acc: 0.7450\n",
            "Epoch 00116: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5021 - acc: 0.7578 - val_loss: 0.4960 - val_acc: 0.7656\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4682 - acc: 0.8500\n",
            "Epoch 00117: val_acc improved from 0.79688 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-117-0.82812-0.80859.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.4974 - acc: 0.8086 - val_loss: 0.5247 - val_acc: 0.8281\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5130 - acc: 0.7700\n",
            "Epoch 00118: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5160 - acc: 0.7695 - val_loss: 0.5450 - val_acc: 0.7812\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5244 - acc: 0.7600\n",
            "Epoch 00119: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5262 - acc: 0.7578 - val_loss: 0.5468 - val_acc: 0.7812\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5012 - acc: 0.7550\n",
            "Epoch 00120: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5081 - acc: 0.7500 - val_loss: 0.5406 - val_acc: 0.7812\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4821 - acc: 0.7650\n",
            "Epoch 00121: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4886 - acc: 0.7695 - val_loss: 0.7147 - val_acc: 0.8125\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4696 - acc: 0.7900\n",
            "Epoch 00122: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4624 - acc: 0.7930 - val_loss: 0.8984 - val_acc: 0.7969\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4492 - acc: 0.7750\n",
            "Epoch 00123: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4503 - acc: 0.7695 - val_loss: 0.8966 - val_acc: 0.7812\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5061 - acc: 0.7800\n",
            "Epoch 00124: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5481 - acc: 0.7812 - val_loss: 1.0963 - val_acc: 0.7812\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4187 - acc: 0.7900\n",
            "Epoch 00125: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4348 - acc: 0.7812 - val_loss: 1.0914 - val_acc: 0.8125\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4130 - acc: 0.7950\n",
            "Epoch 00126: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3985 - acc: 0.8125 - val_loss: 1.0926 - val_acc: 0.8125\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3976 - acc: 0.8050\n",
            "Epoch 00127: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4015 - acc: 0.8086 - val_loss: 0.9080 - val_acc: 0.7812\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3891 - acc: 0.8200\n",
            "Epoch 00128: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3792 - acc: 0.8281 - val_loss: 0.9199 - val_acc: 0.7500\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3464 - acc: 0.8350\n",
            "Epoch 00129: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3556 - acc: 0.8320 - val_loss: 0.9303 - val_acc: 0.7500\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3267 - acc: 0.8450\n",
            "Epoch 00130: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3374 - acc: 0.8359 - val_loss: 1.3583 - val_acc: 0.7812\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3179 - acc: 0.8350\n",
            "Epoch 00131: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3131 - acc: 0.8438 - val_loss: 1.5229 - val_acc: 0.7812\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4944 - acc: 0.8600\n",
            "Epoch 00132: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4561 - acc: 0.8672 - val_loss: 1.0832 - val_acc: 0.6719\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4568 - acc: 0.7900\n",
            "Epoch 00133: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4774 - acc: 0.8008 - val_loss: 1.1547 - val_acc: 0.5781\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6208 - acc: 0.7750\n",
            "Epoch 00134: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5749 - acc: 0.7734 - val_loss: 0.8410 - val_acc: 0.6406\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3711 - acc: 0.8450\n",
            "Epoch 00135: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3615 - acc: 0.8398 - val_loss: 0.9257 - val_acc: 0.7188\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2992 - acc: 0.9050\n",
            "Epoch 00136: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3091 - acc: 0.8945 - val_loss: 1.1214 - val_acc: 0.7812\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4307 - acc: 0.8350\n",
            "Epoch 00137: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4069 - acc: 0.8320 - val_loss: 1.0874 - val_acc: 0.7812\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3454 - acc: 0.8450\n",
            "Epoch 00138: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3230 - acc: 0.8516 - val_loss: 0.4822 - val_acc: 0.8125\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3145 - acc: 0.8700\n",
            "Epoch 00139: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3082 - acc: 0.8750 - val_loss: 0.5260 - val_acc: 0.7969\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2940 - acc: 0.8850\n",
            "Epoch 00140: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3107 - acc: 0.8750 - val_loss: 0.8959 - val_acc: 0.7969\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2744 - acc: 0.8800\n",
            "Epoch 00141: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2876 - acc: 0.8750 - val_loss: 1.0927 - val_acc: 0.7812\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4351 - acc: 0.8850\n",
            "Epoch 00142: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3924 - acc: 0.8906 - val_loss: 1.0894 - val_acc: 0.7812\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4018 - acc: 0.8850\n",
            "Epoch 00143: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3615 - acc: 0.8945 - val_loss: 0.9175 - val_acc: 0.8281\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3484 - acc: 0.8850\n",
            "Epoch 00144: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3171 - acc: 0.8984 - val_loss: 1.1036 - val_acc: 0.8281\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3251 - acc: 0.8950\n",
            "Epoch 00145: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3735 - acc: 0.8906 - val_loss: 1.1094 - val_acc: 0.7656\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3002 - acc: 0.9150\n",
            "Epoch 00146: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.2896 - acc: 0.9062 - val_loss: 1.1190 - val_acc: 0.7656\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3900 - acc: 0.9050\n",
            "Epoch 00147: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3595 - acc: 0.8984 - val_loss: 1.0856 - val_acc: 0.7969\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4490 - acc: 0.8250\n",
            "Epoch 00148: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4046 - acc: 0.8438 - val_loss: 1.3178 - val_acc: 0.7344\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4048 - acc: 0.8500\n",
            "Epoch 00149: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3939 - acc: 0.8477 - val_loss: 0.9169 - val_acc: 0.7031\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3806 - acc: 0.8250\n",
            "Epoch 00150: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.3859 - acc: 0.8281 - val_loss: 0.7103 - val_acc: 0.7500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1fw/uxDqowpRDCcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.828125,validation accuracy: 0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieKOkWqV7pBf",
        "colab_type": "code",
        "outputId": "3527bc32-f8e0-409f-e82e-e963fdb57ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.675"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fATWaXtj7o_j",
        "colab_type": "code",
        "outputId": "24bedb33-bdd6-45db-f0eb-2f988851c01a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 13\n",
        "# increased dropout (0.25 -> 0.75)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('13'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 4.7133 - acc: 0.5550\n",
            "Epoch 00001: val_acc improved from -inf to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.67188-0.56250.hdf5\n",
            "256/256 [==============================] - 15s 59ms/sample - loss: 4.7378 - acc: 0.5625 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 4.3688 - acc: 0.6000\n",
            "Epoch 00002: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 3.8789 - acc: 0.5703 - val_loss: 0.6460 - val_acc: 0.6719\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.0601 - acc: 0.4650\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.9937 - acc: 0.4609 - val_loss: 0.7318 - val_acc: 0.3281\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7670 - acc: 0.4150\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7612 - acc: 0.4102 - val_loss: 0.7017 - val_acc: 0.3281\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7058 - acc: 0.5050\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6994 - acc: 0.5234 - val_loss: 0.6738 - val_acc: 0.6719\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6963 - acc: 0.5300\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6961 - acc: 0.5234 - val_loss: 0.6570 - val_acc: 0.6719\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6827 - acc: 0.5850\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6863 - acc: 0.5859 - val_loss: 0.6465 - val_acc: 0.6719\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6778 - acc: 0.5950\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6736 - acc: 0.6055 - val_loss: 0.6411 - val_acc: 0.6719\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6825 - acc: 0.5900\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6778 - acc: 0.5977 - val_loss: 0.6385 - val_acc: 0.6719\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6858 - acc: 0.5650\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6715 - acc: 0.6016 - val_loss: 0.6373 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6815 - acc: 0.6100\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6836 - acc: 0.6016 - val_loss: 0.6367 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6981 - acc: 0.5800\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6788 - acc: 0.6055 - val_loss: 0.6368 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6974 - acc: 0.5550\n",
            "Epoch 00013: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6789 - acc: 0.5977 - val_loss: 0.6372 - val_acc: 0.6719\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6637 - acc: 0.6250\n",
            "Epoch 00014: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6717 - acc: 0.6055 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6556 - acc: 0.6350\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6711 - acc: 0.6055 - val_loss: 0.6379 - val_acc: 0.6719\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6722 - acc: 0.6100\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6772 - acc: 0.6016 - val_loss: 0.6389 - val_acc: 0.6719\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6682 - acc: 0.6200\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6735 - acc: 0.5977 - val_loss: 0.6402 - val_acc: 0.6719\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6658 - acc: 0.6000\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6650 - acc: 0.5977 - val_loss: 0.6414 - val_acc: 0.6719\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6762 - acc: 0.6000\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6787 - acc: 0.6016 - val_loss: 0.6425 - val_acc: 0.6719\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6836 - acc: 0.5900\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6788 - acc: 0.5977 - val_loss: 0.6435 - val_acc: 0.6719\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6725 - acc: 0.5950\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6734 - acc: 0.6016 - val_loss: 0.6440 - val_acc: 0.6719\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6829 - acc: 0.6150\n",
            "Epoch 00022: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6861 - acc: 0.5977 - val_loss: 0.6443 - val_acc: 0.6719\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6803 - acc: 0.5900\n",
            "Epoch 00023: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6733 - acc: 0.6016 - val_loss: 0.6442 - val_acc: 0.6719\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6757 - acc: 0.6000\n",
            "Epoch 00024: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6744 - acc: 0.6016 - val_loss: 0.6433 - val_acc: 0.6719\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6705 - acc: 0.5950\n",
            "Epoch 00025: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6684 - acc: 0.6016 - val_loss: 0.6424 - val_acc: 0.6719\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6875 - acc: 0.5900\n",
            "Epoch 00026: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6829 - acc: 0.6016 - val_loss: 0.6414 - val_acc: 0.6719\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6655 - acc: 0.6100\n",
            "Epoch 00027: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6695 - acc: 0.5938 - val_loss: 0.6403 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6786 - acc: 0.5950\n",
            "Epoch 00028: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6757 - acc: 0.6016 - val_loss: 0.6398 - val_acc: 0.6719\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6746 - acc: 0.6150\n",
            "Epoch 00029: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6739 - acc: 0.6055 - val_loss: 0.6394 - val_acc: 0.6719\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6798 - acc: 0.5850\n",
            "Epoch 00030: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6692 - acc: 0.5977 - val_loss: 0.6391 - val_acc: 0.6719\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6753 - acc: 0.5950\n",
            "Epoch 00031: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6756 - acc: 0.6016 - val_loss: 0.6387 - val_acc: 0.6719\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6546 - acc: 0.6300\n",
            "Epoch 00032: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6680 - acc: 0.6016 - val_loss: 0.6381 - val_acc: 0.6719\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6790 - acc: 0.6000\n",
            "Epoch 00033: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6761 - acc: 0.5977 - val_loss: 0.6383 - val_acc: 0.6719\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6754 - acc: 0.5850\n",
            "Epoch 00034: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6610 - acc: 0.6016 - val_loss: 0.6381 - val_acc: 0.6719\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6677 - acc: 0.6000\n",
            "Epoch 00035: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6695 - acc: 0.5977 - val_loss: 0.6376 - val_acc: 0.6719\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6662 - acc: 0.6000\n",
            "Epoch 00036: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6680 - acc: 0.5977 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6886 - acc: 0.5900\n",
            "Epoch 00037: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6842 - acc: 0.6016 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6671 - acc: 0.6050\n",
            "Epoch 00038: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6689 - acc: 0.6016 - val_loss: 0.6372 - val_acc: 0.6719\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6829 - acc: 0.5900\n",
            "Epoch 00039: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6765 - acc: 0.6016 - val_loss: 0.6371 - val_acc: 0.6719\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6570 - acc: 0.6350\n",
            "Epoch 00040: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6717 - acc: 0.5977 - val_loss: 0.6368 - val_acc: 0.6719\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6864 - acc: 0.5700\n",
            "Epoch 00041: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6739 - acc: 0.6016 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6730 - acc: 0.6100\n",
            "Epoch 00042: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6769 - acc: 0.5977 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6688 - acc: 0.6000\n",
            "Epoch 00043: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6669 - acc: 0.6016 - val_loss: 0.6377 - val_acc: 0.6719\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6643 - acc: 0.6350\n",
            "Epoch 00044: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6765 - acc: 0.6055 - val_loss: 0.6372 - val_acc: 0.6719\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6725 - acc: 0.6200\n",
            "Epoch 00045: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6759 - acc: 0.6016 - val_loss: 0.6371 - val_acc: 0.6719\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6619 - acc: 0.6150\n",
            "Epoch 00046: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6674 - acc: 0.6016 - val_loss: 0.6373 - val_acc: 0.6719\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6650 - acc: 0.6200\n",
            "Epoch 00047: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6708 - acc: 0.6016 - val_loss: 0.6372 - val_acc: 0.6719\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6715 - acc: 0.6000\n",
            "Epoch 00048: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6706 - acc: 0.6016 - val_loss: 0.6374 - val_acc: 0.6719\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6823 - acc: 0.5800\n",
            "Epoch 00049: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6741 - acc: 0.5977 - val_loss: 0.6373 - val_acc: 0.6719\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6767 - acc: 0.6000\n",
            "Epoch 00050: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6739 - acc: 0.6016 - val_loss: 0.6365 - val_acc: 0.6719\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6685 - acc: 0.6150\n",
            "Epoch 00051: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6055 - val_loss: 0.6355 - val_acc: 0.6719\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6782 - acc: 0.5800\n",
            "Epoch 00052: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6671 - acc: 0.5977 - val_loss: 0.6349 - val_acc: 0.6719\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6482 - acc: 0.6200\n",
            "Epoch 00053: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6572 - acc: 0.6016 - val_loss: 0.6341 - val_acc: 0.6719\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6563 - acc: 0.6200\n",
            "Epoch 00054: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6596 - acc: 0.6094 - val_loss: 0.6336 - val_acc: 0.6719\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6712 - acc: 0.6050\n",
            "Epoch 00055: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6742 - acc: 0.5977 - val_loss: 0.6332 - val_acc: 0.6719\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6685 - acc: 0.5900\n",
            "Epoch 00056: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6657 - acc: 0.6016 - val_loss: 0.6330 - val_acc: 0.6719\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6420 - acc: 0.6350\n",
            "Epoch 00057: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6561 - acc: 0.6055 - val_loss: 0.6324 - val_acc: 0.6719\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6711 - acc: 0.6050\n",
            "Epoch 00058: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6775 - acc: 0.6016 - val_loss: 0.6323 - val_acc: 0.6719\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6650 - acc: 0.5900\n",
            "Epoch 00059: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6640 - acc: 0.5898 - val_loss: 0.6323 - val_acc: 0.6719\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6754 - acc: 0.5900\n",
            "Epoch 00060: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6711 - acc: 0.6016 - val_loss: 0.6321 - val_acc: 0.6719\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6773 - acc: 0.5850\n",
            "Epoch 00061: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6695 - acc: 0.6016 - val_loss: 0.6318 - val_acc: 0.6719\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6582 - acc: 0.6100\n",
            "Epoch 00062: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6661 - acc: 0.6016 - val_loss: 0.6316 - val_acc: 0.6719\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6511 - acc: 0.6200\n",
            "Epoch 00063: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6544 - acc: 0.6055 - val_loss: 0.6317 - val_acc: 0.6719\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6669 - acc: 0.6000\n",
            "Epoch 00064: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6651 - acc: 0.5977 - val_loss: 0.6319 - val_acc: 0.6719\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6493 - acc: 0.6300\n",
            "Epoch 00065: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6586 - acc: 0.6055 - val_loss: 0.6325 - val_acc: 0.6719\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6675 - acc: 0.6000\n",
            "Epoch 00066: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6645 - acc: 0.5977 - val_loss: 0.6328 - val_acc: 0.6719\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6638 - acc: 0.5900\n",
            "Epoch 00067: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6623 - acc: 0.6055 - val_loss: 0.6329 - val_acc: 0.6719\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6739 - acc: 0.6000\n",
            "Epoch 00068: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6684 - acc: 0.6016 - val_loss: 0.6337 - val_acc: 0.6719\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6590 - acc: 0.6000\n",
            "Epoch 00069: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6563 - acc: 0.6133 - val_loss: 0.6349 - val_acc: 0.6719\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6581 - acc: 0.6300\n",
            "Epoch 00070: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6644 - acc: 0.5977 - val_loss: 0.6378 - val_acc: 0.6719\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6595 - acc: 0.6050\n",
            "Epoch 00071: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6683 - acc: 0.5938 - val_loss: 0.6399 - val_acc: 0.6719\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6697 - acc: 0.6050\n",
            "Epoch 00072: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6624 - acc: 0.6133 - val_loss: 0.6427 - val_acc: 0.6719\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6548 - acc: 0.5800\n",
            "Epoch 00073: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6517 - acc: 0.5938 - val_loss: 0.6486 - val_acc: 0.6719\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6591 - acc: 0.6250\n",
            "Epoch 00074: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6525 - acc: 0.6328 - val_loss: 0.6583 - val_acc: 0.6719\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6483 - acc: 0.6350\n",
            "Epoch 00075: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6542 - acc: 0.6172 - val_loss: 0.6745 - val_acc: 0.6719\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6349 - acc: 0.6200\n",
            "Epoch 00076: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6436 - acc: 0.6172 - val_loss: 0.6966 - val_acc: 0.6719\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6464 - acc: 0.5800\n",
            "Epoch 00077: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6430 - acc: 0.6055 - val_loss: 0.7267 - val_acc: 0.6719\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6294 - acc: 0.6450\n",
            "Epoch 00078: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6447 - acc: 0.6211 - val_loss: 0.7815 - val_acc: 0.6719\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6464 - acc: 0.6050\n",
            "Epoch 00079: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6441 - acc: 0.6094 - val_loss: 0.8336 - val_acc: 0.6719\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6469 - acc: 0.6350\n",
            "Epoch 00080: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6431 - acc: 0.6289 - val_loss: 0.9399 - val_acc: 0.6719\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6198 - acc: 0.6500\n",
            "Epoch 00081: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6298 - acc: 0.6406 - val_loss: 2.9530 - val_acc: 0.6719\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6346 - acc: 0.6050\n",
            "Epoch 00082: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6261 - acc: 0.6211 - val_loss: 5.1340 - val_acc: 0.6719\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6199 - acc: 0.6100\n",
            "Epoch 00083: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6180 - acc: 0.6250 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6063 - acc: 0.6800\n",
            "Epoch 00084: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6089 - acc: 0.6602 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6148 - acc: 0.6850\n",
            "Epoch 00085: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6257 - acc: 0.6641 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6338 - acc: 0.6150\n",
            "Epoch 00086: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6328 - acc: 0.6133 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6131 - acc: 0.6200\n",
            "Epoch 00087: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6114 - acc: 0.6133 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5630 - acc: 0.6950\n",
            "Epoch 00088: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6006 - acc: 0.6719 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6742 - acc: 0.6650\n",
            "Epoch 00089: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6606 - acc: 0.6562 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6768 - acc: 0.6400\n",
            "Epoch 00090: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6520 - acc: 0.6562 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5812 - acc: 0.6850\n",
            "Epoch 00091: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5832 - acc: 0.6875 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5773 - acc: 0.6950\n",
            "Epoch 00092: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5804 - acc: 0.6836 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5822 - acc: 0.6700\n",
            "Epoch 00093: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6009 - acc: 0.6758 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6395 - acc: 0.6900\n",
            "Epoch 00094: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6838 - acc: 0.6719 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5843 - acc: 0.6550\n",
            "Epoch 00095: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5772 - acc: 0.6680 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5249 - acc: 0.7550\n",
            "Epoch 00096: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5941 - acc: 0.7266 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6979 - acc: 0.6950\n",
            "Epoch 00097: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6681 - acc: 0.6992 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6477 - acc: 0.6850\n",
            "Epoch 00098: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6729 - acc: 0.6992 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5317 - acc: 0.7650\n",
            "Epoch 00099: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5366 - acc: 0.7422 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5534 - acc: 0.6900\n",
            "Epoch 00100: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5516 - acc: 0.6875 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7072 - acc: 0.6750\n",
            "Epoch 00101: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6689 - acc: 0.6797 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7546 - acc: 0.7150\n",
            "Epoch 00102: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7653 - acc: 0.7188 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5878 - acc: 0.7500\n",
            "Epoch 00103: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5910 - acc: 0.7344 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5366 - acc: 0.7300\n",
            "Epoch 00104: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5401 - acc: 0.7266 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5528 - acc: 0.7000\n",
            "Epoch 00105: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5402 - acc: 0.7227 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5323 - acc: 0.7600\n",
            "Epoch 00106: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5396 - acc: 0.7578 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5677 - acc: 0.7600\n",
            "Epoch 00107: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5419 - acc: 0.7578 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7160 - acc: 0.7250\n",
            "Epoch 00108: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6826 - acc: 0.7305 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5131 - acc: 0.7500\n",
            "Epoch 00109: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5741 - acc: 0.7422 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5728 - acc: 0.7200\n",
            "Epoch 00110: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5539 - acc: 0.7188 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5175 - acc: 0.7450\n",
            "Epoch 00111: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5335 - acc: 0.7188 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5418 - acc: 0.7050\n",
            "Epoch 00112: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5429 - acc: 0.7109 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5326 - acc: 0.7250\n",
            "Epoch 00113: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5865 - acc: 0.7266 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5649 - acc: 0.7150\n",
            "Epoch 00114: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5439 - acc: 0.7188 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4967 - acc: 0.7700\n",
            "Epoch 00115: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4931 - acc: 0.7656 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4909 - acc: 0.7350\n",
            "Epoch 00116: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.4926 - acc: 0.7344 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5364 - acc: 0.7950\n",
            "Epoch 00117: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5827 - acc: 0.7773 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5091 - acc: 0.8000\n",
            "Epoch 00118: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5142 - acc: 0.7852 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7182 - acc: 0.6900\n",
            "Epoch 00119: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6671 - acc: 0.6992 - val_loss: 4.9140 - val_acc: 0.6719\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6082 - acc: 0.7200\n",
            "Epoch 00120: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6935 - acc: 0.6914 - val_loss: 1.2380 - val_acc: 0.6719\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6427 - acc: 0.6750\n",
            "Epoch 00121: val_acc improved from 0.67188 to 0.68750, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-121-0.68750-0.69922.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6229 - acc: 0.6992 - val_loss: 0.8207 - val_acc: 0.6875\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6157 - acc: 0.6500\n",
            "Epoch 00122: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6166 - acc: 0.6328 - val_loss: 0.6500 - val_acc: 0.6719\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6637 - acc: 0.5850\n",
            "Epoch 00123: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6686 - acc: 0.5781 - val_loss: 0.6258 - val_acc: 0.6719\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6194 - acc: 0.6300\n",
            "Epoch 00124: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6299 - acc: 0.6133 - val_loss: 0.5909 - val_acc: 0.6719\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6400 - acc: 0.5850\n",
            "Epoch 00125: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6294 - acc: 0.6055 - val_loss: 0.5623 - val_acc: 0.6719\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6202 - acc: 0.6350\n",
            "Epoch 00126: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6243 - acc: 0.6211 - val_loss: 0.5468 - val_acc: 0.6719\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6216 - acc: 0.5700\n",
            "Epoch 00127: val_acc did not improve from 0.68750\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6170 - acc: 0.5898 - val_loss: 0.5388 - val_acc: 0.6719\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6177 - acc: 0.6450\n",
            "Epoch 00128: val_acc improved from 0.68750 to 0.70312, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-128-0.70312-0.64062.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6217 - acc: 0.6406 - val_loss: 0.5361 - val_acc: 0.7031\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6164 - acc: 0.6550\n",
            "Epoch 00129: val_acc improved from 0.70312 to 0.71875, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-129-0.71875-0.67969.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6080 - acc: 0.6797 - val_loss: 0.5340 - val_acc: 0.7188\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6105 - acc: 0.6350\n",
            "Epoch 00130: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6158 - acc: 0.6133 - val_loss: 0.5315 - val_acc: 0.7188\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6082 - acc: 0.6300\n",
            "Epoch 00131: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5970 - acc: 0.6484 - val_loss: 0.5274 - val_acc: 0.7188\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5753 - acc: 0.6950\n",
            "Epoch 00132: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5776 - acc: 0.6953 - val_loss: 0.5215 - val_acc: 0.7188\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5805 - acc: 0.6950\n",
            "Epoch 00133: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5907 - acc: 0.6758 - val_loss: 0.5171 - val_acc: 0.7188\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5640 - acc: 0.7150\n",
            "Epoch 00134: val_acc did not improve from 0.71875\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5601 - acc: 0.7188 - val_loss: 0.5158 - val_acc: 0.7188\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5716 - acc: 0.6600\n",
            "Epoch 00135: val_acc improved from 0.71875 to 0.73438, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-135-0.73438-0.66406.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6256 - acc: 0.6641 - val_loss: 0.5311 - val_acc: 0.7344\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6420 - acc: 0.6900\n",
            "Epoch 00136: val_acc improved from 0.73438 to 0.75000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-136-0.75000-0.71094.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6137 - acc: 0.7109 - val_loss: 0.7330 - val_acc: 0.7500\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5404 - acc: 0.7450\n",
            "Epoch 00137: val_acc improved from 0.75000 to 0.76562, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-137-0.76562-0.73438.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.5420 - acc: 0.7344 - val_loss: 0.9036 - val_acc: 0.7656\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5818 - acc: 0.7900\n",
            "Epoch 00138: val_acc improved from 0.76562 to 0.79688, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-138-0.79688-0.75781.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.6440 - acc: 0.7578 - val_loss: 0.8969 - val_acc: 0.7969\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5289 - acc: 0.7850\n",
            "Epoch 00139: val_acc did not improve from 0.79688\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5482 - acc: 0.7773 - val_loss: 0.7257 - val_acc: 0.7812\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5027 - acc: 0.7700\n",
            "Epoch 00140: val_acc improved from 0.79688 to 0.82812, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-140-0.82812-0.75000.hdf5\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 0.5188 - acc: 0.7500 - val_loss: 0.5258 - val_acc: 0.8281\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5610 - acc: 0.6800\n",
            "Epoch 00141: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5540 - acc: 0.6953 - val_loss: 0.4908 - val_acc: 0.7812\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5021 - acc: 0.7400\n",
            "Epoch 00142: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5089 - acc: 0.7383 - val_loss: 0.4632 - val_acc: 0.8281\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5540 - acc: 0.7150\n",
            "Epoch 00143: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5363 - acc: 0.7344 - val_loss: 0.5183 - val_acc: 0.7656\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4974 - acc: 0.7950\n",
            "Epoch 00144: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5003 - acc: 0.7852 - val_loss: 0.6223 - val_acc: 0.6406\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5995 - acc: 0.7450\n",
            "Epoch 00145: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5703 - acc: 0.7500 - val_loss: 0.7109 - val_acc: 0.6094\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5179 - acc: 0.7400\n",
            "Epoch 00146: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5726 - acc: 0.7344 - val_loss: 1.1688 - val_acc: 0.6094\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6201 - acc: 0.7450\n",
            "Epoch 00147: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5882 - acc: 0.7344 - val_loss: 1.3567 - val_acc: 0.6406\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7010 - acc: 0.7100\n",
            "Epoch 00148: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6694 - acc: 0.7266 - val_loss: 1.0873 - val_acc: 0.6562\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5994 - acc: 0.7250\n",
            "Epoch 00149: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5787 - acc: 0.7305 - val_loss: 0.5215 - val_acc: 0.7500\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5501 - acc: 0.7250\n",
            "Epoch 00150: val_acc did not improve from 0.82812\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.5454 - acc: 0.7188 - val_loss: 0.4429 - val_acc: 0.7812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4HNXVh9+7Xb3LRbLcey+4YLDp\nvZd8lFADhBBKEkINIZBQP8hHEiChJLTEAYzpYDBgirFNcS+Si2zJtiSr97arLff7Y2ZXK2kljWyt\ninXf59lnNbMzO0dreX57yj1HSClRKBQKhQLA1NsGKBQKhaLvoERBoVAoFAGUKCgUCoUigBIFhUKh\nUARQoqBQKBSKAEoUFAqFQhFAiYJCoVAoAihRUCgUCkUAJQoKhUKhCGDpbQO6SnJyshwxYkRvm6FQ\nKBT9ig0bNpRJKVM6O67ficKIESNYv359b5uhUCgU/QohxH4jx6nwkUKhUCgCKFFQKBQKRQAlCgqF\nQqEI0O9yCqFwu93k5+fjdDp72xSFIoDD4SA9PR2r1drbpigUhjkiRCE/P5+YmBhGjBiBEKK3zVEo\nkFJSXl5Ofn4+I0eO7G1zFArDHBHhI6fTSVJSkhIERZ9BCEFSUpLyXhX9jiNCFAAlCIo+h/qbVPRH\njhhRUCgUij5DdT7sXN683VAB29/uPXu6gBKFI4BXXnmFgwcP9si1rr76apYtWwbAddddR1ZWVrvH\nfv3116xduzaw/dxzz/Haa6+F1b6amhp+//vfM3PmTGbOnMkll1xCZmZmi2MeeeSRQ3rvzn5fhSLA\n9/+ANy8HT5O2vfE1WHYt1Jf1rl0GUKLQy3g8ng63jXC4onAo1wT45z//yaRJk9p9vbUo3HjjjVx5\n5ZWHdC0jVFRUcNJJJ5GWlsbatWvZtGkTd9xxB9dddx3ff/994Lj2REFKic/na/f9O/t9FYoA1fkg\nfVBX1LwNUFfSezYZRIlCN/Haa68xbdo0pk+fzhVXXAHAvn37OOGEE5g2bRonnngiBw4cALRv2zfe\neCPz5s3jzjvv5IEHHuCKK65g4cKFXHHFFXi9Xu644w6OOuoopk2bxvPPPx+4zuOPP87UqVOZPn06\nd999N8uWLWP9+vVcfvnlzJgxg8bGxhZ2HXfccdx2223MmDGDKVOm8OOPPwIYvqaUkptvvpnx48dz\n0kknUVJS0uK9/S1HPv30U2bNmsX06dM58cQT2bdvH8899xxPPfUUM2bM4Ntvv+WBBx7gySefBGDz\n5s3Mnz+fadOmcf7551NZWRl4z7vuuou5c+cybtw4vv32WwAyMzOZO3cuM2bMYNq0aWRnZ7f5N7j9\n9tt58MEHufHGG4mIiABg9uzZfPDBB9x5550A3H333TQ2NjJjxgwuv/xy9u3bx/jx47nyyiuZMmUK\neXl5/OIXv2DOnDlMnjyZP/zhDyF/3+joaH73u98xffp05s+fT3Fxcdf/aBRHLjUF2nN1Qcvthr7v\nKRwRJanBPPhhJlkHa7r1PScNjeUPZ09u9/XMzEweeugh1q5dS3JyMhUVFQDccsstXHXVVVx11VW8\n9NJL3Hrrrbz33nuAVka7du1azGYzDzzwAFlZWaxevZqIiAheeOEF4uLiWLduHS6Xi4ULF3LKKaew\nc+dO3n//fX744QciIyOpqKggMTGRZ555hieffJI5c+aEtK+hoYHNmzezatUqrr32WrZv3w5g6Jqb\nNm1i165dZGVlUVxczKRJk7j22mtbvH9paSnXX389q1atYuTIkQG7brzxRqKjo/ntb38LwMqVKwPn\nXHnllTz99NMsXryY+++/nwcffJC//OUvgOa5/PjjjyxfvpwHH3yQL774gueee47bbruNyy+/nKam\nJrxebwsb6urqyM3N5fTTT+eHH37g5ptvJjk5mSFDhvDggw8ya9YsNm7cyGOPPcYzzzzD5s2bAU24\ns7OzefXVV5k/fz4ADz/8MImJiXi9Xk488US2bt3KtGnTWlyvvr6e+fPn8/DDD3PnnXfy4osvct99\n93XwV6QYUNTonntNK1FQ4aOBwZdffsnFF19McnIyAImJiQB89913XHbZZQBcccUVrF69OnDOxRdf\njNlsDmyfc845gW+3n332Ga+99hozZsxg3rx5lJeXk52dzRdffME111xDZGRki+t0xqWXXgrAokWL\nqKmpoaqqyvA1V61axaWXXorZbGbo0KGccMIJbd7/+++/Z9GiRYF6/M7sqq6upqqqisWLFwNw1VVX\nsWrVqsDrF1xwAaB9y9+3bx8ACxYs4JFHHuHxxx9n//79Abv97Nixg9mzZwNw55138vbbb7NkyRK+\n/PJLvF4v48ePZ+/evSHtGT58eEAQAJYuXcqsWbOYOXMmmZmZIfMINpuNs846q42dCgVeD9TqYaOA\nOOjPDeW9Y1MXOOI8hY6+0fcloqKi2t2WUvL0009z6qmntjhmxYoVh3St1qWR/m0j11y+fDk9jd1u\nB8BsNgfyHZdddhnz5s3j448/5owzzuD5559vI1B+kTWZTGRkZAAwb948AEpKStrNBwR/Drm5uTz5\n5JOsW7eOhIQErr766pBrDaxWa+BzDLZToaC+BKTuydYcBI8L6kv115SnMCA44YQTeOuttygv174F\n+MNHRx99NG+88QYAS5Ys4dhjjzX0fqeeeir/+Mc/cLvdAOzevZv6+npOPvlkXn75ZRoaGlpcJyYm\nhtra2nbf78033wRg9erVxMXFERcXZ/iaixYt4s0338Tr9VJYWMhXX33V5tz58+ezatUqcnNzDdkV\nFxdHQkJCIF/w73//O+A1tEdOTg6jRo3i1ltv5dxzz2Xr1q0tXp8wYQIbN24EwOv1kp+fT1VVFT/8\n8AP5+fl8/fXXLFiwANBu6P7fszU1NTVERUURFxdHcXExn3zySYd2KRRtqAkq+qgpgNrC5m2/OPRh\njjhPoTeYPHkyv/vd71i8eDFms5mZM2fyyiuv8PTTT3PNNdfwxBNPkJKSwssvv2zo/a677jr27dvH\nrFmzkFKSkpLCe++9x2mnncbmzZuZM2cONpuNM844g0ceeSSQuI6IiOC7775rE1pxOBzMnDkTt9vN\nSy+91KVrnn/++Xz55ZdMmjSJjIyMwI01mJSUFF544QUuuOACfD4fqampfP7555x99tlcdNFFvP/+\n+zz99NMtznn11Ve58cYbaWhoYNSoUZ1+NkuXLuXf//43VquVwYMHc++997Z4PSYmhtTUVFauXMnj\njz/O+eefT3JyMqeffjpPPfUUL774IjabDYAbbriBadOmMWvWLB5++OEW7zN9+nRmzpzJhAkTGDZs\nGAsXLuzQLoWiDf78QVSK9rM/2Qz9ItGMlDJsD+A0YBewB7g7xOsZwFfAJmArcEZn7zl79mzZmqys\nrDb7FBqLFy+W69at620zeoSioiI5e/Zs+eabb0q32y2llHLHjh3yv//9b6/ZpP42ByBrn5XyD7FS\n/vcSKZ8cL+WWpdr246OkfOmMXjMLWC8N3LfDFj4SQpiBZ4HTgUnApUKI1kHd+4ClUsqZwCXA38Nl\nj+LIZ9CgQXz22WesW7eOefPmMXXqVB544AGmTJnS26YpBhI1BWBxwKApWsK5ap+2f8i0fuEphDN8\nNBfYI6XMARBCvAGcCwSXckggVv85DuiZZbkDiK+//rq3TehREhMTeeKJJ3rbDMVApuYgxKZBXBog\n4eBmsMdB/HAo3Nrp6b1NOBPNaUBe0Ha+vi+YB4CfCiHygeXALaHeSAhxgxBivRBifWlp30/UKBSK\nAYbXA0312s81ByF2qCYMAHk/attRydBYAR2smu8L9Hb10aXAK1LKdOAM4N9CiDY2SSlfkFLOkVLO\nSUlJ6XEjFQqFokNWPgDPHaPd8P2eQuxQ7bX6Es1riEzWWl80VvaqqZ0RTlEoAIYFbafr+4L5GbAU\nQEr5HeAAksNok0KhUHQ/eeugIgcOfAe1fk9haPPrfk8B+nxZajhFYR0wVggxUghhQ0skf9DqmAPA\niQBCiIlootC3PzGFQqEIRkoo3an9vO5F8Hk0EXDEg1XrPkBsWrMo9PFkc9hEQUrpAW4GVgA70KqM\nMoUQfxRCnKMfdjtwvRBiC/A6cLVeOqXoAqp1djPhbJ0NPftZK/oJdcXgrAJhhiz9e29cOgjRnFeI\nHaqFj6DPr2oOa05BSrlcSjlOSjlaSvmwvu9+KeUH+s9ZUsqFUsrpUsoZUsrPwmlPX0S1zu4+Drd1\nthGUKPQ8B6saeXtDfm+b0T5+L2HGpc3tLfyho+DnIE+h3uXhzXUH6IvfgXs70XzEoFpn98/W2QD/\n+c9/Au/985//HK/Xi9fr5eqrr2bKlClMnTqVp556qtPPWtH9eH2Sm5Zs5Pa3tlDrDN2apNcp0UVh\n4a/BrK2ab/YQgp4jk7Sf68tZvq2Qu97exvaCLnR09g/sCTNHXpuLT+6Gom3d+56Dp8Lpj7X7smqd\n3X9bZ+/YsYM333yTNWvWYLVauemmm1iyZAmTJ0+moKAg8FlVVVURHx/f6Wet6F7+tTqHzXlaV98a\np4cYh7WXLQpB6U6ISICk0TD6RMj5qlkA4oLCR2YrOOKgoYwir9ZkMaesjqnpbXuRtaGxCv53FJz5\nJMy5tvPjD4MjTxR6gY5aZ7/zzjuA1jrb/20VOm+dvXXr1kDsvrq6ukdaZ4e6Zk+1zr744osDr7fX\nOvvhhx8mPz+fCy64gLFjx7Z4z1Cts6Ojo5k1axb3339/oHX2rFmzWpy3cuVKNmzYwFFHHQVAY2Mj\nqampnH322eTk5HDLLbdw5plncsopp3T4Oym6n5zSOv782W4So2xU1DfpnkJEp+f1OKW7IGUCCEHZ\nwvupTDuPsf7OxHN+BqkTNTEArR9SfSnFTZoo7CtrMH4N6YWYoZ0fe5gceaLQwTf6voRqnd0+Pdk6\nW0rJVVddxaOPPtrmtS1btrBixQqee+45li5d2m4zQUV4WLYhH69P8rszJnL7W1uoaeyD7cmlhNId\nMOk8AB7+volPtsex7RgfVrMJYofAlAubj49Mhvoyik0uAHLL6oxdx5+3SJ3QndaHROUUugHVOrv/\nts4+8cQTWbZsWSBXUlFRwf79+ykrK8Pn83HhhRfy0EMPBd67s89a0X3kVTYyND6CManRAH0zp1Bf\nqi1GS5mAlJLVe8pwun3klNaHPj4qGRrKKanRPIXc8i54CpYIiMvoJsPb58jzFHoB1Tq7/7bOXrJk\nCQ899BCnnHIKPp8Pq9XKs88+S0REBNdccw0+vSWB35Po7LNWdB8FlQ2kxUcQ49BuUzV9URT83+BT\nxpNdUkdpreYBZB6sZvzgmLbHRyZB3o8Uu3VPobQOKWUbb77tdXZAyjgw9cD3eCOtVPvSQ7XO7hqq\ndbZqnd1fmfvw5/L2pZtlaa1TDr/rI/nq2tzeNqkt3z+vtcWuKZT/+jZHDr/rIznqno/lnz7MDH38\nFw9K3wMJcvQ9H8op938qh9/1kSyvc3V+nT9PlPLtGw7LVHq7dbZC0dOo1tn9m8+zinl0+Q4Amjw+\nSmpdLTyFWmcfzCmU7tCSyNGDWLOnjBFJkUwZGktWYTulplEpCOklylfPnBEJgIG8grNGa8edMr6b\njQ+NCh8d4ajW2Yr+wgdbDvLx1oP85pRxFFe7kBLS4iOwW8zYLCZqGvtY+EhK2PMFDJuH2yf5Pqec\nc2emISV8sr0wdFhIr0KKEQ3MH5XEV7tKySmtZ/bwDir2SndpzynhTzLDEZRoln1wZaBiYKP+JrtG\nYVUjPgm5ZfXkV2kJ2LQELWcT67BS09c8hYOboOoATDqXrflV1Dd5OWZMMpOGxlLV4OZgtbPtOVbt\n94nExezhCVhMgn3l7SSl/fRg5REcIZ6Cw+GgvLycpKSkzhM2CkUPIKWkvLwch8PR26aEnUc/2cHu\nolqEEPzsmJEsHHNojY4L9ZtodnEdTre2ODEt3i8Klr6XaM56D0wWGH8Gq9eWIwQsGJVETpl2k886\nWBOwP4BVKwOPwEVaQgTDEiPJLTMgChaHNqSnBzgiRCE9PZ38/HzUAB5FX8LhcJCent7bZoSVWqeb\n57/JIS0+gor6JuwW0yGJgs8nKa7xi0ItJpP25W5IvCaqMRHWvpVTkBKy3oeRiyEykXX7spkwOJaE\nKBsTLCaE0ETh5EmDWp7n9xSEi+RoOyOTo8jtbAFb6U5IHgsmc8fHdRNHhChYrdbAalqFQtFz+Ffk\n/v6siXyw5SCZB7vQyyeIsnoXHp8WbssuqSPabiE1xo7dot0IYx2WvrVOoXALVO6DY2/H65Nszqvi\n3BnaauMou4WRyVFkHqxue55N60YwKEJiNZsYmRzFd3vL8flkQAjbULoLMtqWgoeLIyanoFAoep5c\nPR4+MjmaSUNi2V/ecEg378IqzUtwWE1kl9RRUNUYyCcAxDgsfSvRnPWe1ip7wlnsLa2jzuVhZkZC\n4OVJQ2JDC6Q+X2FwhBYeG5kcRaPbS3FtiPwDgKsWqvN6rPIIlCgoFIrDIFdfuTs8KZJJQ2MB2FHY\n9RXf/nzC/FFJ7CurZ395A0OD4vGxjj4UPgqEjhZBZCIb92sdfmdlxAcOGZMaTUFVI02eVvOYdVFI\ntWui4P/MfsipCH2tmkLtOWFE99nfCUoUFArFIZNbVkdafAQOq5nJQ7Vyy6xQYZNOKKrW2pAfOzYF\nj09SUNVIenwrT6GvhI+KtmmjNydr/Y42HagiLsLKyOTmXmKpMVoupLze1fJcm3ZMki4KM9LjGRzr\nYPm2wtDXcukC6zDQSbWbUKKgUCho8vj493f78Hh9nR4bTG55AyOS9W+/MXaSomyBhVtf7Sxhe4Ex\ngSiscWIzm5g3srlePzh8FOuw4nT7cHfRvrAQCB2dDcDGA5XMzIhvUfmYEqM1dSypaSkKbpO2P9Gm\neT0mk+C0KYP5encpda4QnpBL//zsIVpmhAklCgqFgq93lfD79zP5cV87YYwQSCnJLa0LfEMWQjBp\nqBZLr6hv4sb/bODRT3YYeq+iaieD4xyMSY3Gn29Na+UpQB9Y1SwlZL4HI46BqCSqG91kl9QxKyif\nAJpAAoFeSH5KndotN97S/HucMXUITR4fK3cUt72e31NQoqBQKHqS/EotfNP6JtYRFfVN1Dg9jEyO\nDuybNDSW7OI6Xl27D5fHx5a8ary+zhfxFVZpouCwmslI1DyPlolmbbhOryebizOhYm8gdLQ1X5tN\nMjMonwDNnkJpnfZ5rtpdyvFPfs3SDQU0ShuxluYpanOGJ5AaY+eTbUVtr6dEQaFQ9AYFVR2LgpSS\nT7YVUh10U94XqDyKDOybNCSWJq+P577ZS6TNTJ3LQ3ZJ54nnwppGhsRpcfgxqdoNsEWiOUIThV73\nFLLeA2FqDh3tr0IImD6spSgkRWsdef2f59q95eSW1fOXL7JpwE60aBYFk0lw+pTBfLWrhH9+m8PS\ndXm4PPpkQSUKCoWiNyjQPYWSdkRha341v1iykbuWbQ207/DPDAj2FCbr1TQuj4+7TtPaMmw6UNXh\ntX0+SXG1iyFxmggsHJPEhMExxAaN3uwT7bP9oaPhCyE6BYBNeZWMTY1uYSuA3WImPtJKiV5qWlDV\nSFp8BJfOzaBJOIg1t/w9LpiVjscneejjHdz59lae+FTvd+QXBVsfEgWh8VMhxP36doYQYm74TVMo\nFD1FZ56Cvzrm08wiPtZ/3ldej8UkSA8K84xMjsZhNTEmNZor5g8nMcoWKNlsj4qGJpq8voCncM3C\nkXz6q0UtjvHfdHt1AVvJDijPDoSOfD7JpgNVbfIJflKi7YHPs6CygeFJkTx6wVSGJCdi9TW2OHb6\nsHi2P3AqWx84hcvmZfCvNbls2F9BQVExTuxsLOi5wU5GPIW/AwuAS/XtWuDZsFmkUCjChtPtZdOB\ntjfpgx2IgpSS5dsLOXZsMtPT47j//UxKa13kltUzLDFSGzupYzYJHjl/Ko9fOA2TSTBzWDyb8kJ7\nCoXVjewpqQ0sXBsc136fqGZPoRfDR1nvASIQOsotr6e60d0mn+AnNTZIFKoam8NhtkhwN7Y5PsJm\nJtZh5d4zJjI0LoIbXtvAN9tzqJER/OWL7LD8SqEwIgrzpJS/BJwAUspKwBZWqxQKRVh4f3MBF/xj\nLfmVzf12Gpu8lNdrMe5QorC9oIa8ikbOnjaUJy6eTp3Lw/l/X8OmA1UtavP9XDArndnDtW/PMzPi\n2VNSR3VD22/4v35zM+c/u5ZNeZpIDY1rf4qdP6fQq4lmf+goRutn5A+Ldegp1LlazIYAtAVsTe33\nO4q2W3j8wmlUNbqZkADmiFhW7S4lu7hnvAUjouAWQpgBCSCESAH6QLGwQqHoKgVVTqSkxfoBf+go\nxm4JVMsE8/G2QiwmwSmTBzFuUAxv3DAfKbVVyKFEIRh/64fN+S29hZJaJz/kVlDr8vDocq01dEee\nQrS9l0tSS3ZC2S6YdG5g18YDlcTYLYxOiQ55SkqM5ikUVWufeaCayhoJ7o47ox4zNpnN95/MrFQz\ncfFJ2C0mXlqT222/TkcYaYj3N+BdIFUI8TBwEXBfWK1SKBRhoUy/6WcdrOG0KUOAZlGYPiye1XvK\ncHt9gZCQlJJPthdy9Jhk4iO1AMGsjAQ+vvUY/v71Xi6c1XEX2OnD4hECPt56ELMQTE2LIy7SyorM\nYqSES+dm8PqPB7CaBUlR7QcgzCZBtD0Mq5pLd2tTzTrBueVt7AjEpHMC+zYdqGJGRny7jexSYuw4\n3T52FmmL+QIrtK0RIcNHrYlxWMFViyUilgtmpfHOxgLuOHUCiR18Tt1Bp6IgpVwihNgAnAgI4Dwp\npbEVKQqFok9RXucfLN/crM1feTRDF4WyuuZKoH3lDewvb+CGRaNavE98pI17z5jY6fWi7RampsWx\ndH0+S9fnMzwpkk9uO5blWwsZnRLFw+dNYU9JLdWN7va7hOponVK70VNorILnjwVPO83ognAAuxzT\nGR8zGIA6l4ddRTWcfMLYds/xt7rw51QCnoItqsPwUQtctRCVzLULR/L6j3ksXZ/HjYtHGzv3EOlQ\nFPSwUaaUcgKwM6yWKBSKsFNWp+UOgmcIH6xqxGwSTEnTyklLa4NFQQtzTBh86CWRL119FLll9eRV\nNPCbpVu4951t/JBbzs3Hj8FkErx67VzqXd5O3yfGYe3e6qNdyzVBOO8fkNBx6/2fvbaeatsolunb\nW/Or8Mm2i9aC8S9g26KLQiA8Zo0EdxdEwR7L2EExvHrtXOaP6mBsZzfRoShIKb1CiF1CiAwp5YGw\nW6NQKMKK31MorHZSUd9EYpSNgqpGBsc6GKwLQXCyOa9Cu3kNS4xs+2YGSY62kxxt56gRiWzOq+K1\n7/YDcPpULXwVabMQaes8kh0bYaGmsRs9haz3IW4YTL8UOpjY6HR7WVlfQVJQfY0/yTxzWOeisDW/\nusVsCC18ZFQUagIL1xaPSzF2zmFiJNGcAGQKIVYKIT7wP8JtmEKh6H7K6poYN0hLjGbpIaSCSm1h\nVUqIfj0HyhtwWE2kRNu75fp3nTaB9IQIRqdEddn7iHFYqXV1k6fgrIa9X2qJ405G+PrLdSsamgIN\n+bbma5VX/jxLKPyfWZ3L06JlB7YoTRQ6m+Etpe4p9NzCNTCWaP79ob65EOI04K+AGfinlPKxVq8/\nBRyvb0YCqVLK9qVXoVAcMk63lzqXh0VjU9hdXEdWYTXHjE2moKqRuSMTSW7VmgHgQEUDGYmR3Tb7\nPMpu4a0bF+Dxyi6/Z4zDwp6SbvIUdn0C3qYW1UTt4U/ESwnldU0MjnNQUNXI8KSOvaf4SCtWs8Dt\nlS1nNesjOXE3BiaxhcTdCNLb90RBSvnNobyxno94FjgZyAfWCSE+kFJmBb33r4OOvwWYeSjXUigU\nneOvPBo7KJqhcQ4yD9bg8fooqnGSFh8RaM0QXJZ6oKKBYQmHHjoKxZAO1iN0RGx35hSy3ofYNEib\n0+mhfk8BNMEcHOegqNrJ1LSOZxwIIUiJtnOw2tnSU7DqZbzuho5FoRf6HoGxNhfzhRDrhBB1Qogm\nIYRXCGFkEOtcYI+UMkdK2QS8AXQky5cCrxszW6FQdBV/kjkpys6kobFkHayhuNaF1ycDN62UaHtg\nBoCUkryKhsPKJ3QnMXr1kews7NIZbifsWQkTzwZT5xF0f3UWQGmdE5fHS1ldkyFx84fkWngKfiHo\nLK/g0m+z9thOr9OdGMkpPIN2w84GIoDrMNbmIg3IC9rO1/e1QQgxHBgJfGngfRUKxSHgTzInx9iZ\nNCSWvaV1PLlCa7zmb8GQEmMPeAoV9U3UN3kDrax7m9gIKx6fpNGtVSpV1jfx0upcfAZac7egPBu8\nLhhmrIVbflUjVrMW6iqtdVFcrX0+HS228xNSFPzho87KUgOi0Mc8BQAp5R7ALKX0SilfBk7rZjsu\nAZZJKUPWpQkhbhBCrBdCrC8tLe3mSysUAwN/+Cgpysbi8SnEOKx8ur2IQbGaSEDzKlzQQkdAnxGF\nMfrK4aXr8pBScvc7W/njR1nsKDISuAiiRK+uT5lg6PCCykYmDmku1y3UR4cO6YootBc+6ojAKM6e\n9RSMJJobhBA2YLMQ4n+BQoyJSQEwLGg7Xd8XikuAX7b3RlLKF4AXAObMmXOYvqNCMTDxh49SYuwM\nS4xkyx9OaXOMv7OnlJI8PWyS0UlCtac4cWIqx41P4fFPd9Hg9rIiU5tUVlnfxTxD6U5tnGbSmHYP\nueG19UwaGsuvThpHQVUjs4cnsL+8gZJaF0U12mI3I6IwKFY7JnSi2aAo9EFP4Qr9uJuBerQb/YUG\nzlsHjBVCjNRF5RKgTSmrEGICWtnrd0aNVigUXaeszkW03YLDam73mJQYO41uL/VN3sAaheDW2L2J\nEFoHVotJ8L+f7grclCsamjo5sxWlOyFxFFhCl9lKKVm9p4x3NxXg9UmKqp2Bkl3NU/B3de38c7ls\nXgbP/XR2YHIcoJWkgoHwUR8VBSnlfimlU0pZI6V8UEr5Gz2c1Nl5HjQhWQHsAJZKKTOFEH8UQpwT\ndOglwBvysLNHCoWiI8rrmgITwdojeK3CgfIGkqPthhaW9RRD4yP443mTSY2x89T/zAC03EKXKN0F\nqe2HjmqcHhqavOwvb2DD/kqF7iZfAAAgAElEQVQ8eiLe70UVVTuJcVgCTfo6IjXGwWlTBrfcaTWa\naPaLQh8LHwkhFgIPAMODj5dSjmrvnKBjlgPLW+27v9X2A8ZMVSgU7fHGjwdYvr2Ih86d0m64p6zO\nRXIni9D8/Xpyy+r0NQp9w0sI5vyZ6Zw3Iw3fwS18aLuXNZXPAyOMnexxQUVOYFBOKIqqm3shvbVe\nq5UZqnsKW/KrOFjVaCh01C6Gw0d6rsQWugtruDASPvoX8H/AMcBRQQ+FQtFH+HDrQVbtLuXMp7/l\ns8wQA+DRPYVOOmzOzIhnSJyDR5fvJLesvs8kmVsjhMC88RWmmvYxrPAT4yeW79EWhHWQZD5Y3VyC\n6p8yl66LQkmNFj4yEjpqF1sXEs0WB1h6dnyNEVGollJ+IqUskVKW+x9ht0yhUBgmu7iOY8cmMyo5\nipuWbGRHYduKnLI6F8kxHXsKUXYLj1wwleySOopqnH1WFPB6YMeHAIwv70Ile2nnlUd+T2HeyEQa\nmrSCyLSECFL1fEtuWT1DYrvBU+gsp+Cs6fF8AnQgCkKIWUKIWcBXQognhBAL/Pv0/QqFog9Q3eCm\npNbFMWOSeeWaucRHWrlj2ZZAnx4Aj9dHRUMTyQZ68R8/PpWLZmtzEvrKwrU2HFgLDWXkWEYzqnEb\n1Ib2jtpQshOECZLGIKXkzmVbWLunrMUhhdVOhIALZmnLqhIirUTaLIF8S53Lw5D4wxGFLuQU+pIo\nAH/WH/OAOcAjQfueDL9pCoXCCNklWkJy7KBoEqJs/OncKWwvqOH5b/YGjqlscCMlnXoKfn5/1iSu\nPnoEx41PDYvNh03me2CNZEnq7ZiQAa+hU0p3am2yrQ6qGtwsXZ/Pyp0lLQ4pqm4kNcbO4nHa7x68\nsM/PYeUUTGYw2/usKLSbaJZSHt/eawqFou+QXVIHwNhU7QZy+tQhnDltCH/+fDdur+TWE8cGFq51\nlmj2Exdh5YFzJofH4MPF59VEYOwp1JqmkFM4jFGZ78Hc6zs/t3QXpGrDgfyL81pXL/lzBoPjHEwe\nGsv4QdrnGiwKh5VTAK3VhZGS1B6uPAJj1Ue3AS8DtcCLwCzgbinlZ2G2rXvZ/g5seEX7OXYonPus\npth+aovho19BU8ezUxWK3qRYX1g2OCimPa+sntftTtI/fD6w728Sbkuso2SVkx3rraTGOPiPtZap\na+JgozXUW/cfPE6oL4HJ55FwwMbH3rncvP8dxKtnow2H7IDyPTDxLKBZFFqvcyisdgZWT//3uvlY\n9BYXwe3DD8tTAG1Vc2cjOV21EJ9xeNc5BIwUIF8rpfyrEOJUIAltMdu/gf4lCj6vVo5WVwS538BJ\nD0LMoObXCzdrk5gGTenxEjCFwijlVTU4PV5SI+Ix6a2n3a5G4qwS4WnubmoGxiVaSbD5OFDRwIHG\nBuwCbMINHl87795fEDD+TBh7KollhbzatJibJpRi9hhYrzD8aJioLZNqz1MoqnZyzJhkAOIimwU0\nIdKGxSTw+GQ3iEIEuDv5AurqnUSzEVHwS+8ZwGv6ArTuaa7ek0y7WHtseAU+vE0rSwvGp/dpP+/v\nMGR6j5unUBjh+se+pKCukZePOYrjJ2gx7ysf+YKFY5P5P30xVzApQHlRDTct2Uh+ZSMbrjoJHP3c\nUwgiIcrGQZI5eM6bXU6K54XwFGqdbi2RHOKmbzIJkqPt1Lk8LVcoHwq2SGOeQh9LNPvZIIT4DE0U\nVgghYoD++1XDpOugr9WwDv+2qe+s3lQMTNxeH//8NocN+yta7JdSBnID/vr56kY3xTUuxg5q/+Yx\nYXAsH91yDJ/cduzh38z6GIn65LPKrra6APIq/Z5Cc+8kfznqkPjQOYOUGLuh7qidYo3sOFTdS1PX\nwJin8DNgBpAjpWwQQiQB14TXrDCiREHRhzlY1cjN/93IxgNVnDZ5MLOvaB7UXufy4PL4MJsEn2UW\n0XT+VPYEkswdhzwjbRZGpxx5YdEEvcS2oqutLmgOH9W5PDR5fNgspkBfo/bCQ+dMH0qTtxu+E1sj\nwVnV/useF/jcfVMUpJQ+YGPQdjnQfxevBUShdfjI2/J1haIXuOqlHymsdjIkzkFJrbPFa+V6l9PT\nJg/m422FrN1bFvhmO3bQkXfDN0KiLgpVDV3rlOr2+jhY5SQxykZFfRNVDU2kxjoCn+fgdhanXb+o\n0+4+xrBFQm1h+6/30iwFMDhP4YjCX3HUrqfQfgdJhSKc1Lk8ZJfU8YvjRrNgVBLFNa4Wr/tDR+fN\nTCPabuGpz3fzxro8HFYT6d08MrO/4A8fddVTKKxy4vVJpqdrIzX9eQV/i4tBh7Ni2QidhY96qRke\nDERREJ2IglCioOgd9pVpN4lRyVGkxmqeQnDzYL8oDIlzcPn8DPaW1rOnpI6TJw3GbOp/tR/dQYzD\ngtkkupxT8IeOpqXHA82iUlTtJDnajs0S5lujtZNEc/F27Tku5LDKsGJkncJoIF9K6RJCHAdMQ6tC\n6iAg1odR4SNFHyVXF4WRKVEU1ThxeyWVDe5AiCR4SM49p0/kntMn9pqtfQWTSZAQae2yp+AXhRnD\nNFHwJ5v9obuwY43seEVz1vsQmQTD5offllYYkcO3Aa8QYgza9LNhwH/DalU4aVcUVKJZ0bv4RWFE\nUlQgfFFc05xX8HsKiQb6Fw0k4iNth+QpWM0iMGbTHz4q6ilRsOnho1BjZNyNsOtTmHg2mHv+fmRE\nFHz6wJzzgaellHcAQ8JrVhhpN6fgbfm6QtHD7CurZ2icA4fVzKBYbfVssCiU1zURH2nFah54Ud+O\nSIy0ddlTyKtsID0hMjB0qLK+CSklBVWNgV5HYcUaAUityqg1e77QFrZNOjf8doTAyF+XWwhxKXAV\n8JG+r/8WO6uSVEUfJaesnpEpWq99/7CbkqBks5EhOQORhChrl+c051U0MCwxEqvZRIzDQkV9E6V1\nLupcHkYmR4XJ0iCsHcxUyHofIhJhxKLw2xECI6JwDbAAeFhKmSuEGInW5qJ/okRB0QdYt6+CWmfL\nG1luWT0jknRR0D2FolaeQmdDcgYiiVG2Ls9pDp4qlxilhZ9yS/XwXU+Igq2d9tlupx46OqtXQkdg\nbEZzlpTyVinl6/p2rpTy8fCbFiaUKCh6mZ1FNVz83He8sConsK+yvonqRnfgW6rdYiYh0tomp2C0\n9fVAIiHSFgj/GKHO5aGqwR0o403Qw0/7ypurv8KOv9S0oeWqdfLXQVMtTDgr/Da0Q6eiIIRYKIT4\nXAixWwiRI4TIFULkdHZen8WfM5CtViX6t1VOQRFmXlqdC8C32c3DXXL8lUdBN6RBsY4WaxXK6lyG\nhuQMNBKjbHh8klqXp/ODac7T+BPKCZFWKhuayCmrx2Y29UxOIXms9ly2u+X+kh3a8+Bp4behHQ5l\nRvMc+vOM5s4WrwmVxFMYw+eT7Cqq7dI5ZXUu3tt8kEibma35VVQ3aiGk3HZEwb+q2eXxUuP0qJxC\nCBIim5PFRvCLgj9vkxBlo7LeTW5pPRlJkT2z5iNpjLYmyj8e1E/pTnDEQczg8NvQDgNvRnNH4SOT\nBfphA1hF7/BZVhGn/mUVW/ONL9lZ8v0Bmjw+7jtzEj4J3+do/5X2ldVjNokWnT4HxdoDNzB/dU2S\nEoU2+Et0yw2Kgj9576/wStRLWveV1/dMkhnAYofEUSFEYZc2P7oX70NGROHImtHcmSgoFAbJLtaa\n0X20tYMeNkE0eXz8+/v9HDc+hYtmpxNhNQfmA+eW1TMsIaJFuemgWAeltS68PklZrXbDS45W4aPW\n+CeildWGKO8MQcBTiG32FBqavOSW1fdMPsFPynhtZnQwpTu1/b2IkbvgPP15TtA+CZzQ/eb0AB2t\naFaioOgC/lWxH28t5J7TJ9DZmJFNByopq3NxyVHDsFlMzBuVyGpdFHLK2n5LTY114JNQXudqHqep\nEs1t8ItCiUFRKKpxEm23EG3X/r/7PQ23V/ZM5ZGflAmw6xNtrYLFDvVl0FAGKb27Ut1Il9Qja1Zz\nRzkFlWRWdIEDFQ0IAQVVjWzNr2a63jKhPdbsKcMkYMFobarXwtHJPLxrB099vpvdxbUsGpfc4vhB\nMf4FbEGiEKVEoTVJUTaEgFKDolBS4wqU/EJzTgLoufARaLOipRfK98KgSc2hpF72FIxUH8UJIf5P\nCLFef/xZCBHXE8aFBRU+UnQTeRUNHD8+FYtJsHx72xCS0+3l4Y+zAuGK1XvKmJYeT1yEtvZzoT7y\n8a8rs1k8LoWbFo9pcX5wqwt/36PkGBU+ao3FbCIpykZpXWhRcHt9PP7pTg5WaQ3oimucDIppbmUR\n3Dakx8NH0CwGAVGY0HM2hMBITuEloBb4if6oAV4Op1FhJdAlNUT4SHVIVRjE5fFSWONkalocC8ck\ns3xbIQerGlu0W1iRWcSL3+byzJd7qHW62ZJfHZj9CzBhcAznzhjKPadP4J9XzmkxDxiCRKHWSXmd\niwirmUib+uISiuRoe7uewqrdpfzj6718uOUgoH2eg4I8hcQo7XOPspkDoageIWmsVu3oF4OSndr6\nhdihPWdDCIz8hY2WUl4YtP2gEGJzuAwKO8pTUHQDBZWNSAkZiZGkJURw57KtHP3YlwAs/fkC5o5M\n5JNtRQAs25DPjGHxeH0y4B2A1uHzr5fMbPcaydFaWMQfPlJeQvukxLQvCv7RpdkldUgpKa5xtZiX\n4A8fjUiO6jQv1K1YHZAwsqWnkDK+1ysgjXgKjUKIY/wbQoiFQCcTp/swKtGs6ISdRTW8tT6vzf7i\nGievrMlFShlIMmckRXL+zDT+8j8zePzCqSRF2Xhh1V7qXR6+2lXCglFJNLq9/OnjLBxWE7OGd5x3\nCMZiNpEcbeezzCLW7askSeUT2iVYFJo8Pv7x9V6qGppo8vj4PKsYgOziWqob3TR5fIHKI4C4CCtC\n9HA+IWD4BK0MFfRy1N7NJ4AxT+EXwKt6HkEAFcDV4TQqrKhEc79BStmz39x07nt3OxsOVLJoXEqL\nb5Qvrcnl+W9ymDsyibxK7XtRht5U7byZ2jCUgionT3+ZzUurc3F5fNx20ljESli7t5xjxyZjt3Tt\nb+y4cSl8pt/ULpjV8wNX+gt+UZBSsm5fBY9/upPMg9VcOCudWqeHUclRZJfUBXpJBYePLGYTJ08c\nxEkTB/WC4eMhewVUF0B9Sa/nE8BY9dFmYLoQIlbfrgm7VeFEhY/6BVvyqrhpyUae+p8ZzB2Z2PkJ\n3Xjd9fsrAfh0exFXHT0i8NoavXx0zZ4ySutc2CwmUlotJvvp/Aye+3ovT32xm+RoO0eNSORnx4xk\n7d7yFvkEozxx8XSeOPRfZ8CQEm2nyeujptETaBny0dZCsgpriHFYuGLBcB78MIvNB7SFhq3Hbb5w\n5Zw279kjpE7S7j1PTdK2e7kcFToIHwkhfqo//0YI8RvgOuC6oO1OEUKcJoTYJYTYI4S4u51jfiKE\nyBJCZAohwj+8p4ui4PNJlm3Ip8YZujWvzyf5YMvBQGUDwDe7S3nmy2ye+TKbLXk9N6CuutHNW+vz\ncLq9nR/czWzLr+bT7UWGm5L5Ka9zsWxDPm5vcy8ql8fLb9/aQkFVI898tafF8R6vj7fW55Ff2cHU\nqlas2l3K9oLqwPb2gmo2HqgMbO8urmXlDu3b+L9W5xJttzAyOSoQiwZtRXHmQe370Jq9ZRwob2BY\nQgSmVi0RUmMcnD19KD4Jp0/RxmSeMCGVv14yg8vnDzdss6Jr+MNBpXVO9pXV47CamDw0lpzSek6e\nOIjJQ7WCyW91YQ+uPupVJpwJpz4CJ/weTnscRi3ubYs69BT8AbaYEK91+j9fCGEGngVOBvKBdUKI\nD6SUWUHHjAXuARZKKSuFEKmGLT9UTB1UH4UIH/24r4LfvrWFc7OHtkkKVtQ38es3N/PN7lKmpsXx\n7k1Hs7Wgmmte/hGf/gk99UU2d502nuuPHRXWUMjW/Cp++d+N5FU0sqekjnvO6JlvHFJK/rU6l8c+\n2YnHJzl/ZhoPnTeFKHvnXte6fRXc8t9NFNU4Kapu5OYTtCZhf1uZTXZJHceOTWbV7lKyi2sZOyiG\nomont7y+kXX7Kol1WPjzT2Zw8qT2XX6n28tDH2fxn+8PkBRl47NfL8Lrk1z24vd4fJIVv1pEXKSV\nq176kcJqJ2dNGxLwDmIcFv66MpuSGiepsQ6+21uOlDAtPY4fcipIT4ggI6glRTA3LBrF17tKuHhO\nOgBCCM6doUI/4cTvsZXUugItyJ+8eDqX//MHLp4zjLGp0QCBFeTB6xR6FVskLPhlb1vRgnb/50op\nn9d//EJKuSb4NT3Z3BlzgT1Syhz9nDeAc4GsoGOuB56VUlbq1yzpgu2Hhr/sVLYSBRlaFPwhg/c3\nH+TMqUOIdlj43bvbKatz4fL4QMJP5qSzdH0+f/tyDx9vPcjgWAef3LYIieSed7bxyPKd/JhbwZMX\nTyc+qHnXve9uo6rBzZM/mU58hJU/fJDJnpI6nrx4OsMSI3h0+U7e3VSAL8S37yFxDl65Zi5D4yP4\nPqecK//1I8nRNk6ckMqL3+Zw2pTBFFU7+dNHWdS6PNjMJq5fNIobjh3F+v2V3PfeNk6aOIjfnDwO\ni95aYXdxLb99a0ugOZufReNSeOT8qbg8Xu5athWAxy+aht1s5o5lW/gsq5hTJg1iwuAYnv5qD8u3\nFYYcfJ4SY+exC6YxZ3gCL3ybwxMrdjEsIYJjxybz15XZnDxpMJkHq3numxwump3OvWdMZMGjK3lp\nzT5OmzKYX7+5GafbywNnT2LZxnyuf209f/mfGZw3Mw2vT3LJC9+xM6hBnccraXR7+cmcdN7dVMAf\nPsjE5fHh8viwmAR3vb2VjMRIimucXHLUMN5cn4cArj56BI1uL3/5IpsVmUVcsWAEa/aWEW238IvF\no/nFko1kl9Rx9OikkH9i4wfHsOH3J4d8TREe/KWkpbooTBwSw8QhsWwM+ndIjrZTVuciLsKKw6ry\nh+1hJIj+NNC611Gofa1JA4JLOPJpbpnhZxyAEGINYAYekFJ+2vqNhBA3ADcAZGRkGDC5A0wmrTbY\nYPho9Z4ypqbF4fFJbn9rC/UuDyOSo7hodjpmIThvZhpT0uKod3n528psAF67dm6g5vzvl8/i1bX7\neHj5Ds7822puPG40SMlz3+RQUuvEZjZx5t++JTHSxr7yeqLtFs55ZjUZiZHsLKrl7OlD2/S7kRLe\nXJfHPe9s4++Xz+KOZVsYGu/g3ZsWYjELTn1qFVe/vI7qRjdT0+I4dcpgckrreeyTnXyyrZDtB2uI\ndVj4+9d7Wb+/kgtmplFe38QzX+4hym7hwlnpgaq4xiYvyzbkszW/isYmH3UuLYx25t9WY7eYKKp2\n8vuzJnHtwhEIITh6TDKfZRYjQziTX+0s4dIXv2fK0Fi25FdzxtTBPHbhNNweHyc/tYr/eeE7qhrc\nzB2ZyO/PmkRchJULZqXx1vo83lh3gHGpMTx7+SzGpEZz6bwMLvzHWv72ZTbnTB/KFzuKWbevss3n\ntXhcCseNT2VYQiR//lxrU/y7MyYSZbdw77vbWLu3nJ8vHsU9p0/kvJlplNa6Ak3pxqRG89HWQn46\nfzhr9pQxf1QSR49JxiTAJ2nRvE7Ru/hFobDaSV5FA6dPadtldGxqNGV1rhZJZkVb2hUFIcQC4Ggg\npVUOIRbtBt5d1x8LHAekA6uEEFOllC0C8VLKF4AXAObMmdO1oHUoTBZDolDjdLM1v5qbjhvNqZMH\nc9Fzazln+lAePn9qm/DIg+dOZsP+Sk6eNIhF41IC+4UQXL1wJDMyErj5vxv5/XvbAUhPiGDZjUcT\nG2Hll0s2UlLr4j8/m8eolGhueX0ju4pqeeGK2ZwyOXQL3RFJkTzwYRYXP/cd+ZWNvHnDAhL0lZmP\nXTiNn726jmsWjuCe0ydis5iQUvLad/t5+OMdnDZlMI9dMJUvdhTzu3e382OuNuhj/qhE/nbJzBbl\negA/OWoYt/x3E3ERFpZcp+n6TUs20NjkZemNC5iVkRA4dv6oJOaPCv0N+jcnj+Pud7bxeWYxD54z\nmSsXDA+E1B46bwq3vr6JXx4/ml+f1Oy9/OyYkby36SDnTxvCH8+dQoRN+9OzW8xcf+wobntDC9/9\na3UuafERPPWT6YFzg7nxuNF8tasEq9nEtceMxCTgix3FFFY7+fVJ4wK2B3P+zDSeWLGLG/69gf3l\nDVx99AjiIqxMS49nc15Vu+EjRc8T67Bgs5jYuL8Sj0+GLC8dOyia73LK2ySZFS3pyFOwAdH6McF5\nhRrgIgPvXQAMC9pO1/cFkw/8IKV0A7lCiN1oIrHOwPsfOiFFoe06hR9yKvD6JEePTmZKWhxb/nBK\nuyWFydF2Vt15PFZz6LzBjGHxfHn7cZTXa7XUSVH2QIjlw1uOwe31BVzapT9fgMvj69DFvXLBCJZv\nK+LHfRVcffSIFhU6i8alsP3BU1vYKoTgqqNH8JM5wwI31vNnpnPKpMHUON2YhCA1xh4y7zErI4Gv\nfnscJkHghrviV4vwSUKGidojxmHl2ctm0djkDdjg54ypQzhxYmqbz3dMagxb/nBKyOucMXUIjy7f\nyZ8+yiKnrJ77zpwYUhAArGYTS3++ACFEoF/+P6+cg1fKFp1Jg7lx8WjcXh9/1T1Af/XQMWOS2ZxX\npTyFPoTQ/37X7dO+4IxKCSUK2m0sta8kmfsoHeUUvgG+EUK8IqXcfwjvvQ4Yq890LgAuAS5rdcx7\nwKXAy0KIZLRwUvinupksIRLNHjC3bDOwZk9ZiwVHndWYd3aDtFlMDIlrO9XJbBKYTS1v4J3FPE0m\nwVOXzOCNHw/wi+NGt3m9PVtb34yj7BZDSeHWv1t7N18jtLbBT3s2t/e5Ws0mrjx6OP/76S6ibGZ+\nctSwkMf5aW2zySQw0X7y32wS/OqkccwdmcimA1WM0ZOVl8/PQCIZNyhUDYait0iJsZOvrx/xz7oO\nxp9sVuGjjjHyP/ufQojAMkwhRIIQYkVnJ0kpPcDNwApgB7BUSpkphPijEOIc/bAVQLkQIgv4Crij\nRwb4mMyGwker95Qxd2RSlxcc9RRp8RHcfsr4Ad0P57K5GcTYLVw6N4NYh7XzEw6Bo0cn88vjxwS8\nqCFxEdxx6oSemdClMIy/AinWYWnR5M7PhMExRFjNAXFXhMbI3SQ5OMbfldJRKeVyYHmrffcH/SyB\n3+iPnkOYO22IV1LjZE9JHRfPTu9R0xRdIz7Sxld3HBfoPKoYuPiTzSNTokOGQeMjbay+6/gWrbIV\nbTHiKfiEEIGSHyHEcAysU+jTGEg07y3VyjL9i14UfZfkaHu7eQHFwCEgCknt53qSou1tFhwqWmLE\nU/gdsFoI8Q1a76Nj0ctD+y0hcwot1yk0z8RV3yoUiv5AQBSSVXjocDDS++hTfSbzfH3Xr6SUZeE1\nK8wYyClUBKqElCgoFP0Bf1XRiGRVFXY4dNT7aIL+PAvIAA7qjwx9X//FQPioXPcUEpQoKBT9gpkZ\n8Swel8KCdlaaK4zRkadwO1obij+HeE0CJ4TFop7AiCjUNREXYVWxaoWin5AcbefVa+f2thn9no7W\nKVyvPx/fc+aEn/zKBiIbvST4PC0r1EPkFFToSKFQDDQ6anNxQUcnSinf6X5zwovXJ7nl9U08VOsm\n0u2mxbrGVg3xyutdIWudFQqF4kimo/DR2fpzKloPpC/17eOBtUC/E4WX1+Sy6UAVHpsZn6fVfIQ2\nieamkKsiFQqF4kimo/DRNQBCiM+ASVLKQn17CPBKj1jXjeSU1vHEil0kR9vwNpnwejqrPmpi9vCe\nm/ilUCgUfQEjWdRhfkHQKUarRupXrMgsxm4xcd+Zk/Bgxudt7Sk0N8Tz+SSVDW6VU1AoFAMOI4vX\nVuq9jl7Xt/8H+CJ8JoWHXxw3mgtnpXGw2kmjNCO9oTwFLadQ3ejG65Mqp6BQKAYcRhav3SyEOB9Y\npO96QUr5bnjNCg+psQ6qG93UYsIXUhS0jyPQ3lqtZlYoFAMMo+01NwK1UsovhBCRQogYKWVtp2f1\nQSLtFnyYkB00xCuv0xauKU9BoVAMNDrNKQghrgeWAf6ZzWlocxD6JVE2Mx5ahY+k1EtSNY0M9D2K\nUn3XFQrFwMJIovmXwEK0iWtIKbPRylT7JZE2C15MyOAVzX6vIRA+Us3wFArFwMSIKLiklE3+DSGE\nhX7cOttmMeETrRri+X/WE81+T0H1XVcoFAMNI6LwjRDiXiBCCHEy8BbwYXjNCjOtex8FRKE5fBSj\nDwJXKBSKgYSRu97dQCmwDfg52iS1+8JpVLgRJgtCBiWaW4lCWZ1LrVFQKBQDkg6rj4QQZuA1KeXl\nwIs9Y1L4EWYzImROoTl8pCqPFArFQKRDT0FK6QWGCyGOqDukMFlA+pp3yLaikBStKo8UCsXAw8g6\nhRxgjRDiA6Dev1NK+X9hsyrMCLMVk7v9nEJ5fRMzhsX3gmUKhULRuxgRhb36wwTEhNecnsFktmBq\nJ6cgpaRShY8UCsUAxUibiwcBhBCx2mb/XMkcjNlibVcUaho9eFTfI4VCMUAxsqJ5jhBiG7AV2CaE\n2CKEmB1+08KHyWzBRLAoaD83SRN5lQ2AWrimUCgGJkbCRy8BN0kpvwUQQhwDvAxMC6dh4cRssWIO\n4Snc/tZ2PvRqg3VSoh2hTlUoFIojGiOi4PULAoCUcrUQwtPRCX0ds8WCCR8+n8RkEkGeguCGRaMY\nnhTJvFFqwI5CoRh4GBGFb4QQz6PNU5Bo8xS+FkLMApBSbgyjfWHBYrZiwUej20uUvXl1sxczV8wf\nzrDEyF62UKFQKHoHI6IwXX/+Q6v9M9FE4oRutagHMFutmISk3tWki4LmKXgwEWEz97J1CoVC0XsY\nqT46vicM6UmsFisADY0uiI1s4SlE2YyOmFAoFIojjwHZ8c1i1UXBpU1YCxYFh3VAfiQKhUIBhFkU\nhBCnCSF2CSH2CCHuDifxI/0AAA9GSURBVPH61UKIUiHEZv1xXTjt8WPVRaHR2VIULBYLQoieMEGh\nUCj6JJ3GSoQQdimlq7N9Ic4zA88CJwP5wDohxAdSyqxWh74ppby5i3YfFv7wUWtRsFrV2gSFQjGw\nMeIpfGdwX2vmAnuklDn6kJ43gHO7Yly4sNo0UXA2ubUdenM8m+5BKBQKxUClXU9BCDEYbR5zhBBi\nJuCPq8QCRmo204C8oO18YF6I4y4UQiwCdgO/llLmhTimW7HpHkGb8JHyFBQKxQCno/DRqcDVQDrw\nZ5pFoQa4t5uu/yHwupTSJYT4OfAqIUpchRA3ADcAZGRkHPZFbTbt5u9s0qeM6qKgPAWFQjHQaVcU\npJSvAq8KIS6UUr59CO9dAAwL2k7X9wVfozxo85/A/7ZjywvACwBz5sw57PnQAVFoVX3k369QKBQD\nFSM5hdlCiMBwASFEghDiIQPnrQPGCiFG6kN6LgE+CD5ACDEkaPMcYIeB9z1s/Ilml8vvKWiL15Sn\noFAoBjpGROF0KWWVf0NKWQmc0dlJUkoPcDOwAu1mv1RKmSmE+KMQ4hz9sFuFEJlCiC3ArWjhqrAj\nzJqD5GodPrIrT0GhUAxsjCzfNQeXoAohIgBDsyqllMuB5a323R/08z3APcbN7SaE1srC5a8+0j0F\nh0o0KxSKAY4RUVgCrBRCvKxvX4OWEO6/mEJ7CnblKSgUigGOkd5Hj+vhnZP0XX+SUq4Ir1lhRheF\npiYt0ezzejABdpVoVigUAxyj3d92AB4p5RdCiEghREy/HsvpFwW3W39uwgE4lKegUCgGOEbGcV4P\nLAOe13elAe+F06iwY9JyCm53U4tnFT5SKBQDHSPVR78EFqItWkNKmQ2khtOosKN7Cm490dykP0fY\nDeXPFQqF4ojFiCi49N5FAAghLGjDdfovfk/B09JTiFCegkKhGOAYEYVvhBD3ovVAOhl4C609Rf/F\n7ym4taojt0d7Vp6CQqEY6BgRhbuBUmAb8HO0dQf3hdOosKN7Ch6PGyklHj3hHOFQnoJCoRjYdFh9\npM9EeE1KeTnwYs+Y1APonoJJenF5fHg8brxSEKXCRwqFYoDToacgpfQCw/XeRUcOuiiY8VHv8uBx\nN+HBTKTN3MuGKRQKRe9iZJ1CDrBGCPEBUO/fKaX8v7BZFW50UbDgpcbpwevx4FWioFAoFIZEYa/+\nMAEx4TWnhwh4Cl5Ka13YvW48mIiyG13Lp1AoFEcmRnIKMVLK3/aQPT2D0KJmZnwU1ThJ93jwYcJu\nMZJ3VygUiiMXIzmFhT1kS8/h9xSEj5IaJ16vFj4SQnRyokKhUBzZGImXbNbzCW/RMqfwTtisCje6\nKESYfRTXOJFeNz6h8gkKhUJhRBQcQDktZydLoN+LQoLDRG6NC5/Ho0RBoVAoMNY6+5qeMKRH0Rev\nxdtNFNc48fmUp6BQKBRgrEtquhDiXSFEif54WwiR3hPGhQ3dU4hzCEpqXUivBylU5ZFCoVAYKbd5\nGfgAGKo/PtT39V90UYi1mfScggepPAWFQqEwJAopUsqXpZQe/fEKkBJmu8KLHj6KtQsamrxaQzyT\nEgWFQqEwIgrlQoifCiHM+uOnaInn/ovuKcTYtBJUj9uNVKKgUCgUhkThWuAnQBFQCFwE9O/ks754\nLVrv6GTGCyqnoFAoFIaqj/YD5/SALT2HEGCyEG3VNi14EWYlCgqFQmGk+uhVIUR80HaCEOKl8JrV\nA5gsROmiYMYXCCkpFArFQMZI+GialLLKvyGlrARmhs+kHsJkwSYk0XYLFuFFKFFQKBQKQ6JgEkIk\n+DeEEIn/397dx9ZV13Ecf3/assLY5oZ7QLeFDZgPAwXmQqYoEjC6IWFEQIeIqCT8gxEMiTJRjBj/\nwAdQE+RBUIcugCDogqjAxBkSNyjI48akPAhdhpsRpmi20d6vf5zfPb29bbdu9vaceT6vpOn9nXva\nffdrTz/39/vdcw4jOxO63NQOtV6mT+qknRpt7V5oNjMbyR/37wB/knRbap8JfKN1JY2Rtnao9XHw\npP1p31ZD7fsVXZGZWeFGstB8k6Qu+q999JGIWN/assZAWwfUepkxaX/a6aPNC81mZiObBkohsO8H\nQaMUCtMnddJBzaFgZsbI1hT+P7V1QK2PGRPTSKHD00dmZhUOhWyh+bDpE+igj85x44quyMyscC0N\nBUmLJW2U1C3pkl3sd7qkkLSwlfUMkKaPjp83lUOmdDK+s3PM/mkzs7JqWSik+ztfDSwB5gNnSZo/\nxH4TgQuBda2qZUht7RB9SKJDNV8Qz8yM1o4UjgW6I+K5iNgJ3AIsHWK/rwNXANtbWMtgaU0ByD47\nFMzMWhoKM4GXGto9aVtO0gJgdkT8uoV1DC2tKQDZZ5/RbGZW3EKzpDbgSuDiEex7vqQuSV1bt24d\nnQLSmgLgUDAzS1oZCpuA2Q3tWWlb3UTgSOAPkl4AFgGrhlpsjojrI2JhRCycNm2U7u/jUDAzG6SV\nofAQME/SXEnjgGVkt/UEICK2RcTUiJgTEXOAtcCpEdHVwpr6DVpTcCiYmbUsFCKiF/gs8DtgA/Dz\niHhK0uWSir8/g9oGjhRU3VM2zMzqWvryOCLuBu5u2nbZMPue0MpaBmnrgN4d2WOPFMzMgEqf0ew1\nBTOzZg6FWg0Ih4KZGZUOhex+CvlowSevmZlVORTqI4Xe/raZWcU5FBwKZma5CodCdkE8oq+/bWZW\ncRUOhXTyWv0ENo8UzMyqHArtTdNHHimYmVU4FLymYGbWzKHgUDAzy1U8FLymYGbWqLqhUL8gXn2k\n4AvimZlVOBQ8UjAzG6TioeA1BTOzRtUOheiD2uv9bTOziqt2KAD07hzYNjOrsAqHQjpZrXf7wLaZ\nWYVVOBTSyKDPIwUzs7oKh0J9pLBjYNvMrMIqHAr1NYUdA9tmZhVW4VDwmoKZWbMKh0J9pLB9YNvM\nrMIcCmu+mX2WRwpmZtV9eTz3/fCOj0LfDjhgCkx9S9EVmZkVrrqhMHk2nP7DoqswMyuV6k4fmZnZ\nIA4FMzPLORTMzCznUDAzs5xDwczMcg4FMzPLORTMzCznUDAzs5wiouga9oikrcBf9/LLpwJ/H8Vy\nWsE1jg7XODrKXmPZ64Py1HhIREzb3U77XCj8LyR1RcTCouvYFdc4Olzj6Ch7jWWvD/aNGht5+sjM\nzHIOBTMzy1UtFK4vuoARcI2jwzWOjrLXWPb6YN+oMVepNQUzM9u1qo0UzMxsFyoTCpIWS9ooqVvS\nJUXXAyBptqT7Ja2X9JSkC9P2gyTdK+mZ9HlKwXW2S/qzpLtSe66kdakvb5U0ruD6Jku6XdLTkjZI\nencJ+/Dz6Wf8pKSbJe1fdD9K+pGkLZKebNg2ZL8p8/1U6+OSFhRY47fSz/pxSXdKmtzw3PJU40ZJ\nHyqqxobnLpYUkqamdiH9uCcqEQqS2oGrgSXAfOAsSfOLrQqAXuDiiJgPLAIuSHVdAqyOiHnA6tQu\n0oXAhob2FcBVEXE48ApwXiFV9fse8NuIeBtwFFmtpelDSTOBzwELI+JIoB1YRvH9+BNgcdO24fpt\nCTAvfZwPXFNgjfcCR0bEO4G/AMsB0rGzDDgifc0P0rFfRI1Img18EHixYXNR/ThilQgF4FigOyKe\ni4idwC3A0oJrIiI2R8Qj6fG/yP6YzSSrbUXabQVwWjEVgqRZwIeBG1JbwInA7WmXout7A3A8cCNA\nROyMiFcpUR8mHcABkjqA8cBmCu7HiPgj8I+mzcP121LgpsisBSZLelMRNUbEPRHRm5prgVkNNd4S\nETsi4nmgm+zYH/Mak6uALwCNC7eF9OOeqEoozAReamj3pG2lIWkOcAywDpgREZvTUy8DMwoqC+C7\nZL/YtdR+I/Bqw0FZdF/OBbYCP05TXDdIOpAS9WFEbAK+TfaKcTOwDXiYcvVj3XD9VtZj6DPAb9Lj\n0tQoaSmwKSIea3qqNDUOpyqhUGqSJgC/AC6KiH82PhfZ28MKeYuYpFOALRHxcBH//gh1AAuAayLi\nGODfNE0VFdmHAGlefilZgL0ZOJAhphvKpuh+2x1Jl5JNwa4supZGksYDXwIuK7qWvVGVUNgEzG5o\nz0rbCidpP7JAWBkRd6TNf6sPKdPnLQWVdxxwqqQXyKbcTiSbv5+cpkGg+L7sAXoiYl1q304WEmXp\nQ4APAM9HxNaIeB24g6xvy9SPdcP1W6mOIUmfAk4Bzo7+99WXpcbDyF4APJaOnVnAI5IOpjw1Dqsq\nofAQMC+922Mc2WLUqoJrqs/P3whsiIgrG55aBZybHp8L/GqsawOIiOURMSsi5pD12e8j4mzgfuCM\nousDiIiXgZckvTVtOglYT0n6MHkRWCRpfPqZ12ssTT82GK7fVgGfTO+eWQRsa5hmGlOSFpNNaZ4a\nEf9peGoVsExSp6S5ZIu5D451fRHxRERMj4g56djpARak39XS9OOwIqISH8DJZO9UeBa4tOh6Uk3v\nJRuePw48mj5OJpu3Xw08A9wHHFSCWk8A7kqPDyU72LqB24DOgms7GuhK/fhLYErZ+hD4GvA08CTw\nU6Cz6H4EbiZb43id7A/XecP1GyCyd/A9CzxB9k6qomrsJpuXrx8z1zbsf2mqcSOwpKgam55/AZha\nZD/uyYfPaDYzs1xVpo/MzGwEHApmZpZzKJiZWc6hYGZmOYeCmZnlHApmY0jSCUpXmzUrI4eCmZnl\nHApmQ5D0CUkPSnpU0nXK7inxmqSr0n0RVkualvY9WtLahuv71+9BcLik+yQ9JukRSYelbz9B/fd/\nWJnOcjYrBYeCWRNJbwc+BhwXEUcDfcDZZBey64qII4A1wFfTl9wEfDGy6/s/0bB9JXB1RBwFvIfs\nrFfIroZ7Edm9PQ4luw6SWSl07H4Xs8o5CXgX8FB6EX8A2YXhasCtaZ+fAXek+zlMjog1afsK4DZJ\nE4GZEXEnQERsB0jf78GI6EntR4E5wAOt/2+Z7Z5DwWwwASsiYvmAjdJXmvbb22vE7Gh43IePQysR\nTx+ZDbYaOEPSdMjvW3wI2fFSv6rpx4EHImIb8Iqk96Xt5wBrIruTXo+k09L36EzX2TcrNb9CMWsS\nEeslfRm4R1Ib2dUvLyC7gc+x6bktZOsOkF1i+tr0R/854NNp+znAdZIuT9/jzDH8b5jtFV8l1WyE\nJL0WEROKrsOslTx9ZGZmOY8UzMws55GCmZnlHApmZpZzKJiZWc6hYGZmOYeCmZnlHApmZpb7LzGC\nOtBlMJmyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.71875,validation accuracy: 0.78125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZEsL3un7o9T",
        "colab_type": "code",
        "outputId": "45883624-8c6b-43d9-daf7-0d672b8c105c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdtUPQYw7o4c",
        "colab_type": "code",
        "outputId": "9c8115cc-89a2-48ca-b2be-cd3a0f3a5fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 14\n",
        "# increased dropout (0.25 -> 0.8)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('04'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 4.2294 - acc: 0.5450\n",
            "Epoch 00001: val_acc improved from -inf to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/04_weights-improvement-01-0.67188-0.54297.hdf5\n",
            "256/256 [==============================] - 15s 59ms/sample - loss: 4.4793 - acc: 0.5430 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 5.2637 - acc: 0.6250\n",
            "Epoch 00002: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 2ms/sample - loss: 5.7046 - acc: 0.6055 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 5.1625 - acc: 0.6200\n",
            "Epoch 00003: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 5.4535 - acc: 0.5938 - val_loss: 5.2888 - val_acc: 0.6719\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.9684 - acc: 0.6200\n",
            "Epoch 00004: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 3.7384 - acc: 0.6172 - val_loss: 0.6288 - val_acc: 0.6719\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 2.0080 - acc: 0.5600\n",
            "Epoch 00005: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 1.7897 - acc: 0.5508 - val_loss: 0.7299 - val_acc: 0.3281\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.1067 - acc: 0.4350\n",
            "Epoch 00006: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 1.0103 - acc: 0.4805 - val_loss: 0.7729 - val_acc: 0.3281\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7324 - acc: 0.4600\n",
            "Epoch 00007: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7282 - acc: 0.4609 - val_loss: 0.7553 - val_acc: 0.3281\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7388 - acc: 0.4700\n",
            "Epoch 00008: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7323 - acc: 0.4844 - val_loss: 0.7270 - val_acc: 0.3281\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7047 - acc: 0.4550\n",
            "Epoch 00009: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7036 - acc: 0.4648 - val_loss: 0.7033 - val_acc: 0.3281\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7800 - acc: 0.5000\n",
            "Epoch 00010: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.7573 - acc: 0.5273 - val_loss: 0.6856 - val_acc: 0.6719\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6782 - acc: 0.5550\n",
            "Epoch 00011: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6853 - acc: 0.5469 - val_loss: 0.6722 - val_acc: 0.6719\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6914 - acc: 0.5400\n",
            "Epoch 00012: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6864 - acc: 0.5703 - val_loss: 0.6630 - val_acc: 0.6719\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6658 - acc: 0.5800\n",
            "Epoch 00013: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6652 - acc: 0.5938 - val_loss: 0.6561 - val_acc: 0.6719\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6842 - acc: 0.5800\n",
            "Epoch 00014: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6842 - acc: 0.5859 - val_loss: 0.6515 - val_acc: 0.6719\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6758 - acc: 0.6350\n",
            "Epoch 00015: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6866 - acc: 0.6172 - val_loss: 0.6486 - val_acc: 0.6719\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6886 - acc: 0.5750\n",
            "Epoch 00016: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6839 - acc: 0.5938 - val_loss: 0.6471 - val_acc: 0.6719\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6696 - acc: 0.6150\n",
            "Epoch 00017: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6785 - acc: 0.5938 - val_loss: 0.6457 - val_acc: 0.6719\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6839 - acc: 0.6000\n",
            "Epoch 00018: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6814 - acc: 0.5977 - val_loss: 0.6453 - val_acc: 0.6719\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6971 - acc: 0.5450\n",
            "Epoch 00019: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6760 - acc: 0.5898 - val_loss: 0.6455 - val_acc: 0.6719\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6695 - acc: 0.6000\n",
            "Epoch 00020: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6683 - acc: 0.6016 - val_loss: 0.6453 - val_acc: 0.6719\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6734 - acc: 0.6000\n",
            "Epoch 00021: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6782 - acc: 0.6016 - val_loss: 0.6454 - val_acc: 0.6719\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6812 - acc: 0.6200\n",
            "Epoch 00022: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6859 - acc: 0.6055 - val_loss: 0.6460 - val_acc: 0.6719\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6837 - acc: 0.5900\n",
            "Epoch 00023: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6799 - acc: 0.5977 - val_loss: 0.6470 - val_acc: 0.6719\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6790 - acc: 0.5950\n",
            "Epoch 00024: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6691 - acc: 0.6055 - val_loss: 0.6477 - val_acc: 0.6719\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6845 - acc: 0.5800\n",
            "Epoch 00025: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6803 - acc: 0.5859 - val_loss: 0.6485 - val_acc: 0.6719\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6599 - acc: 0.6150\n",
            "Epoch 00026: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6660 - acc: 0.5977 - val_loss: 0.6489 - val_acc: 0.6719\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7024 - acc: 0.5700\n",
            "Epoch 00027: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6907 - acc: 0.5938 - val_loss: 0.6497 - val_acc: 0.6719\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6879 - acc: 0.5900\n",
            "Epoch 00028: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6864 - acc: 0.5938 - val_loss: 0.6498 - val_acc: 0.6719\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6689 - acc: 0.6200\n",
            "Epoch 00029: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6750 - acc: 0.5938 - val_loss: 0.6497 - val_acc: 0.6719\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6566 - acc: 0.6350\n",
            "Epoch 00030: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6651 - acc: 0.6055 - val_loss: 0.6495 - val_acc: 0.6719\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6791 - acc: 0.5950\n",
            "Epoch 00031: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.6094 - val_loss: 0.6496 - val_acc: 0.6719\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6863 - acc: 0.5750\n",
            "Epoch 00032: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6833 - acc: 0.5820 - val_loss: 0.6491 - val_acc: 0.6719\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6873 - acc: 0.5800\n",
            "Epoch 00033: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6824 - acc: 0.5898 - val_loss: 0.6491 - val_acc: 0.6719\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6668 - acc: 0.5800\n",
            "Epoch 00034: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6620 - acc: 0.5938 - val_loss: 0.6488 - val_acc: 0.6719\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6766 - acc: 0.5950\n",
            "Epoch 00035: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6772 - acc: 0.5938 - val_loss: 0.6482 - val_acc: 0.6719\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6644 - acc: 0.6050\n",
            "Epoch 00036: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6656 - acc: 0.6016 - val_loss: 0.6475 - val_acc: 0.6719\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6744 - acc: 0.6050\n",
            "Epoch 00037: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6696 - acc: 0.6055 - val_loss: 0.6472 - val_acc: 0.6719\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6924 - acc: 0.5800\n",
            "Epoch 00038: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6764 - acc: 0.6016 - val_loss: 0.6472 - val_acc: 0.6719\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6924 - acc: 0.5700\n",
            "Epoch 00039: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6722 - acc: 0.6055 - val_loss: 0.6467 - val_acc: 0.6719\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6820 - acc: 0.6000\n",
            "Epoch 00040: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6755 - acc: 0.6055 - val_loss: 0.6459 - val_acc: 0.6719\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6810 - acc: 0.6050\n",
            "Epoch 00041: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6778 - acc: 0.6055 - val_loss: 0.6453 - val_acc: 0.6719\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6829 - acc: 0.5900\n",
            "Epoch 00042: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6806 - acc: 0.5938 - val_loss: 0.6453 - val_acc: 0.6719\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6495 - acc: 0.6300\n",
            "Epoch 00043: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6646 - acc: 0.6055 - val_loss: 0.6452 - val_acc: 0.6719\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6534 - acc: 0.6250\n",
            "Epoch 00044: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6645 - acc: 0.6055 - val_loss: 0.6456 - val_acc: 0.6719\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6762 - acc: 0.6050\n",
            "Epoch 00045: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6806 - acc: 0.5977 - val_loss: 0.6469 - val_acc: 0.6719\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6742 - acc: 0.6000\n",
            "Epoch 00046: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6719 - acc: 0.6055 - val_loss: 0.6478 - val_acc: 0.6719\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6845 - acc: 0.5900\n",
            "Epoch 00047: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6758 - acc: 0.6055 - val_loss: 0.6486 - val_acc: 0.6719\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6853 - acc: 0.5950\n",
            "Epoch 00048: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6855 - acc: 0.6016 - val_loss: 0.6487 - val_acc: 0.6719\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6662 - acc: 0.6100\n",
            "Epoch 00049: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6681 - acc: 0.6094 - val_loss: 0.6486 - val_acc: 0.6719\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6786 - acc: 0.5850\n",
            "Epoch 00050: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6744 - acc: 0.6016 - val_loss: 0.6485 - val_acc: 0.6719\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6729 - acc: 0.6050\n",
            "Epoch 00051: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6787 - acc: 0.6055 - val_loss: 0.6482 - val_acc: 0.6719\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6790 - acc: 0.6000\n",
            "Epoch 00052: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6706 - acc: 0.6094 - val_loss: 0.6484 - val_acc: 0.6719\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6614 - acc: 0.6250\n",
            "Epoch 00053: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6642 - acc: 0.6094 - val_loss: 0.6482 - val_acc: 0.6719\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6823 - acc: 0.5900\n",
            "Epoch 00054: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6742 - acc: 0.6016 - val_loss: 0.6484 - val_acc: 0.6719\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6610 - acc: 0.6300\n",
            "Epoch 00055: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6653 - acc: 0.6094 - val_loss: 0.6480 - val_acc: 0.6719\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6640 - acc: 0.6200\n",
            "Epoch 00056: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6721 - acc: 0.5977 - val_loss: 0.6480 - val_acc: 0.6719\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6657 - acc: 0.6250\n",
            "Epoch 00057: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6683 - acc: 0.6016 - val_loss: 0.6478 - val_acc: 0.6719\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6813 - acc: 0.5900\n",
            "Epoch 00058: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6811 - acc: 0.5898 - val_loss: 0.6478 - val_acc: 0.6719\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6781 - acc: 0.6150\n",
            "Epoch 00059: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6798 - acc: 0.6055 - val_loss: 0.6481 - val_acc: 0.6719\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6812 - acc: 0.5750\n",
            "Epoch 00060: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6745 - acc: 0.5938 - val_loss: 0.6480 - val_acc: 0.6719\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6754 - acc: 0.5750\n",
            "Epoch 00061: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6669 - acc: 0.6055 - val_loss: 0.6479 - val_acc: 0.6719\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6745 - acc: 0.6100\n",
            "Epoch 00062: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6771 - acc: 0.6016 - val_loss: 0.6469 - val_acc: 0.6719\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6748 - acc: 0.5950\n",
            "Epoch 00063: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6767 - acc: 0.6016 - val_loss: 0.6467 - val_acc: 0.6719\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6723 - acc: 0.6000\n",
            "Epoch 00064: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.5977 - val_loss: 0.6462 - val_acc: 0.6719\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6918 - acc: 0.5750\n",
            "Epoch 00065: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6760 - acc: 0.6016 - val_loss: 0.6461 - val_acc: 0.6719\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6749 - acc: 0.6000\n",
            "Epoch 00066: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6746 - acc: 0.5977 - val_loss: 0.6454 - val_acc: 0.6719\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6784 - acc: 0.5950\n",
            "Epoch 00067: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6757 - acc: 0.6055 - val_loss: 0.6453 - val_acc: 0.6719\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6744 - acc: 0.5800\n",
            "Epoch 00068: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6724 - acc: 0.5938 - val_loss: 0.6451 - val_acc: 0.6719\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6704 - acc: 0.6150\n",
            "Epoch 00069: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6781 - acc: 0.5977 - val_loss: 0.6442 - val_acc: 0.6719\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6636 - acc: 0.6050\n",
            "Epoch 00070: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6712 - acc: 0.5859 - val_loss: 0.6437 - val_acc: 0.6719\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6767 - acc: 0.5950\n",
            "Epoch 00071: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6743 - acc: 0.5938 - val_loss: 0.6438 - val_acc: 0.6719\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6590 - acc: 0.6450\n",
            "Epoch 00072: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6782 - acc: 0.6016 - val_loss: 0.6437 - val_acc: 0.6719\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6821 - acc: 0.5850\n",
            "Epoch 00073: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6749 - acc: 0.6016 - val_loss: 0.6447 - val_acc: 0.6719\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6721 - acc: 0.6000\n",
            "Epoch 00074: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6704 - acc: 0.5977 - val_loss: 0.6450 - val_acc: 0.6719\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6822 - acc: 0.5700\n",
            "Epoch 00075: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6708 - acc: 0.6016 - val_loss: 0.6452 - val_acc: 0.6719\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6722 - acc: 0.6000\n",
            "Epoch 00076: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6710 - acc: 0.6016 - val_loss: 0.6445 - val_acc: 0.6719\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6693 - acc: 0.6100\n",
            "Epoch 00077: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6770 - acc: 0.6016 - val_loss: 0.6443 - val_acc: 0.6719\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6733 - acc: 0.6050\n",
            "Epoch 00078: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6718 - acc: 0.6094 - val_loss: 0.6442 - val_acc: 0.6719\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6621 - acc: 0.6250\n",
            "Epoch 00079: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6739 - acc: 0.6016 - val_loss: 0.6440 - val_acc: 0.6719\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6794 - acc: 0.5950\n",
            "Epoch 00080: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6727 - acc: 0.6055 - val_loss: 0.6443 - val_acc: 0.6719\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6639 - acc: 0.6100\n",
            "Epoch 00081: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6653 - acc: 0.6055 - val_loss: 0.6444 - val_acc: 0.6719\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6652 - acc: 0.5950\n",
            "Epoch 00082: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6673 - acc: 0.5977 - val_loss: 0.6444 - val_acc: 0.6719\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6816 - acc: 0.5800\n",
            "Epoch 00083: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6732 - acc: 0.6055 - val_loss: 0.6441 - val_acc: 0.6719\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6588 - acc: 0.6100\n",
            "Epoch 00084: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6690 - acc: 0.5898 - val_loss: 0.6430 - val_acc: 0.6719\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6757 - acc: 0.5700\n",
            "Epoch 00085: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6613 - acc: 0.6094 - val_loss: 0.6424 - val_acc: 0.6719\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6757 - acc: 0.5850\n",
            "Epoch 00086: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6730 - acc: 0.6016 - val_loss: 0.6411 - val_acc: 0.6719\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6890 - acc: 0.5800\n",
            "Epoch 00087: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6781 - acc: 0.6016 - val_loss: 0.6401 - val_acc: 0.6719\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6787 - acc: 0.6150\n",
            "Epoch 00088: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6750 - acc: 0.6016 - val_loss: 0.6392 - val_acc: 0.6719\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6485 - acc: 0.6200\n",
            "Epoch 00089: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6618 - acc: 0.6016 - val_loss: 0.6391 - val_acc: 0.6719\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6732 - acc: 0.6100\n",
            "Epoch 00090: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6718 - acc: 0.6055 - val_loss: 0.6393 - val_acc: 0.6719\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6791 - acc: 0.5950\n",
            "Epoch 00091: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6751 - acc: 0.5977 - val_loss: 0.6396 - val_acc: 0.6719\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6613 - acc: 0.6300\n",
            "Epoch 00092: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6721 - acc: 0.6016 - val_loss: 0.6398 - val_acc: 0.6719\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6752 - acc: 0.5950\n",
            "Epoch 00093: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6684 - acc: 0.6094 - val_loss: 0.6404 - val_acc: 0.6719\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6740 - acc: 0.6050\n",
            "Epoch 00094: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6784 - acc: 0.5977 - val_loss: 0.6406 - val_acc: 0.6719\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6798 - acc: 0.5950\n",
            "Epoch 00095: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6681 - acc: 0.6172 - val_loss: 0.6410 - val_acc: 0.6719\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6703 - acc: 0.6050\n",
            "Epoch 00096: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6719 - acc: 0.6016 - val_loss: 0.6408 - val_acc: 0.6719\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6737 - acc: 0.5650\n",
            "Epoch 00097: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6696 - acc: 0.5820 - val_loss: 0.6407 - val_acc: 0.6719\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6714 - acc: 0.6100\n",
            "Epoch 00098: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6678 - acc: 0.6016 - val_loss: 0.6399 - val_acc: 0.6719\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6567 - acc: 0.6250\n",
            "Epoch 00099: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6666 - acc: 0.6016 - val_loss: 0.6395 - val_acc: 0.6719\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6686 - acc: 0.6100\n",
            "Epoch 00100: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6697 - acc: 0.6133 - val_loss: 0.6393 - val_acc: 0.6719\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6656 - acc: 0.6250\n",
            "Epoch 00101: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6741 - acc: 0.6016 - val_loss: 0.6390 - val_acc: 0.6719\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6535 - acc: 0.6100\n",
            "Epoch 00102: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6601 - acc: 0.6016 - val_loss: 0.6387 - val_acc: 0.6719\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6572 - acc: 0.6200\n",
            "Epoch 00103: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6685 - acc: 0.6016 - val_loss: 0.6384 - val_acc: 0.6719\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6695 - acc: 0.5950\n",
            "Epoch 00104: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6709 - acc: 0.5977 - val_loss: 0.6385 - val_acc: 0.6719\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6706 - acc: 0.6000\n",
            "Epoch 00105: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6693 - acc: 0.6016 - val_loss: 0.6383 - val_acc: 0.6719\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6674 - acc: 0.6000\n",
            "Epoch 00106: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6653 - acc: 0.6055 - val_loss: 0.6379 - val_acc: 0.6719\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6562 - acc: 0.6250\n",
            "Epoch 00107: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6676 - acc: 0.6016 - val_loss: 0.6368 - val_acc: 0.6719\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6660 - acc: 0.6300\n",
            "Epoch 00108: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6728 - acc: 0.6055 - val_loss: 0.6364 - val_acc: 0.6719\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6712 - acc: 0.5850\n",
            "Epoch 00109: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6619 - acc: 0.6016 - val_loss: 0.6364 - val_acc: 0.6719\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6699 - acc: 0.5850\n",
            "Epoch 00110: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6657 - acc: 0.6016 - val_loss: 0.6360 - val_acc: 0.6719\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6546 - acc: 0.6100\n",
            "Epoch 00111: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6615 - acc: 0.6016 - val_loss: 0.6352 - val_acc: 0.6719\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6707 - acc: 0.6050\n",
            "Epoch 00112: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6705 - acc: 0.6016 - val_loss: 0.6349 - val_acc: 0.6719\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6889 - acc: 0.5800\n",
            "Epoch 00113: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6787 - acc: 0.5938 - val_loss: 0.6344 - val_acc: 0.6719\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6656 - acc: 0.6250\n",
            "Epoch 00114: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6740 - acc: 0.6055 - val_loss: 0.6337 - val_acc: 0.6719\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6783 - acc: 0.5550\n",
            "Epoch 00115: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6661 - acc: 0.5938 - val_loss: 0.6337 - val_acc: 0.6719\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6693 - acc: 0.5900\n",
            "Epoch 00116: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6603 - acc: 0.6055 - val_loss: 0.6331 - val_acc: 0.6719\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6779 - acc: 0.5750\n",
            "Epoch 00117: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6640 - acc: 0.6094 - val_loss: 0.6328 - val_acc: 0.6719\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6522 - acc: 0.6150\n",
            "Epoch 00118: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6594 - acc: 0.5977 - val_loss: 0.6319 - val_acc: 0.6719\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6457 - acc: 0.6200\n",
            "Epoch 00119: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6613 - acc: 0.5938 - val_loss: 0.6315 - val_acc: 0.6719\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6742 - acc: 0.5800\n",
            "Epoch 00120: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6726 - acc: 0.5898 - val_loss: 0.6311 - val_acc: 0.6719\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6844 - acc: 0.6050\n",
            "Epoch 00121: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6741 - acc: 0.6055 - val_loss: 0.6310 - val_acc: 0.6719\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6441 - acc: 0.6250\n",
            "Epoch 00122: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6588 - acc: 0.5977 - val_loss: 0.6308 - val_acc: 0.6719\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6614 - acc: 0.6000\n",
            "Epoch 00123: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6582 - acc: 0.6094 - val_loss: 0.6313 - val_acc: 0.6719\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6561 - acc: 0.6050\n",
            "Epoch 00124: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6568 - acc: 0.6133 - val_loss: 0.6314 - val_acc: 0.6719\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6463 - acc: 0.6150\n",
            "Epoch 00125: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6485 - acc: 0.6133 - val_loss: 0.6312 - val_acc: 0.6719\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6618 - acc: 0.5850\n",
            "Epoch 00126: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6571 - acc: 0.5938 - val_loss: 0.6309 - val_acc: 0.6719\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6569 - acc: 0.6100\n",
            "Epoch 00127: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6564 - acc: 0.6133 - val_loss: 0.6305 - val_acc: 0.6719\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6493 - acc: 0.5900\n",
            "Epoch 00128: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6534 - acc: 0.5938 - val_loss: 0.6300 - val_acc: 0.6719\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6432 - acc: 0.6250\n",
            "Epoch 00129: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6497 - acc: 0.6133 - val_loss: 0.6291 - val_acc: 0.6719\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6437 - acc: 0.6100\n",
            "Epoch 00130: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6522 - acc: 0.5859 - val_loss: 0.6282 - val_acc: 0.6719\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6382 - acc: 0.6150\n",
            "Epoch 00131: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6443 - acc: 0.6133 - val_loss: 0.6273 - val_acc: 0.6719\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6564 - acc: 0.5850\n",
            "Epoch 00132: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6612 - acc: 0.5859 - val_loss: 0.6265 - val_acc: 0.6719\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6260 - acc: 0.6650\n",
            "Epoch 00133: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6366 - acc: 0.6328 - val_loss: 0.6256 - val_acc: 0.6719\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6627 - acc: 0.6100\n",
            "Epoch 00134: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6633 - acc: 0.5977 - val_loss: 0.6254 - val_acc: 0.6719\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6589 - acc: 0.6250\n",
            "Epoch 00135: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6603 - acc: 0.6172 - val_loss: 0.6251 - val_acc: 0.6719\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6412 - acc: 0.6400\n",
            "Epoch 00136: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6488 - acc: 0.6289 - val_loss: 0.6247 - val_acc: 0.6719\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6340 - acc: 0.6450\n",
            "Epoch 00137: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6462 - acc: 0.6172 - val_loss: 0.6242 - val_acc: 0.6719\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6615 - acc: 0.5800\n",
            "Epoch 00138: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6599 - acc: 0.5781 - val_loss: 0.6240 - val_acc: 0.6719\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6621 - acc: 0.6050\n",
            "Epoch 00139: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6530 - acc: 0.6172 - val_loss: 0.6232 - val_acc: 0.6719\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6332 - acc: 0.6250\n",
            "Epoch 00140: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6444 - acc: 0.5977 - val_loss: 0.6214 - val_acc: 0.6719\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6360 - acc: 0.6500\n",
            "Epoch 00141: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6412 - acc: 0.6484 - val_loss: 0.6201 - val_acc: 0.6719\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6445 - acc: 0.6100\n",
            "Epoch 00142: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6423 - acc: 0.6055 - val_loss: 0.6188 - val_acc: 0.6719\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6403 - acc: 0.6000\n",
            "Epoch 00143: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6302 - acc: 0.6133 - val_loss: 0.6175 - val_acc: 0.6719\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6534 - acc: 0.5850\n",
            "Epoch 00144: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6415 - acc: 0.6055 - val_loss: 0.6165 - val_acc: 0.6719\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6437 - acc: 0.6150\n",
            "Epoch 00145: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6378 - acc: 0.6250 - val_loss: 0.6160 - val_acc: 0.6719\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6318 - acc: 0.6250\n",
            "Epoch 00146: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6260 - acc: 0.6289 - val_loss: 0.6159 - val_acc: 0.6719\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6533 - acc: 0.6200\n",
            "Epoch 00147: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6639 - acc: 0.6016 - val_loss: 0.6156 - val_acc: 0.6719\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6174 - acc: 0.6350\n",
            "Epoch 00148: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6167 - acc: 0.6289 - val_loss: 0.6148 - val_acc: 0.6719\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6176 - acc: 0.6250\n",
            "Epoch 00149: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6114 - acc: 0.6367 - val_loss: 0.6140 - val_acc: 0.6719\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6359 - acc: 0.6400\n",
            "Epoch 00150: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 0s 2ms/sample - loss: 0.6291 - acc: 0.6289 - val_loss: 0.6133 - val_acc: 0.6719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.62890625,validation accuracy: 0.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sug0jfkK1HT",
        "colab_type": "code",
        "outputId": "a31d7c46-1ed0-4a7e-e2a6-3d104bf378f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_oliv_test_scld)\n",
        "accuracy_score(olivetti_labels_test, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyjTzaWNmCNs",
        "colab_type": "code",
        "outputId": "4b532ee5-e4a2-454d-f454-e30ee874beae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 14\n",
        "# added more conv2d layers\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 1)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.7))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_oliv_train_scld,\n",
        "    olivetti_labels_train_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=150,\n",
        "    callbacks=check('14'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 256 samples, validate on 64 samples\n",
            "Epoch 1/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 3.3255 - acc: 0.6050\n",
            "Epoch 00001: val_acc improved from -inf to 0.56250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-01-0.56250-0.58984.hdf5\n",
            "256/256 [==============================] - 7s 29ms/sample - loss: 3.4448 - acc: 0.5898 - val_loss: 0.7051 - val_acc: 0.5625\n",
            "Epoch 2/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 2.5867 - acc: 0.5850\n",
            "Epoch 00002: val_acc did not improve from 0.56250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 2.5493 - acc: 0.5938 - val_loss: 0.6854 - val_acc: 0.5625\n",
            "Epoch 3/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.6579 - acc: 0.5700\n",
            "Epoch 00003: val_acc did not improve from 0.56250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 1.5877 - acc: 0.5742 - val_loss: 0.6853 - val_acc: 0.5625\n",
            "Epoch 4/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7039 - acc: 0.5650\n",
            "Epoch 00004: val_acc did not improve from 0.56250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.7041 - acc: 0.5977 - val_loss: 0.6930 - val_acc: 0.4844\n",
            "Epoch 5/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7140 - acc: 0.5250\n",
            "Epoch 00005: val_acc did not improve from 0.56250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.7065 - acc: 0.5312 - val_loss: 0.6965 - val_acc: 0.4375\n",
            "Epoch 6/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6990 - acc: 0.5750\n",
            "Epoch 00006: val_acc did not improve from 0.56250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6985 - acc: 0.5781 - val_loss: 0.6953 - val_acc: 0.4375\n",
            "Epoch 7/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6775 - acc: 0.6000\n",
            "Epoch 00007: val_acc improved from 0.56250 to 0.64062, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-07-0.64062-0.57812.hdf5\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.6818 - acc: 0.5781 - val_loss: 0.6918 - val_acc: 0.6406\n",
            "Epoch 8/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7031 - acc: 0.5600\n",
            "Epoch 00008: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6885 - acc: 0.5781 - val_loss: 0.6891 - val_acc: 0.5625\n",
            "Epoch 9/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6764 - acc: 0.5950\n",
            "Epoch 00009: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6745 - acc: 0.5898 - val_loss: 0.6873 - val_acc: 0.5625\n",
            "Epoch 10/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6431 - acc: 0.6300\n",
            "Epoch 00010: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6524 - acc: 0.6172 - val_loss: 0.6868 - val_acc: 0.5625\n",
            "Epoch 11/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6549 - acc: 0.6350\n",
            "Epoch 00011: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6654 - acc: 0.6172 - val_loss: 0.6872 - val_acc: 0.5625\n",
            "Epoch 12/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6801 - acc: 0.6450\n",
            "Epoch 00012: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6854 - acc: 0.6328 - val_loss: 0.6879 - val_acc: 0.5625\n",
            "Epoch 13/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6710 - acc: 0.5950\n",
            "Epoch 00013: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6575 - acc: 0.6250 - val_loss: 0.6891 - val_acc: 0.5625\n",
            "Epoch 14/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6671 - acc: 0.6350\n",
            "Epoch 00014: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6771 - acc: 0.6328 - val_loss: 0.6905 - val_acc: 0.5625\n",
            "Epoch 15/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6714 - acc: 0.6300\n",
            "Epoch 00015: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6592 - acc: 0.6445 - val_loss: 0.6915 - val_acc: 0.5625\n",
            "Epoch 16/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6685 - acc: 0.6450\n",
            "Epoch 00016: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6691 - acc: 0.6367 - val_loss: 0.6927 - val_acc: 0.5625\n",
            "Epoch 17/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6522 - acc: 0.6500\n",
            "Epoch 00017: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6484 - acc: 0.6484 - val_loss: 0.6935 - val_acc: 0.5625\n",
            "Epoch 18/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6671 - acc: 0.5950\n",
            "Epoch 00018: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6566 - acc: 0.6250 - val_loss: 0.6942 - val_acc: 0.5625\n",
            "Epoch 19/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6468 - acc: 0.6450\n",
            "Epoch 00019: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6416 - acc: 0.6484 - val_loss: 0.6955 - val_acc: 0.5625\n",
            "Epoch 20/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6563 - acc: 0.6400\n",
            "Epoch 00020: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6560 - acc: 0.6484 - val_loss: 0.6967 - val_acc: 0.5625\n",
            "Epoch 21/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6500 - acc: 0.6450\n",
            "Epoch 00021: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6462 - acc: 0.6484 - val_loss: 0.6977 - val_acc: 0.5625\n",
            "Epoch 22/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6610 - acc: 0.6400\n",
            "Epoch 00022: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6669 - acc: 0.6328 - val_loss: 0.6988 - val_acc: 0.5625\n",
            "Epoch 23/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6699 - acc: 0.6250\n",
            "Epoch 00023: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6648 - acc: 0.6289 - val_loss: 0.6996 - val_acc: 0.5625\n",
            "Epoch 24/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6466 - acc: 0.6400\n",
            "Epoch 00024: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6488 - acc: 0.6328 - val_loss: 0.7003 - val_acc: 0.5625\n",
            "Epoch 25/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6384 - acc: 0.6350\n",
            "Epoch 00025: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6408 - acc: 0.6328 - val_loss: 0.7011 - val_acc: 0.5625\n",
            "Epoch 26/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6385 - acc: 0.6600\n",
            "Epoch 00026: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6512 - acc: 0.6445 - val_loss: 0.7024 - val_acc: 0.5625\n",
            "Epoch 27/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6626 - acc: 0.6250\n",
            "Epoch 00027: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6559 - acc: 0.6406 - val_loss: 0.7030 - val_acc: 0.5625\n",
            "Epoch 28/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6400 - acc: 0.6400\n",
            "Epoch 00028: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6313 - acc: 0.6484 - val_loss: 0.7041 - val_acc: 0.5625\n",
            "Epoch 29/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6477 - acc: 0.6350\n",
            "Epoch 00029: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6491 - acc: 0.6328 - val_loss: 0.7064 - val_acc: 0.5625\n",
            "Epoch 30/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6648 - acc: 0.6200\n",
            "Epoch 00030: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6646 - acc: 0.6211 - val_loss: 0.7089 - val_acc: 0.5625\n",
            "Epoch 31/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6586 - acc: 0.6300\n",
            "Epoch 00031: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6447 - acc: 0.6406 - val_loss: 0.7103 - val_acc: 0.5625\n",
            "Epoch 32/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6347 - acc: 0.6650\n",
            "Epoch 00032: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6436 - acc: 0.6367 - val_loss: 0.7134 - val_acc: 0.5625\n",
            "Epoch 33/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6121 - acc: 0.6700\n",
            "Epoch 00033: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6319 - acc: 0.6367 - val_loss: 0.7160 - val_acc: 0.5625\n",
            "Epoch 34/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6481 - acc: 0.6200\n",
            "Epoch 00034: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6391 - acc: 0.6406 - val_loss: 0.7176 - val_acc: 0.5625\n",
            "Epoch 35/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6616 - acc: 0.6400\n",
            "Epoch 00035: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6457 - acc: 0.6445 - val_loss: 0.7202 - val_acc: 0.5625\n",
            "Epoch 36/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6538 - acc: 0.6250\n",
            "Epoch 00036: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6415 - acc: 0.6523 - val_loss: 0.7244 - val_acc: 0.5625\n",
            "Epoch 37/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6024 - acc: 0.7000\n",
            "Epoch 00037: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6275 - acc: 0.6562 - val_loss: 0.7315 - val_acc: 0.5625\n",
            "Epoch 38/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6246 - acc: 0.6650\n",
            "Epoch 00038: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6222 - acc: 0.6719 - val_loss: 0.7384 - val_acc: 0.5625\n",
            "Epoch 39/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6388 - acc: 0.6250\n",
            "Epoch 00039: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6392 - acc: 0.6328 - val_loss: 0.7466 - val_acc: 0.5625\n",
            "Epoch 40/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6403 - acc: 0.6300\n",
            "Epoch 00040: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6257 - acc: 0.6523 - val_loss: 0.7548 - val_acc: 0.5625\n",
            "Epoch 41/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6325 - acc: 0.6250\n",
            "Epoch 00041: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6255 - acc: 0.6289 - val_loss: 0.7603 - val_acc: 0.5625\n",
            "Epoch 42/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6066 - acc: 0.6850\n",
            "Epoch 00042: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5956 - acc: 0.6914 - val_loss: 0.7715 - val_acc: 0.5625\n",
            "Epoch 43/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5801 - acc: 0.6950\n",
            "Epoch 00043: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5976 - acc: 0.6797 - val_loss: 0.7893 - val_acc: 0.5625\n",
            "Epoch 44/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6084 - acc: 0.6750\n",
            "Epoch 00044: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5980 - acc: 0.6719 - val_loss: 0.8083 - val_acc: 0.5625\n",
            "Epoch 45/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5851 - acc: 0.7100\n",
            "Epoch 00045: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5822 - acc: 0.7070 - val_loss: 0.8131 - val_acc: 0.5625\n",
            "Epoch 46/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5683 - acc: 0.7550\n",
            "Epoch 00046: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5805 - acc: 0.7227 - val_loss: 0.8161 - val_acc: 0.5625\n",
            "Epoch 47/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5481 - acc: 0.7300\n",
            "Epoch 00047: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5545 - acc: 0.7383 - val_loss: 0.8266 - val_acc: 0.5625\n",
            "Epoch 48/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5970 - acc: 0.6650\n",
            "Epoch 00048: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5640 - acc: 0.6953 - val_loss: 0.8247 - val_acc: 0.5625\n",
            "Epoch 49/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5516 - acc: 0.7650\n",
            "Epoch 00049: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5646 - acc: 0.7383 - val_loss: 0.8149 - val_acc: 0.5625\n",
            "Epoch 50/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5235 - acc: 0.7950\n",
            "Epoch 00050: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5262 - acc: 0.7891 - val_loss: 0.8327 - val_acc: 0.5625\n",
            "Epoch 51/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5396 - acc: 0.7500\n",
            "Epoch 00051: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5417 - acc: 0.7617 - val_loss: 0.8867 - val_acc: 0.5625\n",
            "Epoch 52/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5365 - acc: 0.7550\n",
            "Epoch 00052: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5021 - acc: 0.7852 - val_loss: 0.9844 - val_acc: 0.5625\n",
            "Epoch 53/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4701 - acc: 0.7900\n",
            "Epoch 00053: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5101 - acc: 0.7773 - val_loss: 1.1129 - val_acc: 0.5625\n",
            "Epoch 54/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5401 - acc: 0.7950\n",
            "Epoch 00054: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5501 - acc: 0.7812 - val_loss: 1.0637 - val_acc: 0.5625\n",
            "Epoch 55/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5002 - acc: 0.7900\n",
            "Epoch 00055: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4871 - acc: 0.8008 - val_loss: 1.0374 - val_acc: 0.5625\n",
            "Epoch 56/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4743 - acc: 0.7850\n",
            "Epoch 00056: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4635 - acc: 0.7969 - val_loss: 1.1086 - val_acc: 0.5625\n",
            "Epoch 57/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4783 - acc: 0.8100\n",
            "Epoch 00057: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4569 - acc: 0.8203 - val_loss: 1.3044 - val_acc: 0.5625\n",
            "Epoch 58/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5902 - acc: 0.8050\n",
            "Epoch 00058: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5631 - acc: 0.7852 - val_loss: 1.7638 - val_acc: 0.5625\n",
            "Epoch 59/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4073 - acc: 0.7950\n",
            "Epoch 00059: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4832 - acc: 0.7930 - val_loss: 6.5740 - val_acc: 0.5625\n",
            "Epoch 60/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4966 - acc: 0.8150\n",
            "Epoch 00060: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4669 - acc: 0.8203 - val_loss: 6.2435 - val_acc: 0.5625\n",
            "Epoch 61/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4867 - acc: 0.7950\n",
            "Epoch 00061: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5697 - acc: 0.7891 - val_loss: 5.7752 - val_acc: 0.5625\n",
            "Epoch 62/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7049 - acc: 0.8150\n",
            "Epoch 00062: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6377 - acc: 0.8242 - val_loss: 3.5565 - val_acc: 0.5625\n",
            "Epoch 63/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5031 - acc: 0.8050\n",
            "Epoch 00063: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4936 - acc: 0.7930 - val_loss: 3.4938 - val_acc: 0.5625\n",
            "Epoch 64/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4957 - acc: 0.8150\n",
            "Epoch 00064: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4681 - acc: 0.8203 - val_loss: 2.7009 - val_acc: 0.5625\n",
            "Epoch 65/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4376 - acc: 0.8100\n",
            "Epoch 00065: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4430 - acc: 0.7969 - val_loss: 1.9343 - val_acc: 0.5625\n",
            "Epoch 66/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3882 - acc: 0.8550\n",
            "Epoch 00066: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3850 - acc: 0.8477 - val_loss: 1.1280 - val_acc: 0.5625\n",
            "Epoch 67/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5406 - acc: 0.8450\n",
            "Epoch 00067: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5004 - acc: 0.8359 - val_loss: 0.7607 - val_acc: 0.5625\n",
            "Epoch 68/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3894 - acc: 0.8250\n",
            "Epoch 00068: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3729 - acc: 0.8398 - val_loss: 0.6955 - val_acc: 0.5312\n",
            "Epoch 69/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3896 - acc: 0.8500\n",
            "Epoch 00069: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3954 - acc: 0.8438 - val_loss: 0.7116 - val_acc: 0.5312\n",
            "Epoch 70/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3631 - acc: 0.8700\n",
            "Epoch 00070: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3766 - acc: 0.8633 - val_loss: 0.7522 - val_acc: 0.4844\n",
            "Epoch 71/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3859 - acc: 0.8550\n",
            "Epoch 00071: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4054 - acc: 0.8438 - val_loss: 0.7492 - val_acc: 0.4844\n",
            "Epoch 72/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3645 - acc: 0.8500\n",
            "Epoch 00072: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3622 - acc: 0.8516 - val_loss: 0.7203 - val_acc: 0.4688\n",
            "Epoch 73/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3916 - acc: 0.8650\n",
            "Epoch 00073: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3738 - acc: 0.8711 - val_loss: 0.6940 - val_acc: 0.5938\n",
            "Epoch 74/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2677 - acc: 0.9100\n",
            "Epoch 00074: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.2936 - acc: 0.8828 - val_loss: 0.8023 - val_acc: 0.5625\n",
            "Epoch 75/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4187 - acc: 0.8850\n",
            "Epoch 00075: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3872 - acc: 0.8828 - val_loss: 0.7274 - val_acc: 0.5625\n",
            "Epoch 76/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4588 - acc: 0.8850\n",
            "Epoch 00076: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4211 - acc: 0.8789 - val_loss: 0.9832 - val_acc: 0.4375\n",
            "Epoch 77/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4391 - acc: 0.8800\n",
            "Epoch 00077: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4017 - acc: 0.8828 - val_loss: 1.0795 - val_acc: 0.4375\n",
            "Epoch 78/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2474 - acc: 0.9250\n",
            "Epoch 00078: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.2650 - acc: 0.9023 - val_loss: 1.1130 - val_acc: 0.4375\n",
            "Epoch 79/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3556 - acc: 0.8900\n",
            "Epoch 00079: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4008 - acc: 0.8945 - val_loss: 1.0774 - val_acc: 0.4062\n",
            "Epoch 80/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3165 - acc: 0.9050\n",
            "Epoch 00080: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.2839 - acc: 0.9141 - val_loss: 1.0395 - val_acc: 0.3906\n",
            "Epoch 81/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.2031 - acc: 0.9250\n",
            "Epoch 00081: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3590 - acc: 0.9102 - val_loss: 0.9648 - val_acc: 0.3906\n",
            "Epoch 82/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4265 - acc: 0.9200\n",
            "Epoch 00082: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4320 - acc: 0.9180 - val_loss: 0.8905 - val_acc: 0.3750\n",
            "Epoch 83/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6302 - acc: 0.8800\n",
            "Epoch 00083: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.7117 - acc: 0.8633 - val_loss: 1.4585 - val_acc: 0.4062\n",
            "Epoch 84/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.2476 - acc: 0.7850\n",
            "Epoch 00084: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 1.0649 - acc: 0.7812 - val_loss: 2.6347 - val_acc: 0.6250\n",
            "Epoch 85/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6180 - acc: 0.7650\n",
            "Epoch 00085: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6426 - acc: 0.7852 - val_loss: 2.9987 - val_acc: 0.5781\n",
            "Epoch 86/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5668 - acc: 0.7550\n",
            "Epoch 00086: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6222 - acc: 0.7422 - val_loss: 1.3864 - val_acc: 0.6094\n",
            "Epoch 87/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6817 - acc: 0.6800\n",
            "Epoch 00087: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6542 - acc: 0.6992 - val_loss: 0.9024 - val_acc: 0.5938\n",
            "Epoch 88/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6265 - acc: 0.6300\n",
            "Epoch 00088: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6186 - acc: 0.6484 - val_loss: 0.8942 - val_acc: 0.5625\n",
            "Epoch 89/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7538 - acc: 0.6500\n",
            "Epoch 00089: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.7439 - acc: 0.6250 - val_loss: 0.8453 - val_acc: 0.5625\n",
            "Epoch 90/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5831 - acc: 0.7100\n",
            "Epoch 00090: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6040 - acc: 0.6875 - val_loss: 0.7291 - val_acc: 0.5781\n",
            "Epoch 91/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5930 - acc: 0.6950\n",
            "Epoch 00091: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5893 - acc: 0.7188 - val_loss: 0.6824 - val_acc: 0.5625\n",
            "Epoch 92/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6315 - acc: 0.6650\n",
            "Epoch 00092: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6234 - acc: 0.6836 - val_loss: 0.6574 - val_acc: 0.5625\n",
            "Epoch 93/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5981 - acc: 0.6400\n",
            "Epoch 00093: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5934 - acc: 0.6406 - val_loss: 0.6424 - val_acc: 0.6094\n",
            "Epoch 94/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5573 - acc: 0.7050\n",
            "Epoch 00094: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5451 - acc: 0.7109 - val_loss: 0.6365 - val_acc: 0.6094\n",
            "Epoch 95/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5387 - acc: 0.6850\n",
            "Epoch 00095: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5388 - acc: 0.6719 - val_loss: 0.6435 - val_acc: 0.6250\n",
            "Epoch 96/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5269 - acc: 0.7500\n",
            "Epoch 00096: val_acc did not improve from 0.64062\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5139 - acc: 0.7578 - val_loss: 0.6764 - val_acc: 0.6250\n",
            "Epoch 97/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5718 - acc: 0.7850\n",
            "Epoch 00097: val_acc improved from 0.64062 to 0.65625, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-97-0.65625-0.77344.hdf5\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5599 - acc: 0.7734 - val_loss: 0.7422 - val_acc: 0.6562\n",
            "Epoch 98/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5500 - acc: 0.7750\n",
            "Epoch 00098: val_acc did not improve from 0.65625\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5389 - acc: 0.7695 - val_loss: 0.9354 - val_acc: 0.6250\n",
            "Epoch 99/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4358 - acc: 0.8150\n",
            "Epoch 00099: val_acc did not improve from 0.65625\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5121 - acc: 0.8047 - val_loss: 2.2239 - val_acc: 0.6250\n",
            "Epoch 100/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4738 - acc: 0.8050\n",
            "Epoch 00100: val_acc did not improve from 0.65625\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4666 - acc: 0.8164 - val_loss: 2.5845 - val_acc: 0.6250\n",
            "Epoch 101/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6446 - acc: 0.8250\n",
            "Epoch 00101: val_acc improved from 0.65625 to 0.67188, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-101-0.67188-0.82812.hdf5\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.5951 - acc: 0.8281 - val_loss: 2.6534 - val_acc: 0.6719\n",
            "Epoch 102/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6082 - acc: 0.8450\n",
            "Epoch 00102: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5480 - acc: 0.8555 - val_loss: 2.9600 - val_acc: 0.6562\n",
            "Epoch 103/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4207 - acc: 0.8700\n",
            "Epoch 00103: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4120 - acc: 0.8594 - val_loss: 5.3118 - val_acc: 0.5938\n",
            "Epoch 104/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4313 - acc: 0.8450\n",
            "Epoch 00104: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3997 - acc: 0.8477 - val_loss: 6.4772 - val_acc: 0.5625\n",
            "Epoch 105/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7194 - acc: 0.8600\n",
            "Epoch 00105: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6555 - acc: 0.8359 - val_loss: 4.3176 - val_acc: 0.6562\n",
            "Epoch 106/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6209 - acc: 0.8250\n",
            "Epoch 00106: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5859 - acc: 0.8125 - val_loss: 3.4129 - val_acc: 0.6562\n",
            "Epoch 107/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4606 - acc: 0.8250\n",
            "Epoch 00107: val_acc did not improve from 0.67188\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4240 - acc: 0.8359 - val_loss: 3.9788 - val_acc: 0.6406\n",
            "Epoch 108/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4022 - acc: 0.8550\n",
            "Epoch 00108: val_acc improved from 0.67188 to 0.76562, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-108-0.76562-0.85547.hdf5\n",
            "256/256 [==============================] - 1s 4ms/sample - loss: 0.3847 - acc: 0.8555 - val_loss: 1.0234 - val_acc: 0.7656\n",
            "Epoch 109/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4237 - acc: 0.8100\n",
            "Epoch 00109: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5087 - acc: 0.7500 - val_loss: 0.6046 - val_acc: 0.6719\n",
            "Epoch 110/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7103 - acc: 0.6300\n",
            "Epoch 00110: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6999 - acc: 0.6094 - val_loss: 0.5834 - val_acc: 0.6875\n",
            "Epoch 111/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6451 - acc: 0.6150\n",
            "Epoch 00111: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6424 - acc: 0.6250 - val_loss: 0.5539 - val_acc: 0.7188\n",
            "Epoch 112/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5780 - acc: 0.6750\n",
            "Epoch 00112: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6103 - acc: 0.6523 - val_loss: 0.5603 - val_acc: 0.7188\n",
            "Epoch 113/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6077 - acc: 0.6750\n",
            "Epoch 00113: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6106 - acc: 0.6836 - val_loss: 0.5808 - val_acc: 0.7500\n",
            "Epoch 114/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5753 - acc: 0.7250\n",
            "Epoch 00114: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5854 - acc: 0.7188 - val_loss: 0.5954 - val_acc: 0.6875\n",
            "Epoch 115/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6019 - acc: 0.6950\n",
            "Epoch 00115: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5955 - acc: 0.7031 - val_loss: 0.6057 - val_acc: 0.6562\n",
            "Epoch 116/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5733 - acc: 0.6750\n",
            "Epoch 00116: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5747 - acc: 0.6836 - val_loss: 0.6123 - val_acc: 0.6562\n",
            "Epoch 117/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5716 - acc: 0.7200\n",
            "Epoch 00117: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5668 - acc: 0.7188 - val_loss: 0.6128 - val_acc: 0.6562\n",
            "Epoch 118/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5149 - acc: 0.7850\n",
            "Epoch 00118: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5227 - acc: 0.7773 - val_loss: 0.6062 - val_acc: 0.6562\n",
            "Epoch 119/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5203 - acc: 0.7700\n",
            "Epoch 00119: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5156 - acc: 0.7734 - val_loss: 0.5940 - val_acc: 0.6719\n",
            "Epoch 120/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4895 - acc: 0.8050\n",
            "Epoch 00120: val_acc did not improve from 0.76562\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4958 - acc: 0.8008 - val_loss: 0.5793 - val_acc: 0.7344\n",
            "Epoch 121/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4951 - acc: 0.7900\n",
            "Epoch 00121: val_acc improved from 0.76562 to 0.78125, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-121-0.78125-0.78125.hdf5\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4966 - acc: 0.7812 - val_loss: 0.5599 - val_acc: 0.7812\n",
            "Epoch 122/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4783 - acc: 0.7750\n",
            "Epoch 00122: val_acc improved from 0.78125 to 0.79688, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-122-0.79688-0.80078.hdf5\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4617 - acc: 0.8008 - val_loss: 0.5410 - val_acc: 0.7969\n",
            "Epoch 123/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4227 - acc: 0.8600\n",
            "Epoch 00123: val_acc improved from 0.79688 to 0.81250, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/14_weights-improvement-123-0.81250-0.85547.hdf5\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4312 - acc: 0.8555 - val_loss: 0.5269 - val_acc: 0.8125\n",
            "Epoch 124/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5537 - acc: 0.7850\n",
            "Epoch 00124: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4972 - acc: 0.8203 - val_loss: 0.5241 - val_acc: 0.8125\n",
            "Epoch 125/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4716 - acc: 0.8500\n",
            "Epoch 00125: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4335 - acc: 0.8633 - val_loss: 0.5391 - val_acc: 0.8125\n",
            "Epoch 126/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4330 - acc: 0.8750\n",
            "Epoch 00126: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4062 - acc: 0.8711 - val_loss: 0.5916 - val_acc: 0.7969\n",
            "Epoch 127/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5211 - acc: 0.8350\n",
            "Epoch 00127: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4767 - acc: 0.8438 - val_loss: 0.9716 - val_acc: 0.7969\n",
            "Epoch 128/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3625 - acc: 0.8400\n",
            "Epoch 00128: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3979 - acc: 0.8594 - val_loss: 2.3823 - val_acc: 0.7344\n",
            "Epoch 129/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3859 - acc: 0.8750\n",
            "Epoch 00129: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4784 - acc: 0.8711 - val_loss: 2.4683 - val_acc: 0.7188\n",
            "Epoch 130/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5803 - acc: 0.8700\n",
            "Epoch 00130: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5258 - acc: 0.8711 - val_loss: 2.0463 - val_acc: 0.7656\n",
            "Epoch 131/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3968 - acc: 0.8750\n",
            "Epoch 00131: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4413 - acc: 0.8672 - val_loss: 1.8828 - val_acc: 0.7500\n",
            "Epoch 132/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4557 - acc: 0.8850\n",
            "Epoch 00132: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.4772 - acc: 0.8828 - val_loss: 2.7647 - val_acc: 0.6562\n",
            "Epoch 133/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.3446 - acc: 0.9150\n",
            "Epoch 00133: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.3771 - acc: 0.9141 - val_loss: 3.6157 - val_acc: 0.5938\n",
            "Epoch 134/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.5682 - acc: 0.9100\n",
            "Epoch 00134: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5795 - acc: 0.8945 - val_loss: 3.7992 - val_acc: 0.5625\n",
            "Epoch 135/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.4677 - acc: 0.8950\n",
            "Epoch 00135: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.5777 - acc: 0.8711 - val_loss: 1.4787 - val_acc: 0.6094\n",
            "Epoch 136/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 1.4328 - acc: 0.6050\n",
            "Epoch 00136: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 1.2945 - acc: 0.6133 - val_loss: 0.8340 - val_acc: 0.5938\n",
            "Epoch 137/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.7635 - acc: 0.6300\n",
            "Epoch 00137: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.7628 - acc: 0.6211 - val_loss: 0.7282 - val_acc: 0.5625\n",
            "Epoch 138/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6824 - acc: 0.6050\n",
            "Epoch 00138: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6821 - acc: 0.6172 - val_loss: 0.7248 - val_acc: 0.5625\n",
            "Epoch 139/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6555 - acc: 0.6350\n",
            "Epoch 00139: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6593 - acc: 0.6211 - val_loss: 0.7352 - val_acc: 0.5625\n",
            "Epoch 140/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6590 - acc: 0.6100\n",
            "Epoch 00140: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6509 - acc: 0.6133 - val_loss: 0.7419 - val_acc: 0.5625\n",
            "Epoch 141/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6413 - acc: 0.6500\n",
            "Epoch 00141: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6506 - acc: 0.6406 - val_loss: 0.7429 - val_acc: 0.5625\n",
            "Epoch 142/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6619 - acc: 0.6400\n",
            "Epoch 00142: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6474 - acc: 0.6523 - val_loss: 0.7389 - val_acc: 0.5625\n",
            "Epoch 143/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6628 - acc: 0.5950\n",
            "Epoch 00143: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6600 - acc: 0.6055 - val_loss: 0.7328 - val_acc: 0.5625\n",
            "Epoch 144/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6495 - acc: 0.6200\n",
            "Epoch 00144: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6466 - acc: 0.6289 - val_loss: 0.7284 - val_acc: 0.5625\n",
            "Epoch 145/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6513 - acc: 0.6400\n",
            "Epoch 00145: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6495 - acc: 0.6367 - val_loss: 0.7263 - val_acc: 0.5625\n",
            "Epoch 146/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6484 - acc: 0.6600\n",
            "Epoch 00146: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6396 - acc: 0.6719 - val_loss: 0.7256 - val_acc: 0.5625\n",
            "Epoch 147/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6365 - acc: 0.6250\n",
            "Epoch 00147: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6281 - acc: 0.6406 - val_loss: 0.7259 - val_acc: 0.5625\n",
            "Epoch 148/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6407 - acc: 0.6300\n",
            "Epoch 00148: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6417 - acc: 0.6250 - val_loss: 0.7266 - val_acc: 0.5625\n",
            "Epoch 149/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6557 - acc: 0.6150\n",
            "Epoch 00149: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6421 - acc: 0.6367 - val_loss: 0.7273 - val_acc: 0.5625\n",
            "Epoch 150/150\n",
            "200/256 [======================>.......] - ETA: 0s - loss: 0.6347 - acc: 0.6400\n",
            "Epoch 00150: val_acc did not improve from 0.81250\n",
            "256/256 [==============================] - 1s 3ms/sample - loss: 0.6372 - acc: 0.6367 - val_loss: 0.7288 - val_acc: 0.5625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4Y1edv9+jZlmyLfcyLmOPp/ee\nXgZCSAIhBUiBZQmwZIGwtG3s8oMFdtnCLiy7EJYeysImAUISkpBCMjPpyXRPn/FU927JRV3n98e5\nV5JtyZY9ku2x7/s8eixdXV0d29L93G8XUkoMDAwMDAwATDO9AAMDAwOD2YMhCgYGBgYGUQxRMDAw\nMDCIYoiCgYGBgUEUQxQMDAwMDKIYomBgYGBgEMUQBQMDAwODKIYoGBgYGBhEMUTBwMDAwCCKZaYX\nMFmKi4tlbW3tTC/DwMDA4KJiz5493VLKkon2u+hEoba2lt27d8/0MgwMDAwuKoQQ51LZz3AfGRgY\nGBhEMUTBwMDAwCCKIQoGBgYGBlEMUTAwMDAwiGKIgoGBgYFBFEMUDAwMDAyiGKJgYGBgYBDFEAUD\ngxnksf0ttPR7Z3oZBgZRDFEwMJhGIpHYTPTf7Wvm0w/u539fT6mmyGAGkFKO+J/NBwxRMDCYJvqG\nAmz6p+f4xC/3cKCpny8+ehiAdrdvhldmkIw//cmbfOHRgzO9jGnFEAUDg2li54ku+oaDPH2onVvu\nfwUhYGGRgw6PIQqzkVA4whune3mioY1QODLTy5k2MioKQogbhBDHhRCNQojPJ3h+oRDieSFEgxBi\nhxCiKpPrMTCYSbYf76TIaePxT17JVUuK+c871rOyIs8QhVnKme4hAuEIA74Qe8/3z/Rypo2MiYIQ\nwgzcD9wIrATuFkKsHLXbfwA/l1KuBb4K/Eum1mNgMJOEI5IXT3RxzdISVle6+MVHLuG6lWWU5dnp\n9PhnenkGCTjaPhC9v+N45wyuZHrJpKWwFWiUUp6WUgaAB4FbRu2zEnhBu789wfMGBhct7uEgH/zJ\nmxxsdnOguZ++4SDXLi8dsU9Znp0Bf4ghf2iGVmmQjGNtHiwmwYaafLYf75rp5UwbmRSFSqAp7nGz\nti2eA8Dt2v3bgFwhRFEG12RgMG388WgHO090cd+v9vJkQxsmAVcvKR6xT1leFoDhQpqFHGsfYHFp\nDtevLOdom2feJATMdKD5r4BrhBD7gGuAFiA8eichxL1CiN1CiN1dXfNHsQ0ubnac6CIny0Jz3zA/\nfvkMG2oKyHfYRuxTlmcHoMNwIc06jrV5WF6ey7XL1FyanSfmhwspk6LQAlTHPa7StkWRUrZKKW+X\nUm4AvqBtGxPRkVL+QEq5WUq5uaRkwsFBBgYzTigc4cUTXbx9VTl/8ZYlAFy7dOxnVxeFzoH5cRV6\nseAeDtLq9rG8Io/l5bmU59nZMU9cSJmcvLYLWCKEqEOJwV3A++J3EEIUA71Sygjwd8BPMrgeA4Np\n40BzP25vkG3LS7hhVTlFOTZuXrtgzH66+2i+uCYuFo61ewBYXp6LEILLFxex83gXUkqEEDO8usyS\nMUtBShkCPgk8AxwFHpZSHhZCfFUI8S5tt2uB40KIE0AZ8LVMrcfAYDrZfqwLk4CrFpdgMZv408tq\nKXDaxuyXk2XBYTMb7qNZxjEt82hFRR4AW2sL6RkKcKpraCaXNS1kdEazlPIp4KlR274Ud/83wG8y\nuQYDg5lgx4lONi0swOWwjrufEIKyPDsdhvtoVnGs3UO+w0pprrLkttYVAvDmmV4Wl+bM5NIyzkwH\nmg0MLlrO9Qxx789386EH3uS+X+2NuhwOtbg51OLh2mWlExxBUZaXRaeRfTSrONI2EHUdAdQVOynO\nsbHrbO8MryzzZNRSMDCYy3zzuRPsONHF8vJc9jX189zhDt65roInDrRRnJPFu9aNjSEkoizPzt7z\nfRlerUGqdA74ONjczyeuXRzdJoRgS20hb56Z+6JgWAoGBlPgXM8Qvz/Qyj2X1/L4J6/kj5+7hquX\nlvDI3ha2LS/h2c9eTXWhI6VjleXZ6fD4kXJ+deOcrfz+QBsRCbesHynqW+sKaen3zvlW54alYGAw\nBb638xQWs4k/u7IOgOKcLH74p5s43ztMTaFjUhkqpblZBEIR3N7gmDoGg+nnsf0trK7MY0lZ7ojt\nW2pVXGHXmV4qN4yuw507GJaCgcEkaXf7+M2eZu7YXEWpVmcAysWwsMg56ZTFcpdRwJZJpJQ8dbCN\nwQStRJ493M6PXjrNj18+w+muQRo7B2lodnPr+rEn/RUVeeRmWXhzjscVDEvBwGCSPLKvmWBY8udX\n16fleHoBW7vHx7Ly3An2NpgsP331LF/5/RG+9M6VfFiz7EBlGN37iz3Rx19/2sTaKhcmQcJ4kNkk\nWLkgjxNxjfLmIoYoGBhMkh3Huli1IC/lmMFElOXqloKRgZRujrZ5+JenjgHQ0DyyWcJ3t5/CaTPz\n7OeuIRKRfOmxQ2w/3sVVS4pHWIDxZNvM9A0FMr7umcQQBQODJDR2DvDaqR4+cFltdJvbG2TP+T4+\nfk16rASAUq2q2UhLTS+BUIRP/d8+XA4rdUVOGprd0efOdg/xREMrH71qEZX52QD85J4tPHukg+Xj\nWGtWs4lAeG4nBBgxBQODJHz7hUa++NhhmvuGo9tePtlNOCKjTdLSgd1qJtduoXtwdl+B9g0FLqoM\nqcOtbk52DvL3Ny3n6qXFnO4ewuMLArFEgY/EuZOEELx9VTkLi5xJj2kzmwjO8SlshigYGCRAH4oD\njGiEtuN4J65sK+ur89P6fvkOK25vMK3HTBf+UJh/eeooG//pOX7+2rmZXk7KnO9VYr5qgYs1Ver/\ndajZTe9QgN/uHZsokAoWs5jzozkNUTAwSIA+FAdiU7ciEcmOE8rnbDGn96vjyrbSPzz7LAVfMMzt\n332V7794mkKHje/uaMQfGtPdfgT/9ceTvHCsY5pWGMM9HORzD+2P+vyb+1Q9QVVBNmsrXQA0tLh5\nsqGVYFjy/ksWTvo9rGYTQcN9ZGAw/9hxXDW0u3ndAl5p7MEfCnOkzUPXgJ9tKbavmAz52bZZaSns\nPtvH4VYP/3r7Gv777g10ePz8Zk9z0v17hwJ86/kTPL6/dRpXqXjzbC+P7GvhpcZuAM73DFOck4XD\nZqHAaaOm0MHBZje/29fC8vLcaLO7yWA1C8N9ZGAwH9lxvJMNNQXcun4B3mCYN0738j87T2ExCa5O\nMBfhQnFlW+mfhaJwQMvYuXF1BZfXF7GuOp/v7TyV1IXy4okupCRhTUCm0bO3GjsHAWjqG6a6MDv6\n/JoqFy+d7GLv+X5unWLxmdWIKRgYzF2+v/MUf/ObA2O2dw34aWh2c+3SEi6rL8JmMfGV3x/myYY2\nPnf9Ukq0zpnpxOWw4pmFonCw2c3CIgcuhxUhBJ/ctpimXi+/b0hsCeiutpkQBT1765QmCnp1uc66\nKhceXwiRpA4hFSwmEyHDfWRgMDf549EOHt7dzL5RzeieP6r84duWl+KwWbh0URGnuoa4bFFR2grW\nRqNiCsFZl93T0NzP2qpYUP2ty0tZVpbLd7efIhIZudZwRLJTC87PhCi0x1kKoXCENreP6oKYKKyp\nVL/HpXVFLMjPTniMibBaBAHDUjAwmJu0adPO7t9+ClAntfu3N/LFxw5RX+JkpeZzvm3DAirzs/nm\nneswmzIzdSs/20ooIhkOjB/EnU66Bvy0un2sq3JFt5lMgk9sq+dk5yDPHhkZTG7QgvPZVjODvplw\nH6k2IWe6h2ju8xKOyBHuo7VVLqoKsvng5bVTfg+rae67j4ziNYN5SSQi6fD4yLVb+OPRDp493M4P\nXjzN7nN9vGNNBV+7bTUmTQBu21DFbRuqMroeV7YaxtPvDeLMmh1fy4MtKp6wptI1Yvs71lTwzedO\n8N0djbx9VVm019N2LTh/7bISdp2d/lbgekwhEI7w6qkegBFV584sCy//7Vsu6D2sZhMRqS4gMnWB\nMNMYloLBvKR7yE8wLLn3qkU4bWbu/cUejncM8K071/Od922Y9m6l+dqEttmUltrQ7EYIWD1KFCxm\nEx+/pp6GZjcvneyObt95vJP11flUFzoY9E9/fKRzwB+17rZrsY1491E6sFqUEMxla8EQBYN5Sbvm\nOlpWnsvf3LCcG1aV8/RnrubWDZUzMpg9T7MUZlNaakOzm8UlOQktl9s2VpJrt/CHQ22AWndDi5ur\nlpTgtFnwBSPTWuTlD4XpHQpwxeIiAF5p7MZiElS4JlecNhFWkzplhiKzK/aTTmaHnWpgMM3o8YQK\nVzbXryq/ID9zOsjPVpaJe3h2iIKUkoZmN9ckSb/NspjZvLAgOols77k+pIRL6go5qnURHfKHcTmm\n57qzU4snLCnNpSQ3i64BPzWFjrQXGVrNmqUQikD6k9BmBYalYDAv0S2Fivz0XklOFZdjdlkKzX1e\nugf9rKt2Jd1nS10hp7qG6Bn08+bZXiwmwYaaAnKyzAAMTKMLqXNA/T9L87JYXJIDMCIdNV3oImO4\njwwMLkJOdgwkbUfd6vZiM5sonCWTzvLjAs2zgf994xwmAVcvSV6ot1WfRHa2j11nelld6SLbZiYn\nS/0uQ/7py6TSM4/KXXbqS1VDu/jMo3Rh00VhDruPDFEwmLN8+Ge7+MrvDyd8rt3to8yVFc0wmmkc\nNjMWk5gVloJ7OMj/vnaOm9ZUUFucvGPomioXWRYTL53s4kBzP5fUKZHIsSuv9HQGm3XLryzXHrUU\n0jXvIh5LvPtojmKIgsFFhTcQ5uZvvxztYBrPn/7kTR7adR5QxVNNvV4Ot3qiz3/58cN87ckjgIop\nVOSl/0pyqgghyHeoAraZ5qevnmUoEOa+bYvH3S/LYmZ9dT6/3asm0ekzjKPuo2msVegY8GEzm8h3\nWKOzlTPhPrKa9UCzIQoGBrOCfef7ONjijlbO6nR4fLx4ootnD6uCqtNdqtXBuZ5hhvwhpJQ8ur+F\n3+xpJhKRtLm9syaeoOPKnvlWF0P+EA+8eoa3Li9NqWHc1rpCfEF1gtxcWwAwI+6jTo+f0rwshBBc\nUlfIl29eyXUrytL+ProoBEJz131kZB8ZXFToQ9P1pmc6+lStY1rmS/zzxzsGKHZmRa/CG7sG6XD7\nKU9zuuKFoprizWydwq/eOE//cJD73jK+laCjWwfLynKjtR0z4T7q8PjUrOvhXixBL/estsFwO5ht\nkJO+BoZ69tFcthQmFAWhkrbfDyySUn5VCFEDlEsp38z46gwMRrErqSio6tuWfi9ub3DE88faBsi1\ne6OPnz7UTiAcoWKSA1YyTb7DFs2imQl8wTA/eOk0l9cXsbGmIKXXbFxYgNUsuHRRYXRbjk0Xhemz\nFNo9Pm5yNcHXbxz75F2/guXvSMv7WOdB9lEqlsJ3gQjwFuCrwADwW2BLBtdlYDCGYDjC3nP9WM2C\nln4vw4EQDu0EpFffSgnH2wdo7BxkUYmTTo+fo20esm1mbBYTeXYLjx9QHT4rptgULVO4sq2c6BiY\nsff/zZ5mugb8fOvO9Sm/JifLwsN/fhm1cSMsnVpMYTr7H3V6/Lwl6xmwOuGGfwa0BILt/wx7f5E2\nUdADzXPZfZRKTOESKeV9gA9AStkHpJTHJ4S4QQhxXAjRKIT4fILna4QQ24UQ+4QQDUKImya1eoN5\nxaEWN95gmLevKgfgdNcQoBda9XOVlj55rN1DY9cgS0pzWF6ey7F2Dwea+llZkccldUVRKyLd1a4X\niit75kZyBsMRvrfzFOur87m8vmhSr91QU0CBM3ZKsJhN2K2maXMfDfpDBP3DrOrfDituhk33wKYP\nqtva90LjczDUk5b3shmBZgCCQggzIAGEECUoy2FctNfcD9wIrATuFkKsHLXb/wMellJuAO5CWSUG\nBgnRq2fft7UGgFNaMLm5z0vfcJC3rSwj32HlYLOb8z3DLC7NYXlFLsfaBjjc6mFtlYsttTG3yGyM\nKQz4QoRnIAf+tVM9NPd5+dg1i9LS5iMnyzpt7qNOj49tpv1khQZh7R0jn1x7J0RCcPiRtLyXUbym\n+G/gd0CpEOJrwMvAP6fwuq1Ao5TytJQyADwI3DJqHwnoKQ4uYPpn+BnMajy+IPc88CYP725i19le\n6oqdbK4txGwS0St+Pci8rsrF8vJc/ni0g1BEKlEoz2PAH2LQH2JtVT5btFx6q1lQ7JxdfQr0pngz\nkYGku6221k3OSkhGrt2SsZkKR9s8fPAnb0aP3+72cZv5ZQL2Eqi7ZuTOZauhdCUc/HVa3jva5mIO\nD9qZMKYgpfylEGIP8FaUo+5WKeXRFI5dCTTFPW4GLhm1z5eBZ4UQfwE4gesSHUgIcS9wL0BNTU0K\nb20wV3jhaCc7jnex47hKQb1zczU2i4mFhY44UejHZjaxrDyX5eV5vH5aWRT1JTkjrujWVrmoL8kh\n127BlW2dNYVrOvHts+PdMdNBY+cghU4bhWl6X2eWmaEMicKTDW3sPNHFiye6uGlNBQ0nz/Jh0z7C\nq/4MzKNOaUIo6+GPX4aXvwX2PFh1O2TnJzz2RNjmu6UghDALIY5JKY9JKe+XUn4nRUFIlbuBn0op\nq4CbgF8IIcasSUr5AynlZinl5pKS9M/HNZi97DjeSZHTxhduWoHdauL6VSr3vL40Z4SlsLwilyyL\nmRUVudHX1pfksFQrZHLYzNSX5GA2Cd62ooy1Vcl7+swU+TPY/6ixczBaCZwOcrIsGQs0N7Qoy1Af\n/Rk58ig2ESZ70/sSv2DNHWB1wB//AZ74LBx4cMrvrbuP5vJIznEtBSllWAsU10gpz0/y2C1Addzj\nKm1bPB8BbtDe6zUhhB0oBjon+V4GcxB9vOO2ZaV89OpFfOTKuujV/eLSHHYc76RzwEdDcz+3bVSD\n2JeXK2/kApc92vK5ptBBeZ49OhTlG3esm5H22BMRtRSmeaaClJLGrkFuXF2RtmPmZFlo6U9/eq2e\nVACw43gXPYN+Nrufo8dZR1HFusQvclXCX5+C4DB8cwUMtE35/XX30VweyZlKTKEAOCyEeF4I8bh+\nS+F1u4AlQog6IYQNFUge/brzKLcUQogVgB0Y27/AYM7RPxzgRy+dHjPnNx59vOM1y5R1GO/uWVyS\nQzAs+fBPdxEMS95/yUIAlpblIoSyJHS+ccc6vvyuVdHHs1EQAFx6++xpthR6hgL0DwdZXJpeSyET\n7qPmPi/9w0E21OTTOeDnoedeZqvpGP6V71GuomTYHOAsBmcJDE39FGPUKSi+OJUDSylDQohPAs8A\nZuAnUsrDQoivArullI8Dfwn8UAjxWVTQ+R452yaXG2SEJxra+Kcnj7JxYUG0UMo9HIy2kIbYeMdE\nnTr1k/6hFg9feufKaEuGbJuZd61bwNa6WDGVXnU723HFDdrxh8KYhIiehDKJ7oZLqyhkKNB8QLMS\nPvWWJXzop7vw7n0IzFB+xZ+kdgBnCQxO3RFhne/uIwAp5c6pHlxK+RTw1KhtX4q7fwS4YqrHN7h4\naXOrCuODzW421qhhLXf+4DXuf99Gblqj3Bg7j3eOyYHXWVyag8UkuHJJMR+6onbEc/9114aMrz8T\n6KLwZEMb//70ce7aWs0X3jE6izv9ZEIUnBmKKRxsdmMzm7hicTGrF+RyS/dLnMpeS31hbWoHyCmF\nwY4pv3+0S+octhQmvAwRQlwqhNglhBgUQgSEEGEhhGei1xkYjIc++Uy/8tt5ohMp4fO/baC138vB\nZjcHmt1cm2TyV06Whd98/HLuf9/GWesOmiw2iwmHzcwbZ3oZ8Ic4oKXaZppTXYM4bOa0tv3IzbIQ\nCEfwh9Jbq3CguZ8VC/KwWUzcWdXPYlMrA8tuT/0AzlIYnLr7KJZ9NI8tBeA7qHjAr4HNwJ8CSzO5\nKIO5j97//qB24tt1po+FRQ66B/y8/0dv0Nw3TGluFrduqEx6jPXVU0srnM38+dX1OGxmDre6eUMr\n1ss0ekuQdKbo5mhB/iF/mCyL+YKO5Q+FOdM9xNLSXA61eLhN+0y8o1gFjJdcfuskFqbFFKQcPwaR\nBIvJsBQAkFI2AmYpZVhK+QBaxpCBwVTRRaGxa5DeoQD7m/q5fmUZ/3jras50D3H9ynKe+czVGRmU\nMpv59HVL+OjVi1hY5KTd40v7lXYiTqU5HRWIZn6lI9j8qzfOc8O3XuLDP9ulFSGqdOLCYAcIM86i\n6gmOEL+wUogEwds3pbWYTQIh5rYopGIpDGvZQ/uFEF8H2jDmMBhcAFJKWt1e6kucnOoa4ldvnCMQ\njrCltpDrV5Vz5ZJiSnKy5oxbaCrUFDqQElr7fdSNM/1sqhxsdvPFxw7x0asW0er2pTWeAKqiGdIz\naOdczzBmk+Clk90ArK3SLMT+JsirHFuwNh45pernUBc4Jp+AILTg/1x2H6Vycv+Att8ngSFU7cG7\nM7kog7mN2xvEF4xEG9v97LVzQCxLqDTXPq8FAWKjJM/3Dmfk+A+8cob9Tf3c96u9QHqDzBCzFNKR\ngdQ14GdhkYPffvxy/t87VrC0TFuruxnyJ2ElgEpLhQvLQDKJ+W0pSCnPaXd9wFcyuxyD+YAeZF5d\n6aIyP5uWfi9Ly3KmvbXDbEYfJdmUAVEYDoR4+nA7t2+spDgni0f3tbC+OrX5CamSk0b3UdeAn5Kc\nLNZX54+MI7mbYOHlkzuYU7cUEohCinEGq8VEaD6LghDiClSPooXx+0spF2VuWQZzGT0dtdxlZ22V\ni5Z+74i6AgMozc3CZjFlRBSeO9LBcCDMnZuruWRREX9/04r0vsHPb6W69BJgLQNpEIXOAR9rqkYl\nFYRD4GkF1yQtBd19NDoD6X+uhDXvgSs/M+EhLCYTgXnuPvox8E3gStRgHf1mYDAldEuhwmVnjRY0\nvFgKzKYLk0lQlZ9NU1/6ReF3+1qozM/O3N+8bT/OnsNAegbt6JbCCAbaQIYn7z7KLgRhHmkp+Aeg\n4yDs+hGkMCfBZhZz2lJIRRTcUso/SCk7pZQ9+i3jKzOYs7S7fZhNgtJcO9evLGdLbUHCquX5TnWh\nI+0xha4BPy+d7OaW9Qsy1yU2MIQ1pAriLtR9NOQPMRQIU5o3ShTcWgNmV9XkDmgyqbhCfEzB3Rw7\n5vnXJjyExWyanzEFIcRG7e52IcS/A48Afv15KeXeDK/NYI7S2u+jNDcLs0mwuDSHX39skn7heUJ1\nYTb7m/rTeswdxzsJRyQ3r1uQ1uNGCQchHMAcUPMZLtR91DWgTjljLAX9RO6aQit9Z+nI/kf9cR3+\nGx6C2vGbLFjNYk5nH40XU/jGqMeb4+5L1MxmA4NJ0+7xzrqpZ7ORmkIHbm8QtzcYbYFxoXQNqpPs\nwqIM1X8E1IhU4XPjtF34TIVOTRTGWAr9WtNmV/LixqTkjOp/5NaOVXc1HH4Ubvw6WJN/Pq3z1VKQ\nUm6bzoUYzB/a3D5WlOdNvOM8p7ogloHkqkzP/AePN4TVLMi2XliVcVKCmrvL71FN8caLKXja1P5F\n9Ul3iVoKuQncR44isE2hhsNZCt0n447VDCYrXP4p+OV74OSzsPJdSV8+10Uhld5HnxZC5AnFj4QQ\ne4UQ10/H4gzmHlJK2t0+w1JIAb1WoTmNwWbd6shYHYhmKeDzqKZ441kKT/0VPPzBcQ/XOaCSEkpz\nR31e3M2TzzzS0S0FvSFzfxPkLYBF2yDLBadeGPflVrMgNANztKeLVALNH5ZSeoDrgSJUMdu/ZnRV\nBnMWjzfEcCBMhSEKE5KJAjaPN0hemlxRCdFFIeQl1xLBFxynTUfr/ljAOAldA34sJkH+6DX3N00+\nyKzjLIWwH/xaX093E+TXqMronFLwjR/HsZhNBELz2FJAzWUGNS7z51LKw3HbDAwmRZtH1ShUuLJn\neCWzH1e2lTy7haZeb9qO6fEFybNPgygAxVYf3mSi4O0DT7M6AYeST5rrHPBTkps1MlNKSq2aeYrz\n2kfXKsRbHfY88I3fndZmNs17S2GPEOJZlCg8I4TIBeauTBqkFSkl4bgvkF6jYLiPUqOmKL1pqekM\nWickGFtrvsnHcCCJKHQcjt0fZxJalyYKI/D2QXBo6u4jp5b+PNSpsqUG2mL1Dll54Bt/MoDFPLfb\nXKQiCh8BPg9skVIOAzbgQxldlcGc4aevnuWyf3k+WuzT1h8rXDOYmOoCR1oL2DIuCoHB6N1Csze5\n+2iEKCTvQ9Q54Kd0tChEM4+m6D6KWgqdqipaRmLHsrtibqUkWOe7+0hKGZFS7pVS9muPe6SUDZlf\nmsHFjpSSX7x+js4BP+0eJQZNfcNYzYKyNA50mcvUFDpo7vOOO8t6MmReFOItBW9y91HHodj9cYbe\nJLQU9BqFyVYz6zjjRCFaBGe4j3SMFtgGGeNgi5vTXcrHrLtAmnqHqczPxpypato5RlWhg0AoEs3X\nvxCklFqgeRKtpidLXEwhT3jxjuc+0gvPklgKoXCEniE/JaMzjwbatDeYQo0CqIpmRzGceyVWuKbH\nJwz3kSEKBpnjd/taovebtWBpU+/wvBuccyFEu6WmwYU06A8RkWQ4phAvCsOJLYVIGDqPQv212sIS\ni0LvUAApE9QoDHaCMKk6halgMsPqd8PxP0Cn5sbSBcaeDyHvuMFvq9lEaA5XNKdSp1AvhMjS7l8r\nhPiUEGLuzUE0SCuhcITfH2jluhVlmE0iZin0eQ1RmATVBSpL63zPhYuC2xsEMiwKcZZCLsOJLYW+\nsyogXbUVrM6kgeZoNfNoURjqVIJguoACvLV3qLTUvT9X7iS9gtmuFVWOE1ewmgWBeW4p/BYICyEW\nAz9ADdn5VUZXZXDR83JjN92DAd67uYoKl52mvmEG/SF6hwLRSl2DiaksyEaI9FgKHq8qJMt4TMGi\nhMwphwhF5FhXix5PKFs1tuVEHEmrmQe7YnGBqVK5CQoXqfhBfGwiSxOFceIKylKY36IQkVKGgNuA\nb0sp/xqoyOyyDC52Ht3XQp7dwrXLSqgpdNDUOxydDVBdaNQopEqWxUx5nj0taam6pZDZOoVBdbVt\ny8EpldUwxoXUcVi5f0qWa83pEotCrJo5gaWQc4FddYWAtXeq+/FZTHatncg4loLFZIzjDAoh7gY+\nCDyhbcvgp8rgYmfIH+KZwx29ugg3AAAgAElEQVS8Y+0CsixmqgscnO/1RkWhxnAfTYrqAkc0JpOM\nH754mj/72e5x94mKQqbrFKwOsLvIjihR8OkupMc/BV8tgp1fh8J6sDkgp5TejhY+9MCbYw7V6VGW\nQvHoDqmDnRduKQCsea/6GV8EZ0/BUrDMbfdRKmkIHwI+BnxNSnlGCFEH/CKzyzK4mHn2SDveYJjb\nNqjgXU2Rg+5BPyc6VDtlw300OaoLHbx6qnvcfZ470sHuc734gmHsSZrdeaYlpjAMthyIhLCHR1kK\nJ56GstWw+K2qIymAswSL90W2H+/iaJuHFRWxRolnuocoz7OP/X2GumK1BhdCUT285wGoipsZFnUf\nJbcUbHPcfZTKjOYjwKfiHp8B/i2TizK4uHl0XyuV+dlsXqjm/lZpwdLXTveQm2Uh32EYmpOhujCb\n9n0+/KEwWZaxJ3wpJUfbPUQknOsZZll5bvS5SEQSCEewW814fJooZPLvHxhUFgCQ5VOFbMOBsIoD\nDHbAFZ+Gy+6L7Z9TSo4cwEyYR/e3jBCFU12DLC7NGXl8/6CyRpxpGsq0+vaRj1N0H0UkhCNyTqZW\np5J9dIUQ4jkhxAkhxGkhxBkhxOnpWJzBxYea7NXFrRtik710d9Gus31UFToy16FzjlJT6EBKaOlL\n7EJqdfsY0FpUN3YOjnjuod1NXPlvLxAIRXB7gwgBObYM1ikEh1U7a7uLrJCyDL3BcCz1s2zViN2H\nrIWYkBQLD4/ta40W6UkpOdU1NFYU9PhDOiyFRKToPgLmbK3CVGY0bybFGc1CiBuEEMeFEI1CiM8n\neP4/hRD7tdsJIUR6x0wZTDu/P9BKRMKt62OFRXoKaiAUiaZYGqTORN1Sj7XFrmpHi8KJjgG6BwM0\n9w3j9qpmeBkbwwkqJdXqgKy86EhOXyAca2tRtnrE7q0hZdXcucJOu8fH62fUpN92j49Bf4j60aKg\nVz+nI6aQiBTcR1aTOm3OZ1GY0oxmIYQZuB+4EVgJ3C2EWBm/j5Tys1LK9VLK9cC3USM/DS5i9pzr\no6bQwZKymAujyGnDYVNuDyPIPHliBWyJLYVj7eqKvMhpo7FrpCj0DakirLM9Q3gy3eIClCjYcsCe\nhyUYZyl0HIacMlVNHMdZrxqSc9fKLHKyLDy2rxWIiVt9yaghOlFLIUMzvU1msOVOWKcAzNkMpFRE\nYbsQ4t+FEJcJITbqtxRetxVolFKellIGgAeBW8bZ/27g/1I4rsEsprFzkCWjru6EENHgslG4NnlK\ncrKwWUzR7K3RHG3zUF2YzZoq1xhLoUcThTPdw5nvewSaKKjsI3PAA0hNFA6NcR0BHB9UlmO5eYC3\nryrnqUNthMKR6O8xxn2k1zRkylKACfsfWczqtDlXg82piMIlKJfRP6PmNn8D+I8UXlcJxE/QaNa2\njUEIsRCoA8YfeWQwqwmFI5zpTuAHJiYGhqUweUwmQVVBdlJRONY+wLKyPBaX5HC6a3BEq/K+Yc1S\n6B5S7qNM9j2CWEwhKw8RCWEngNfnh85jCUWhwW0DwDTcxdtWljLgC7H3fD+NnYPk2S2UjE5H1auf\nR1kcacXuGlcUbJoozNW01FSyj6ZjVvNdwG+klAm7Zwkh7gXuBaipmeJgDYOM09znJRCOjPUDEytY\nMwrXpkZlfjat/WPdR75gmNNdg9y4upzK/Gz8oQit/bFWIr2DMfeR2xvM7ByLSESrU3BGA7a5DGNz\nn1EtJUbFEwAOd0UICBu2oS6u2FSMxSTYfryTxk6VeTQmKWGwE7ILwZxBiycrb/zsI819NFf7H6WS\nfeQSQnxTCLFbu31DCJHKFPEWVEsMnSptWyLuYhzXkZTyB1LKzVLKzSUlGfIlGlwwSU1+YEttIQtc\ndqqMGoUpscCVHR1QFE9j5yARCcvL86JiHO9C6h2Oiyn4QtMzYMfmVI3lUE3xnH3H1PbRmUf+EC1u\nHz5bEQx2kWu3srm2gO3HOhNnHoFWzZxB1xFM6D6ymo1A80+AAeAO7eYBHkjhdbuAJUKIOiGEDXXi\nf3z0TkKI5UAB8FqqizaYeb76+yN86IE3kTJ2taQHOetLxn6Zb1pTwat/99akhVUG41PustM16B9z\nItKDzMsrcllcMlIUvIEwvmCEbKuZlj4v/cOB6RnFaXNEs3jyGCbPcwJMFiheOmJ3va16xFESDSBf\nu6yUY+0DdA/6E4vCYFf6ahSSYXeNn30UFYV5aikA9VLKf9ACxqellF8BFk30Iq1f0ieBZ4CjwMNS\nysNCiK8KId4Vt+tdwIMy/uxiMOvZe76P7ce7eONMb3RbY+cgJblZmQ9mzkMqXHakhA7PSGvhWJuH\nLIuJ2iInBU6bykDSRKFnSLWJWFvlIiLVSSyzLS50UciJuo8KLT4KBk8oQbCMjA+c0i4iLHll0VTT\nbctiVsCMWQoTuI9i2Ufz11LwCiGu1B8IIa4AUpokLqV8Skq5VEpZL6X8mrbtS1LKx+P2+bKUckwN\ng8HspntQnXDu394Y3dbYORi9WjVIL3osoH2UC+lszzB1xc5oZW19aU7UYusbUhXMG7XKcpimttla\n7yOASoubas8+1ZV0FI2dg5hNguyShdB/DqRkaVlOdFTr4pLcMa9JS4fUidDdR0muUw33EXwcuF8I\ncVYIcQ74DqoXksE8RUpJ96CfPLuFl052c6CpX1WgdiZoS2CQFhbkqwD96LhC9+DIcZWLS3OiV+B6\nPGFTzXSJQlxMQXMf3czL2CPDsOY9Y3Y/2TnAwkIH5vJV6src3YQQgutWlJGbZaFydKFj0AuBgczV\nKOjYXRAJqfdLgGW+1ylIKfdLKdcBa4E1UsoNUsoDmV+awWxlSPNV33N5LXl2C//1/Ek6B/wM+EOG\nKGSIZJZC96B/RBfRhYUO+oeDeHxBejX30aISJ3l2lWiYUfdRQAtw22LZR1tkA/3mYqi9aszux9sH\nVJ8mPStJq3r+mxuW8bv7rhjbV2g6ahQgVtWcxIVkm6+WghDiT7SfnxNCfA74M+DP4h4bzFO6teEn\nC4uc3LdtMS8c6+TfnlYZJoYoZIbcLAtOm5lWd+zqVbfYinNs0W16KmpT7zC9mvuoyJlFXbGqDJ62\n7CNbjpqZALyes23MlLQhf4hzvcMsL8+D0hVqozZ8J9duTRJP0GoUMp59pCVXJslAihavReaZKAB6\nfXlugpvxzZ/H6PGE4tws/uyqRVxSV8gje1W2sSEKmUEIQbnLPsJS0C22eEsh2hKj10vvkB+zSZBr\nt1A7HaIQH1MQInrFvTNrbKnTiY4BpFRZU2TlQkFtrD9SMqKWwjS4jyBpBpIeaA6E5pn7SEr5fe3u\nH6WUX4m/Ac9Pz/IMZopP/d8+fvH6uYTPRUUhx4bZJPjPO9fjyraSm2UZOyXLIG1UjKpV0C22eFHQ\n24nolkKBw4bJJKgtmqQoDPXA96+B9kOpLzCakqpdT9pdNFlrOSYXjtlVT6VdUa65aspWTywKA6ov\n0rRkHwH4E1sKtgxYCk29w9z87Zdpc6eUw5NRUgk0fzvFbQZzhBMdAzx+oJVnD7cnfL5Lq5LVWxAs\nyM/mxx/czD/eutpoi51BKlz2ESeNeItNx+Wwkme30NQ3TN9QgEKnEoG7tlbzj7esotBpIyXa9qvb\nrh+mvsDRovD2r/HLks/iDY49eR5r8+C0maOzNihbBT2NSYO7AJx4BvIqIXdB6muaChO0z7ZkIKbw\n2qkeDra4ee3UhL1GM07SNhdCiMuAy4GSUTGEPMCoQJrDPLpPuYLOdA8lfF6/Qo0/wWyuLWRzbcaX\nNq+pcNnpHFAFbFazaYTFFk91oYPzvcMM+8PR/1GFK5sPXFab+pu5tbZlh38HN359TI1BQvSYglWr\nWl9xM20H9uFzj+2If1QLMkfbeJetAhmBrmOwYMPYYw92QePzcPlfgCmVa9kLIEX3UTCN7iM9jVi3\noGaS8f66NlTswMLIeIIHGJtfZjAniEQkj+1XZnprvxd/aGw7qu5BPwUOa/SKyWB6KHdlI6UaZARj\nLTadmkKHch8NB1K3DEbjblY/fW44+WxqrwkMgiV7RFA522qOjePUkFJyrM3D8rgpa6MzkMZw+BGQ\nYVh7Z6q/wdSZIPsoWqeQRveRXnB4tC150dx0kdRSkFLuBHYKIX4qpUzsXDaYc+w+10dLv5drlpaw\n80QXTb3eMcHj0WmQBtODXtTV5vayID+b7gE/QjDmxF9d6OD5Y504bWYKFxVO7c36m5SbJhKEhodg\nxc0TvyYwHB3FqWO3mvEGRopCm9uHxxdiRdzYUApqlaAkE4WGh6FsDZStTPx8OrE5QZiTuo+iohBK\nnyicukgsBZ0fCSHy9QdCiAIhxDMZXJPBDPK7fS04bGb+/GrVyeRsAhdS92DAEIUZoCJfFwUVbFYW\nm22MxVZdkE0gFKFvOEihI4mlIGXMGkiEu0mdqFe/R/nyvX1j9+k7B+ffgKY3IejTZimMHIqTbRtr\nKRxrV1fDIywFk1mlpja9oY4Zfzv2JLTshrV3JF9vOhFCq2pOfNUe7ZIaSY/7yBcM09Q7TL7DSteA\nP+oWnClSaa5eLKWMOgWllH1CiAyH/w2mm3BE8oMXT/ObPU28a11ldID62Z5EouBnbVX+mO0GmaUi\nTwVl2+NEYXQ8AUYOMipI5j46+jj8+kPwqX1QMDY7CHcTVF+qKpHf+B849QKsfnfs+XAQvntpLI5w\n6X2q95F1lChYzQTDMhoHATjapq6Gl5WPamOxYAPs/jH85Pqx6zFZElZFZwy7C3yJpwOne57Cme4h\nIhJuWFXOg7uaON4+QPHimbvoSkUUIkKIGinleYgOxJmbCbrzlEAowod++iavNPZw05pyvvTOleRl\nW8izW6KicP/2RvKyrXzg0oV0DyQ+GRlklrxsC9lWc5ylkNhiixeFpDGFpjeVj741gShEwuBphfxq\nKFqstnlaR+7j8yhB2PJRlTV08GF1pT/KUtDHsPqC4agoHGsfoDI/e2zH1uu+DCvembjnUG455GU4\n6ygeR3GsWG4UMfdR6qfBPed6eeCVs3zrzvVjLDs9nvDOtQt4cFcTR9s8XLE4g0OEJiAVUfgC8LIQ\nYicggKvQBt4YTI7GzkGsZsFCLWf8fM8wr58emYK2pCyHDXG9alIhEIrwyqlurl1aMqWU0G88d5xX\nGnv459vWcPfW6ugx6oqdnO0exhcM850XGinNy+I9G6sYCoQN99EMIIQYkZbaPehnXQKLrTI/GyHU\nuTWpKOi++47DsOrWkc8NtKneP65qdcVstsUKx3T0q+jKTbDoWnjo/XDeDTWXjthNb5XuDYbJ1UTg\ncIublQvyGIM9D+rfkvT3n1ZySpV7LAFmk0CI1OsUpJR89YmjHGjq56/fviz6/ddp7BxECNhcW0BJ\nbtaMxxVSmbz2tDaTWf9vf0ZK2Z3ZZc09/KEw7//R61TmZ/PIJ64A4O9+18ArjSNFwWkzs+v/XYfD\nlvrYxK89eYSfvXaOB+7Zwrblk/PsvXyym+/vPM37LqnhfZeMnGpXW+xk99k+dp3txRsMc65nmMOt\nKvg2ZkyiwbRQVeiIziFQFtvY/4PdaqYs1067x5eaKIxGjzW4qpV/3Vk69qpZz8yx58Hi69RQHV+/\nam8RR7YmCr6AOoG6vUFOdw9x+8aEk3lnD84SaN6V9Gmr2ZSy++jVUz0caFIi2tLvHSsKXYNUFziw\nW80sL8+NxlxmivF6Hy3Xfm4EaoBW7VajbTMYBykl9/58N//6B9UT6Ld7Wujw+GlodjMcCBEIRdh9\nto+7t1bzyuffwiuffwvf/8AmhgJhnjvSMe6xdxzvZNt/7ODJhjaeP9rBz15TVzS/25dssF1iXj7Z\nzWce2kd9iZMvvmNsVkdtkZNWt5enD8WK2P6g3S/ONdxHM8GmmgKOdwzQ6fEpiy3J/0Fvd5FQFAY7\ntaE2ItpvaAT9Wo1CvjY4MackgaWgi4JL1TCsuk09to7MPsrW3EfDwRCgrARg9sekckphuEe50hJg\nNYmUx3Hev70x6kZr6RtbnBffXXhFRR4nOgYJzWCzvfGyj/5S+/mNBLf/yPC6Zp5IBHb9SGVVTIGd\nJ7p49kgH39t5it8faOV7O0+Rk2UhFJHsP9/PwRY3/lCEa5aWUJmfTWV+Nm9bUcYClz1aPJYIKSX/\n9vRxznQPcd+v9vLxX+5lRUUed2yu4tkj7Qz6Q9F9wxHJL14/R8uo2b6+YJiv/P4wf/LjN3BlW/n+\nBzZFv7zx1BY7kFKJzRWLi3DazPzhYBuA4T6aIbbUFSAlPK1Vmyf7P1Rps7ALEmUf6dZB3VVqjsHo\nLBv3efXTVaV+Okujk9Gi6Omaek6/Xj+QIPsIiKalHmhWr1tTmcpE3xnEWaqK6YYTVxhbLaaUKpr3\nnu/j1VM9fPItixGCMd/FcERyujs2enR5eS6BUCRhgsd0MV7vo49qP7cluM0Sx18GaT8AT/4lND43\npZffv72RBS4766pcfOah/ZzvHeYr71qFEPDm2V52nVUTyzbXxvLITSbBLRsqefFkd9K0tB3Huzja\n5uFfb1/Dp9+6hMr8bL5993ru2FyNLxjhmbir+hdPdPHFRw9xw3++yG/3NNPa72X32V5u/vbLPPDK\nWe65vJYnP3UVi0sTDDOBaL+c4UCY61aUsXFhAa1akNMQhZlhQ3UBVrPgKU2ck7nxrllawhWLixKP\nP9VFQT+Rdx4d+by7GbILYyf4nJLoZLQo8e4jgOpLoO4aqNo8YrfsuJgCQENzPzWFjuRZUbMFfWbD\naAtJw2IypTRP4cmGNrIsJj54WS0lOVm0jhKF5r5hAqFIdDjVumplQb10Muahdw8Hae330trvZSju\noi9TjNfm4vbxXiilfCT9y5lF6D1YhnvH3y8Bb5zuYdfZPr5880quXVbKTf/9EtUFDm7bUMmPXz7D\nm2d6sVvNLCpxjjm53rahkv/ZcYonDrRyzxV1I56TUvKd7Y1U5mfz7k1VWM0mPvu2pdHnqgqyeXR/\nC+/epK7wth/vJNtqZnlFLn/569gIjLK8LH7+4a1cvXT8bpN6u2VQs3MHfaHoh7XIyD6aEbJtZlZX\nunhTG4OaTJxvWV/JLeuT+O07DkNOmTqJg3Ih1VwSe76/KeY6glhMIRKJtZiIdx+B2v7BMSPYYzGF\nqCi4WV8zy11HEJvZMNpC0rCZRUqWwsFmN6sW5OHUhgaNthR2nlBiu6RMiUJ9SQ6rFuTx6P5WPnRF\nHY2dA9z4Xy9FBeifbl3Nn1yaIIU4jYwXzdRLGEtRPZBe0B5vA14F5rYohLQrde/4oiCl5OvPHGfb\nslK21hUipeS/XzhJcY6Nu7bWYLeaefyTV5CTZcVkEmytK+ShXU1YzYKb1lSMOd7SslxWVuTxf282\ncdvGKlzZVp4+1M4je5vxhSLsOdfHV29ZFU2L0xFCcNuGSu7f3ki720dZXhY7jndxeX0RP/jTzTx3\npB23N4jZZOK6FaXkJytqiiPfYSPfYSU/20pdsZMtdcqqybNbyLIY7a9miq21hew7rwKXU4rtdBxS\nvYZcVZDlGhtsdjfFUlFB+ddlWBWwOYvUttHuoyTE3EcRegb9tPR7+eDlmT2ppQW9E+toC0kjFfdR\nOCI51Ormjs1KYCvzsznUEquSPtU1yL88dYzL64tGZJHdur6Srz11lNNdg3x3+yksJhNfvWUlJgGb\nFk4uM3EqjOc++pCU8kOAFVgppXy3lPLdwCpt29wmrPrKJKzkjONwq4f/2XGKT/xyD10Dfv7vzSZe\naezhvm2Lo6b74tLc6OSsLbWFeINhPL4QW2oTtyD4xLZ6GrsGueFbL3Lfr/bysf/dw6EWN90Dfq5d\nVhL9kI3m3RuVhfDTV89yunuI873DXLu8FLNJcMPqCu7cUsN7NlWlJAg691xeyyeuVSeI9dX52Mym\nEV05Daaf+M9NkXOS/4twCLqOK1EQQv2MFwW90tkVbyloFmX8VbPfozKNTONfHOiWwnAgREOLHk+4\nGCyFBL9zHJYUAs2nugYZDoRZW6XNq87PptXtIxKRBEIRPv3gPuxWE9+8Y32sMSDwrvULEAK+80Ij\njx1o5f2X1HD31hru3FKT1NWbTlLJe6yWUrbFPe5AZSPNbaKWwvii8Lt9LVjNggFfiI/97x4Ot7q5\ncnExH0zSkXJLXUzpt9YlFoV3rl1AdYGDzz60nz8cbONTb1nMX7x1yRjrYDS1xU5uWlPB/75+DptW\nin/tBC6iifjMdUuj9+1WM1vrCsmyGI3wZpIttYUIAXl2K7bJ/i96T0HYH2tAV7YKDjyoxEAI9XkP\nDI50H0WvmjtjU9J8ngmtBIhZCr5gmIYmN0LA6sqJXzfjJKvP0EglJVVPQ9UzrSq19iPdQ35eOtHN\noRYP3/uTTdELRp2yPDtX1BfzyL4WbGYTH9VazkwXqYjC81qvo//THt8J/DFzS5ol6KIwTkwhFI7w\n+IFWti0r5aqlJXzx0UMUOm188451I5Q/ntJcO3XFTnzBcKyXfALWVefz1KevonvQT1WBI+l+o7lv\n22KeaGjj/h2nWFyaM6K6NR3c//6NGCMTMshTfwMLL4uleCbA5bCyrCx3cv38T+9Qx/ZrhVFlq2I/\nAwPw7Y1qfKZuIeuZRxDnX49zpfjdsXjCOMQHmg8097Oo2BktYpvVJKvP0LCaTROmjTY0u8nJsrBI\ni80tcKnve0ufl9dP91DgsPL2VWUJX3vrhkpebuzmPZurKMuzJ9wnU6RSvPZJIcRtwNXaph9IKX+X\n2WXNAsK6pZC4/wmoopSuAT+3bajkhtXlDPlDbKktoHSCf+Lnb1xOMByZsPrYbjVPShBA5Tm/dXkp\nzx/rvGArIREZHedooDqSDnaMKwoAf3vDcoYCk8hEee1+dYKr36aCzKVaXcryd6girVBc6nXdNVB3\ndexxvKWg43PHMo/GQXehdnr8vNzYzfu2XkROhkT1GRpWsyAYlpzpHqLd7eOyehVrOds9RKvby+X1\nxTS0uFldmRe9QKzULgJb+33sOtvL5trCpOeAd66t4Hi7h49eNb1WAqRmKQDsBQaklH8UQjiEELlS\nypnv8ZpJxgk0S603y6P7Wsi1W9i2vBQhBB+7pj6lQ799VXnalpmIT711Ca+c6uamtWMD2QaznHBw\n/O6lGpOqXI8fUPO2r4x8LqcUbv3u+K+356uGdPH+dZ8npVnJZpPAZjHx2IFWAqEIt26Y5ZXM8ThL\nYyNAR2HRhhy974ev0+b28d5NVayoyOPrzxzDH4rw8w9v5Wirh3uuqI2+RheFfef7ONszPG4Wkd1q\n5gsJCkqngwlFQQjxUVSvo0KgHqgEvge8NbNLm2GSBJqH/CG2/ccOOrVBJ3dtqU6cCz6DrKvO5/BX\nbsCcxIVlMIsJ+2NTz9LFhQ6oMZmUAMRn4vg9IzOUxsFhM9M14Ke2yMG6qlletBZPTgm0NyR8ymY2\ncax9AItJcPfWGh7adZ6IVPUh53uH+dgv9hAIR6JBZlAxoNwsC48fUEKTLNFkpknFUrgP2Aq8ASCl\nPDkvWmcnCTQfbHHTOeDnvZuqqCl08J7NVQlePPMYgnAREomoRnSDHaqS3pomX3I6BtQ4S0ZZCqm5\nj0DFFfoJcuuGyotrhnei+gwNfabCX719GR+7pp47t1RzvneYm9dWcKjFw+3/8woAa0dlWlUWZHOs\nfQCHzcyqRE0BZwGpiIJfShnQ/5lCCAvzoXW2bimEfCMmSjU0qxjD39643KjqNUgv+mcOwNMCRfXq\ns2cypzYjOZ5IGHpPq5bXLbvhbf94YWvLKY3516VMOfsIYsHmW5MV081WckqVSPv6waFd1Yf8ICNs\nrSsk127lXs3nv746n/VaNfKaKhdfunkVfzjYRnXhyGSSynwlChtrCmbtONtURGGnEOLvgWwhxNuA\nTwC/T+XgQogbgP8CzMCPpJT/mmCfO4Avo4TmgJTyfSmuPbOE4tpMePviRMFNZX62IQgG6Scc95lz\nNylR+PktKj30PT+Z3LFe/k94QRMCYbrwATXO0lg7jJBPjelMIfsIIN9hZWNNPrXFzol3nk0441pd\n6KLwyL0w3MMn7nli3Jd+4NKFfCBBzGBBvhKJZOnos4FUROHzwEeAg8CfA08BP5roRUIIM3A/8Dag\nGdglhHhcSnkkbp8lwN8BV8y6iW7xX1BvL7jUVU5Ds3uEn9DAIG2Eg7H7/U3qwqR1rxqEM9wbOzGl\nQtcxyCmHG/4Z8qoufEBNTolypehWAqTsPvqvuzZMvp5iNpAT3+piubrfflDVevScUqI9SfRg82yN\nJ8AEM5q1E/svpJQ/lFK+V0r5Hu1+Ku6jrUCjlPK0lDIAPAjcMmqfjwL3Syn7AKSUifO/ZoJQnCmv\nxRX6hwOc7x1mjSEKBpkg3jp1N0P3CeW+iATh8CSzwPuboHiJGqEZ39doqjhLlXvL545rcZHa96C6\n0DHtufZpwTkqFTd+rnXDw1M65HUrynjvpio2Lpy9Vd3jioKUMgwsFEJMpftZJRCfRtGsbYtnKbBU\nCPGKEOJ1zd00OwiPch+hrAQg4bQrA4MLJj6m4G6KtZ+w58PBX0/uWO6mkQVoF0pOXAGbf1QzvLlK\nzqiivaGu2Hnh4MOJx4ZOwOLSHP79vetmde+wVGy608ArQogvCiE+p9/S9P4WYAlwLXA38EMhxJgz\nrhDiXiHEbiHE7q6uxBWGU+VExwC3f/cV3N7gyCdCAZWbDdGq5oNa75bVs70XvMHFyRhROATmLLjs\nPjj/GvSdTfE4QTVS05W4R9aUiPev65ZCiu6jixa9PkO3FPThQ8vfqYL4LXtmbm0ZJBVROAU8oe2b\nG3ebiBYg/lNZpW2Lpxl4XEoZlFKeAU6gRGIEUsofSCk3Syk3l5RcWJVuS//ISWJPHGhl7/l+GjtH\n1eKF/aryE6KWwoGmfuqKnUZV73zHPwCHH03/cXX3kcWuTkAdh6F0Oay7S21P1VrwtKoBMRmxFDpT\n7pB60aPXZ+ipuHr9yOexq5QAACAASURBVGX3qf9Rw0Mzt7YMkkqbi68ACCHy1MOUK5l3AUuEEHUo\nMbgLGJ1Z9CjKQnhACFGMciedTvH4k0JKyaP7W/jSo4cZ8If4w6evYkVFHm9qw266BwMjXxDyK/N4\nuCcqCgdb3LM6QGQwTRx5DB67D0reUCftdKEHmgvroeekaky35HrIr4HKzXDyj3D1X098HN3vnZ9G\nSyFXq47vb4pZCHPdfQTq99YtBF0USldC/Vvh5NQGcM12JrQUhBCbhRAHgQbgoBDigBBi00Svk1KG\ngE8CzwBHgYellIeFEF8VQrxL2+0ZoEcIcQTYDvy1lDLx/LsL5P7tjXz2oQMs0sbebT/eiT8Ujval\n7x0aJQrhgMoNzy4Eby/ne4Zpc/uMzCOD2JVyovnGF4Lury5apD5/Q12xxnULNijLIRU/tn7ycqWx\nz5CjUJ0gO49MOvvooqZspfqdQYmDLVeJ4YIN0HcG/IMzu74MkIr76CfAJ6SUtVLKWlSF8wOpHFxK\n+ZSUcqmUsl5K+TVt25eklI9r96WU8nNSypVSyjVSygen+HtMyC3rK/n8jct55OOXs7Iijx3Huzik\nzUkG6Bk9/jLkU/7c7ALw9vPDl05jNQveYfQTMgho83PTLgrahUlhXKrj6G6m/ecnPo5+ZetKc7FY\n2Sr1O/vcqvbBlpPe489GylYrcR7sVBZYfnVsDgWMHWU6B0hFFMJSypf0B1LKl4HMDwpNM9WFDj52\nTT1mk2Db8hL2nOvj+aPKV2gzmxK4jwJgsUF2AYGBbh7a3cS7N1ZR4Ure7tpgnhAVhcPj7zdZ9DTo\n+J5C0bkHq1N/T3eT8oVb0/xZLVulBvR4e1U84WJqWTFV9JN/xyFwn48F7+O3zzFSEYWdQojvCyGu\nFUJcI4T4LrBDCLFRCLEx0wvMBNcuKyUckfzs1bPUlzhZkG9P4D7yK0vBUYC7t5NQOJJyF1SDOU6m\nRCFqKWjtknPKwFms7uvDbVIVhXQGmXXKVqs1tuyZH64jgFL95H9YWWD63zW/RrmS0v0ZmAWkUtG8\nTvv5D6O2b0C1pnhLWlc0DWyozifPbsHjC7G1rpDj7QP0DCn3Ufegn08/uI+vdfTRbrbjMZlY7+3j\nnWsXXHxl+gaZITisfnpaJl9pPB56TMFRpPzW+tUoQFYOFNSOvTLd/yuVqrrt72Pb3M1QksYAuI6+\nnvaDsZPlXMdZpGIp519XPZD04L0QKt4wH0VBSrltOhYynVjMJq5aWsKTDW1sqS2kezBAU6/6ou8+\n28crjT1YnUGwZBGw5FPgG+Rz143JlDWYrwTigoudR6D2yvQcV3cfWWxw7d+NbU1dtnrkSSgShuf/\nEQbbYfOHIbdcBaL7m1TWUropWgIm66T6Hs0JylbBqe3qfnztR9kqOPjb2CjTOcJF2JAkPbxzTQVZ\nFhOX1RdR5LTRo7mPWvu9AJQ7TVy6pIJ3bl2JlSC1rrnzTze4QALDsRTNdF4p6u4jsw0u/TgsedvI\n58tWqb47Ac1SOfuyGgIjI3Dot2rbcA+EvJlxH1lsUKzN7J4v7iNQKahBzWU4WhT87pSGIl1MzFtR\nuHFNBXu++DYqXNkU5djoHQoQiUha+r1kW82YIgH15cwuUC8YNVfBYB4TGFIZQtmF6Q00RkUhSQfe\nslVKALqOqccHH1Z+7dJVsV480XTUNNYojF4DzP3CtXj0ID+MrP2YTPD/IiKVOoUxn9BE2y5GcrKU\n96zImUU4InF7g7T0eVmQb0eE/KpOQfcXD48dy2kwTwkOgc2ppWhmwlJIUjEffxIKeuHI47DyXbDh\nT6Btv8oMiqajZmj4ky4K88lS0H9nkyXW5QDigv9zKwMplUDza8DoLKNE22Y3gaGxhSY2J2TlUJSj\n+v31DAVodXupLHBA2yhLoff0yA+Ewdwiu0C5R1IhMKTmaxQugr0/A0+b+ixd6Iky2uYiyTVXQS1Y\nHWpojhCqMd3aO6BkBTz7Bdj785ivPz+NhWvx6MI0n2IKxUuVIORVqoFHOnaX+ju37oOBjulZS1Zu\ndLZLpkgqCkKIclRX02whxAZAd6rnAZldVSbY9SN47ksjt1ns8LmjFDnVl7Bn0E9Ln5dVC1zQpFkK\nevvcX39wmhdsMK1UboKPvpDavoFhJQLla1Qm0jeXq5PGX+xRJ+6pore5MCcRJ5NZXbXu+am65VZA\n7VVq+6Jt8Np31H5ZebGLmXRTvhoQ4CjOzPFnIxabsgocRWOfK18Lx55Qt+ngHd+ELR/J6FuMZym8\nHbgH1cjuG8REwQP8fZLXzF4WbVN/UJ2Ow7D7x+BuptBZC0Cr20vPUIAql1UNOjdnqZ707/2p4T6a\ny5x6AY4/NWLs6rgEh8DqhNW3AxI6jsDr94O75QJFwQ/CPPJqdDTv+jace1Xdr9wU2/fmb8V68ZQs\nz1w2TG453PMEVKybeN+5xLt/kvj/cv0/Qf00ZuUvvDzjb5FUFKSUPwN+JoR4t5TytxlfSaapWKtu\nOmdfVqLg7aVYy6jQ5yVU52n/fItNfblW3TbdqzWYTpwl6kqv6xhUpuAVDWgxBWu28uc371aioNcv\nTBU9jjUepStivux48msyfgUZJV0puBcTJUsTby+sg8Jp+rtPE6lkH22Kn3EghCgQQvxTBtc0PWRr\nAWRvHwVOZa4f1EShMlfTymRZIAZzi7K4qtWJCAXUNLR4i8Kq3Q9cYHO0cDB5kNnAYJpIRRRulFL2\n6w+00Zk3ZW5J00RcqqnVbMKVbeVwq+r+WKH3+Uo18GhwcVNQp07sqYiCfuKPbwZn0yrdAxdoKeit\nVQwMZpBURMEcn4IqhMgGLv5Pri4KWqygKMeGNxjGbBKUZmv+WMtFOFfWYPKYTKpAKZXUQt1FZI2z\nFKKiMHRh6wgHkweZDQymiVRE4ZfA80KIjwghPgI8B/wss8uaBqx29cXWitKKNBdSeZ4di9SzQC5+\n7TNIEb3mYKJ5BfqJ3xbXB0u/H7xAUQj5DevUYMaZUBSklP8G/7+9Mw+PqkwW/q8ICQmLgUiIsogw\nV1CyhwBRrsCAgriLci+y42UcZkQcHzXg6AjMgCOffOK96CcuowKXGVl0FGdwVEAmIrIpiCwiyiJh\nFBAEZBGy1PfHOd3pJN1Jk607Sf2e5zzp857T76l+06frvFX1VjENuMLd/qCq/6e6BasRYlr4KAVH\nAbRuHu0TL243aL0hIclJCf3jd2Wf508pNIwGpApmCufsQcQIOcEsXgOnclq+qi4XkcYi0uw8ynKG\nLzFxRUrBXcDWpnlMUbZKu0HrD77O5gvKKKTkTymIOD6GSvsUzpmj2Qg5waS5+AWwBHjebWqDU1u5\n9hPTvJT5qE2LmOLZKo36QUIX5295fgWvT6FEGvWoxpWPPgomJNUwqplgfAr3AD1xFq2hqruAVtUp\nVI3ROM7H0ewxH9lMoV4S0wIuaFt+BJI3+qikUmhS+XUK5mg2woBglMJZVfWWJRORhjjFdWo/Pj6F\nuCY+5iPvTMGUQr0iIRF2vQvzB8HyKf7P8ZiISq58jmxSBT6Fs6YUjJATbDnO3+LkQLoWWAy8Xb1i\n1RAxLRznoipZHS/khpSLyWjfwmemYDdovSJ9uJP87PBO+Oi/i3IR+eL1KZQoWh9VFUrhnH3njJAT\njFKYBBwGPgd+CSwDHq1OoWqMmDhndeq5k8Q3a8SzQzO4IDqy/GyVRt2ky80wdjn0mejULThxoPQ5\nnrDTyBIzhajGlVcK+efMj2WEnDKjj0QkApinqsOAF2tGpBrEt4BOo2ZF7b4VsIz6h6dAzfHc0gnu\nzp1yktaVfGCIalJ+OGt52EzBCAPKnCmoagHQXkTq5jc1UAEdmynUbzy1CDwFa3zxpM0umYU0skkV\n5D6ydQpG6AlmncJu4CMRWQp458eq+lTgt9QSApXaLK8solG3uaCN8/e4P6VwsnTkEbjmo6rIklo3\nn7+M2kMwSuFrd2sANCvn3NqFN1NqoJmC3aD1kshop7iSP6WQd7q0PwHM0WzUGYLxKTRT1QdrSJ6a\nJdBMId/WKdR7mrcLYD465X+mENkE8s9AYUHZRXLKwpSCEQYE41PoWUOy1DwBzUcepWApB+otsW0D\nmI8CKAVvUrxKmJBMKRhhQDAhqZtFZKmIjBCRQZ4tmM5F5DoR2SkiX4nIJD/HR4vIYRHZ7G5jz/sT\nVIaGUU68+Wk/M4WG0dVX0tAIf2LbOdFHJbOmBlQKnkI7FVQKhYVOeLQFNxghJhifQjRwBPAtRKrA\nG2W9yTU9PQtcC+QCG0RkqapuL3HqQlUdH7zIVYzPqmYvFgVixLaD/J/g1PfQNL6oPe80RLYtfb5n\nMdu5k0DC+V/PG9xgs1MjtJSrFFR1TAX77g58paq7AUTkNeAWoKRSCC2eVc2+WBSI0dyzVuGb4krh\n3KnSq5mhyPlcUfOR5dsywoRgsqS2FZG/isghd3tdRPw8KpWiDeBrlM1120pyu4hsEZElItIugAx3\ni8hGEdl4+PDhIC59HthMwfCH7wI2X86dKp33CCpffc3ybRlhQjA+hVeApUBrd3vbbasK3gYuVdUU\nyqjopqovqGqmqmbGx8f7O6Xi+FMKNlMwYt3nnpIRSOU5miuqFMx8ZIQJwSiFeFV9RVXz3e1VIJhf\n5gOA75N/W7fNi6oeUVV33sxLQNcg+q1afNJne7EC6kZMC8dM5BuBVJDvfDdK1lKAKlAKZj4ywoNg\nlMIRERkuIhHuNhzH8VweG4DLRKSDmyZjCM6Mw4uI+Ja4uhmnwlvN4pkp+EaZWGIyQ6QoAslDnp+q\nax4q7VPw1AW3mYIRWoJRCncB/wF8B3wL3AGU63xW1XxgPPAuzo/9IlXdJiK/F5Gb3dMmiMg2EfkM\nmACMPv+PUEli4kAL4OyJojabKRjgmJCOfVO0H6iWApSIPqoAlm/LCBOCiT7ah/MUf96o6jKcVNu+\nbY/5vH4YeLgifVcZvgvYomOd1/nn7OY0nAikvathyV3OrCF9uNPuL/qosusUvDMFm6EaoSWY6KO5\nItLcZ7+FiLxcvWLVIE3dyqI/HixqswpYBkCngY5i+GYdfPQ07F7ltPvLfeRpq7RPwb53RmgJxnyU\noqrHPDuq+gOQXn0i1TCeKBNfh6LNFAyATv1h/Ab49cfQMAY2ukF3/nwKDSKcc/IqGpJq5iMjPAhG\nKTQQkRaeHRGJI7iV0LUDv0rhJ3tiM4qIvgAuvx4ObXP2/SkFT3uFZwpmPjLCg2B+3P8v8LGILHb3\nBwPTq0+kGqZRM4huXjzKpOCsPbEZxUn5T9j6uvM6oFKoRE0FMx8ZYUIwjuZ5IrKRotxHg/zkL6rd\nlEyTnG/ZKo0S/KwvNL4QTh/x71MAxwFd0egjKwFrhAlBmYFcJVC3FIEvse3gh31F+zZTMEoSEQlJ\nt8P6F4rX8/YlsnHF1yl401yYUjBCS93xDVSGWDf00EP+OSd1tmH40isbLkqGJi39H49qXAVpLkwp\nGKElGEdz3ad5O2fx2hk3yMpCUg1/NI2HjJGBj0c1rYRPweqCG+GBKQXwiUDKtWInRsWJbFwFK5rt\nYcQILaYUAGIvcf4e329RIEbFiWpSidxHZj4ywgNTClBUUOXYfltEZFScSq1TMKVghAemFAAat3Rs\nucf3281pVByPUihZ1zkYCs6BRDgrow0jhNSJ6KO8vDxyc3P56aefKt5J/9ecsMN9B2HAImgUBztq\nPpO3UYtpcS0M6AY7toM0IDo6mrZt2xIZGUQ67HwLgzbCgzqhFHJzc2nWrBmXXnopIlKxTr5vCFoI\nLdrDoXxo3t4pwGMYwXLyMJzIhYROaIOGHDlyhNzcXDp06FD+ewvyrJaCERbUCfPRTz/9xIUXXlhx\nhQBO1EdBXtHUvzJ9GfWTBu7tpIWICBdeeGHws1er4WGECXVCKQCVUwjg+BAK86CwwG2oM0Nj1BRS\npBTgPL+TllrFCBPsl8+D54Y8+Z3ztxbNFF599VX+9a9/1ci1Ro8ezZIlSwAYO3Ys27cHzn6yatUq\n1qxZ492fM2cO8+bNq1b5Tpw4we9+9zvS09NJT09nyJAhbNu2rdg5jz/+eIX6Lu/zepXCj986kWzH\n9jvFm04FUb22wErAGuGBKQUPkU0cxZB3xvlbQ06//Pz8MveDobJKoSLXBHjppZfo0qVLwOMllcK4\nceMYObKMFcGV5OjRo1xzzTW0adOGNWvWsGnTJh566CHGjh3L2rVrvecFUgqqSmFhYcD+y/u8NIx2\nvjvnTsFPx5zt7I+w/c3yhTfzkREmmFLwEBkNCYlObpuExPNWCvPmzSMlJYXU1FRGjBgBwN69e+nb\nty8pKSn069ePb75x6v2OHj2acePG0aNHD7Kzs5kyZQojRoygZ8+ejBgxgoKCAh566CG6detGSkoK\nzz//vPc6M2bMIDk5mdTUVCZNmsSSJUvYuHEjw4YNIy0tjTNnzhSTq0+fPtx3332kpaWRlJTE+vXr\nAYK+pqoyfvx4OnfuzDXXXMOhQ4eK9b1x40YA/vGPf5CRkUFqair9+vVj7969zJkzh1mzZpGWlsaH\nH37IlClTmDlzJgCbN28mKyuLlJQUbrvtNn744QdvnxMnTqR79+506tSJDz/8EIBt27bRvXt30tLS\nSElJYdeuXaX+Bw888ABTp05l3LhxxMTEANC1a1eWLl1KdnY2AJMmTeLMmTOkpaUxbNgw9u7dS+fO\nnRk5ciRJSUns37+fX/3qV2RmZpKYmMjkyZP9ft6mTZvyyCOPkJqaSlZWFgcPHnS+M57v0EXJkJDk\nzB4OBZFL0hzNRphQJ6KPfJn69ja2/+tElfbZpfUFTL4pMeDxbdu2MW3aNNasWUPLli05evQoAPfe\ney+jRo1i1KhRvPzyy0yYMIE333SeGnNzc1mzZg0RERFMmTKF7du3s3r1amJiYnjhhReIjY1lw4YN\nnD17lp49e9K/f3+++OIL3nrrLdatW0fjxo05evQocXFxPPPMM8ycOZPMzEy/8p0+fZrNmzeTk5PD\nXXfdxdatWwGCuuamTZvYuXMn27dv5+DBg3Tp0oW77rqrWP+HDx/mF7/4BTk5OXTo0MEr17hx42ja\ntCkPPvggACtWrPC+Z+TIkcyePZvevXvz2GOPMXXqVJ5++mnAmbmsX7+eZcuWMXXqVJYvX86cOXO4\n7777GDZsGOfOnaOgoKCYDCdPnmTPnj0MHDiQdevWMX78eFq2bMnFF1/M1KlTycjI4NNPP+WJJ57g\nmWeeYfPmzYCjuHft2sXcuXPJysoCYPr06cTFxVFQUEC/fv3YsmULKSkpxa536tQpsrKymD59OtnZ\n2bz44os8+uijxQdexPmhP1jcfOUXC0k1wgSbKVQBK1euZPDgwbRs6WTPjItzQlk//vhjhg4dCsCI\nESNYvbooE+vgwYOJiChaqHTzzTd7n27fe+895s2bR1paGj169ODIkSPs2rWL5cuXM2bMGBo3blzs\nOuVx5513AtCrVy9OnDjBsWPHgr5mTk4Od955JxEREbRu3Zq+ffuW6n/t2rX06tXLG3pZnlzHjx/n\n2LFj9O7dG4BRo0aRk5PjPT5o0CDAecrfu3cvAFdeeSWPP/44M2bMYN++fV65PezYsYOuXbsCkJ2d\nzeuvv86CBQtYuXIlBQUFdO7cma+//tqvPO3bt/cqBIBFixaRkZFBeno627Zt8+tHiIqK4sYbbywl\nZyk8SqG8BW0FeeZoNsKCOjdTKOuJPpxo0qRJwH1VZfbs2QwYMKDYOe+++26FrlUyCsazH8w1ly1b\nVqFrVoZGjZwn5oiICK+/Y+jQofTo0YO///3vXH/99Tz//POlFJRHyTZo0IBLLnHyWfXo0QOAQ4cO\nBfQH+I7Dnj17mDlzJhs2bKBFixaMHj3ab1hpZGSkdxx95SxFgygnA+/x/dD8ksAfuuCsk2XVMEKM\nzRSqgL59+7J48WKOHHGiTDzmo6uuuorXXnsNgAULFnD11VcH1d+AAQN47rnnyMtz6vZ++eWXnDp1\nimuvvZZXXnmF06dPF7tOs2bN+PHHHwP2t3DhQgBWr15NbGwssbGxQV+zV69eLFy4kIKCAr799ls+\n+OCDUu/NysoiJyeHPXv2BCVXbGwsLVq08PoL5s+f7501BGL37t107NiRCRMmcMstt7Bly5Zixy+/\n/HI+/fRTAAoKCsjNzeXYsWOsW7eO3NxcVq1axZVXXgk4P+iez1mSEydO0KRJE2JjYzl48CDvvPNO\nmXKVi8dPUJ4JKd/StRvhQZ2bKYSCxMREHnnkEXr37k1ERATp6em8+uqrzJ49mzFjxvDkk08SHx/P\nK6+8ElR/Y8eOZe/evWRkZKCqxMfH8+abb3LdddexefNmMjMziYqK4vrrr+fxxx/3Oq5jYmL4+OOP\nS5lWoqOjSU9PJy8vj5dffvm8rnnbbbexcuVKunTpwiWXXOL9YfUlPj6eF154gUGDBlFYWEirVq14\n//33uemmm7jjjjt46623mD17drH3zJ07l3HjxnH69Gk6duxY7tgsWrSI+fPnExkZyUUXXcRvf/vb\nYsebNWtGq1atWLFiBTNmzOC2226jZcuWDBw4kFmzZvHiiy8SFeX86N59992kpKSQkZHB9OnFy42n\npqaSnp7O5ZdfTrt27ejZs2eZcpWLVylshc4DA59XkGchqUZYIFqR5F0hJDMzUz0RIB527NjBFVdc\nESKJwps+ffqU6YSuSxw8eJAbbriB7OxsBg0aRMOGDfniiy/YtGmT169S0+zYsYMr3hsCrdNh8KuB\nT/yfdGidAXf8qcZkM+oXIvKJqpb7Q2DmI6POkJCQwHvvvceGDRvo0aMHycnJTJkyhaSkpBALllS+\n+cgczUaYYOajOs6qVatCLUKNEhcXx5NPPhlqMYqTkAg7lzkLIyNj/J+Tf9bMR0ZYUK0zBRG5TkR2\nishXIjKpjPNuFxEVkbpv4zDqHwmJTj6kw18EPqfAch8Z4UG1KQURiQCeBQYCXYA7RaRUTKCINAPu\nA9ZVlyyGEVISXPNV7kb/xwsLnTKeDaNrTibDCEB1zhS6A1+p6m5VPQe8Btzi57w/ADOASlTIMYww\nJq4jxF8Ony/xf/yHPc5MoWWnmpXLMPxQnUqhDbDfZz/XbfMiIhlAO1X9ezXKYRihRQRS/gP2r4Wj\ne0of9zihE2rHwkujbhOy6CMRaQA8BTwQxLl3i8hGEdl4+PDh6heulmGps4uoztTZUImxTh7s/PU3\nWzi4DRBnNmEYIaY6lcIBoJ3Pflu3zUMzIAlYJSJ7gSxgqT9ns6q+oKqZqpoZHx9fjSLXPJY6u+qo\nbOrsYKjwWDe/BNr3hC0LS+dBOrgVLvwZRDWusFyGUWWoarVsOOGuu4EOQBTwGZBYxvmrgMzy+u3a\ntauWZPv27aXaapq5c+dqcnKypqSk6PDhw1VVdc+ePfrzn/9ck5OTtW/fvrpv3z5VVR01apT+8pe/\n1O7du+v999+vkydP1uHDh+tVV12lQ4YM0fz8fH3wwQc1MzNTk5OTdc6cOd7rPPHEE5qUlKQpKSk6\nceJEXbx4sTZp0kQ7deqkqampevr06WJy9e7dWydMmKCpqamamJio69atU1UN+pqFhYV6zz33aKdO\nnbRfv346cOBAXbx4sbfvDRs2qKrqO++8o+np6ZqSkqJ9+/bVPXv2aEJCgrZu3VpTU1M1JydHJ0+e\nrE8++aSqqm7atEl79OihycnJeuutt+rRo0e9fWZnZ2u3bt30sssu05ycHFVV3bp1q3br1k1TU1M1\nOTlZv/zyy1L/g9GjR+uyZctKtR86dEivvvpqVVWdOHGiNmjQQFNTU3Xo0KGqqjp//nxv33fffbfm\n5+drfn6+jho1ShMTEzUpKUmfeuqpcsfaH8W+mxtfUZ18gerfHlBd8QfVH75x2p9OVV04oty+DKMy\nABs1iN/ualunoKr5IjIeeBeIAF5W1W0i8ntXuKXVcuF3JsF3n1dtnxclw8AnAh621Nm1N3X2jh07\nWLhwIR999BGRkZH8+te/ZsGCBSQmJnLgwAHvWB07dozmzZuXO9Zl0uVWyJkJn7zqlH49eQgGPO44\nmtOGnn9/hlENVOviNVVdBiwr0fZYgHP7VKcs1UlZqbPfeOMNwEmd7Sn0AuWnzt6yZYvXdn/8+PEa\nSZ3t75o1lTp78ODB3uOBUmdPnz6d3NxcBg0axGWXXVasT3+ps5s2bUpGRgaPPfaYN3V2RkZGsfet\nWLGCTz75hG7dugFw5swZWrVqxU033cTu3bu59957ueGGG+jfv3+ZnykoYprD/Y6S4a/jYNubRb4G\nczIbYULdW9FcxhN9OGGpswNTk6mzVZVRo0bxxz/+sdSxzz77jHfffZc5c+awaNGigMkEK0TyYPjs\nL7B6lrNvSsEIEyz3URVgqbNrb+rsfv36sWTJEm+Z0aNHj7Jv3z6+//57CgsLuf3225k2bZq37/LG\nOmg69IamCfD1CohqBrFl1FowjBqk7s0UQoClzq69qbMXLFjAtGnT6N+/P4WFhURGRvLss88SExPD\nmDFjKCwsBPDOJMob66CJaAhJd8DaZyGhCzSw5zMjTAjGGx1OW7hGH4UrvhFCdZ3vvvtOu3btqgsX\nLtS8vDxVVd2xY4f++c9/DplMZX43D2xyopHe/k3NCWTUWwgy+sgeT4w6Q9imzg7Exanw80eh65hQ\nS2IYXsx8VMex1NlhjAj0fijUUhhGMWymYBiGYXipM0pBa1lZUaPuY99JozZSJ5RCdHQ0R44csZvQ\nCBtUlSNHjhAdbTUSjNpFnfAptG3bltzcXCyDqhFOREdH07Zt21CLYRjnRZ1QCpGRkd4UC4ZhGEbF\nqRPmI8MwDKNqMKVgGIZheDGlYBiGYXiR2haxIyKHgX0VfHtL4PsqFKc6MBmrBpOxagh3GcNdPggf\nGdurarmlK2udUqgMIrJRVStQHaXmMBmrBpOxagh3GcNdPqgdMvpi5iPDMAzDiykFwzAMw0t9Uwov\nhFqAIDAZqwaTEBdNRQAABi5JREFUsWoIdxnDXT6oHTJ6qVc+BcMwDKNs6ttMwTAMwyiDeqMUROQ6\nEdkpIl+JyKRQywMgIu1E5AMR2S4i20TkPrc9TkTeF5Fd7t8WIZYzQkQ2icjf3P0OIrLOHcuFIhIV\nYvmai8gSEflCRHaIyJVhOIb3u//jrSLyFxGJDvU4isjLInJIRLb6tPkdN3H4H1fWLSKSEUIZn3T/\n11tE5K8i0tzn2MOujDtFZECoZPQ59oCIqIi0dPdDMo7nQ71QCiISATwLDAS6AHeKSJfQSgVAPvCA\nqnYBsoB7XLkmAStU9TJghbsfSu4DdvjszwBmqeq/AT8A/xUSqYr4b+Afqno5kIoja9iMoYi0ASYA\nmaqaBEQAQwj9OL4KXFeiLdC4DQQuc7e7gedCKOP7QJKqpgBfAg8DuPfOECDRfc//c+/9UMiIiLQD\n+gPf+DSHahyDpl4oBaA78JWq7lbVc8BrwC0hlglV/VZVP3Vf/4jzY9YGR7a57mlzgVtDIyGISFvg\nBuAld1+AvsAS95RQyxcL9AL+BKCq51T1GGE0hi4NgRgRaQg0Br4lxOOoqjnA0RLNgcbtFmCeW+53\nLdBcRC4OhYyq+p6q5ru7awFPKtpbgNdU9ayq7gG+wrn3a1xGl1lANuDruA3JOJ4P9UUptAH2++zn\num1hg4hcCqQD64AEVf3WPfQdkBAisQCexvliF7r7FwLHfG7KUI9lB+Aw8Ipr4npJRJoQRmOoqgeA\nmThPjN8Cx4FPCK9x9BBo3ML1HroLeMd9HTYyisgtwAFV/azEobCRMRD1RSmENSLSFHgd+I2qnvA9\npk54WEhCxETkRuCQqn4SiusHSUMgA3hOVdOBU5QwFYVyDAFcu/wtOAqsNdAEP+aGcCPU41YeIvII\njgl2Qahl8UVEGgO/BR4LtSwVob4ohQNAO5/9tm5byBGRSByFsEBV33CbD3qmlO7fQyESrydws4js\nxTG59cWx3zd3zSAQ+rHMBXJVdZ27vwRHSYTLGAJcA+xR1cOqmge8gTO24TSOHgKNW1jdQyIyGrgR\nGKZFcfXhIuPPcB4APnPvnbbApyJyEeEjY0Dqi1LYAFzmRntE4TijloZYJo99/k/ADlV9yufQUmCU\n+3oU8FZNywagqg+raltVvRRnzFaq6jDgA+COUMsHoKrfAftFpLPb1A/YTpiMocs3QJaINHb/5x4Z\nw2YcfQg0bkuBkW70TBZw3MfMVKOIyHU4Js2bVfW0z6GlwBARaSQiHXCcuetrWj5V/VxVW6nqpe69\nkwtkuN/VsBnHgKhqvdiA63EiFb4GHgm1PK5M/44zPd8CbHa363Hs9iuAXcByIC4MZO0D/M193RHn\nZvsKWAw0CrFsacBGdxzfBFqE2xgCU4EvgK3AfKBRqMcR+AuOjyMP54frvwKNGyA4EXxfA5/jRFKF\nSsavcOzynntmjs/5j7gy7gQGhkrGEsf3Ai1DOY7ns9mKZsMwDMNLfTEfGYZhGEFgSsEwDMPwYkrB\nMAzD8GJKwTAMw/BiSsEwDMPwYkrBMGoQEekjbrZZwwhHTCkYhmEYXkwpGIYfRGS4iKwXkc0i8rw4\nNSVOisgsty7CChGJd89NE5G1Pvn9PTUI/k1ElovIZyLyqYj8zO2+qRTVf1jgrnI2jLDAlIJhlEBE\nrgD+E+ipqmlAATAMJ5HdRlVNBP4JTHbfMg+YqE5+/8992hcAz6pqKnAVzqpXcLLh/gantkdHnDxI\nhhEWNCz/FMOod/QDugIb3If4GJzEcIXAQvec/wXecOs5NFfVf7rtc4HFItIMaKOqfwVQ1Z8A3P7W\nq2quu78ZuBRYXf0fyzDKx5SCYZRGgLmq+nCxRpHflTivojlizvq8LsDuQyOMMPORYZRmBXCHiLQC\nb93i9jj3iyer6VBgtaoeB34Qkavd9hHAP9WppJcrIre6fTRy8+wbRlhjTyiGUQJV3S4ijwLviUgD\nnOyX9+AU8OnuHjuE43cAJ8X0HPdHfzcwxm0fATwvIr93+xhcgx/DMCqEZUk1jCARkZOq2jTUchhG\ndWLmI8MwDMOLzRQMwzAMLzZTMAzDMLyYUjAMwzC8mFIwDMMwvJhSMAzDMLyYUjAMwzC8mFIwDMMw\nvPx/z02Qxk0yYMYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.63671875,validation accuracy: 0.5625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXqDmmwPAVAa",
        "colab_type": "text"
      },
      "source": [
        "## testing my own image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7QLCHZoCzLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgn1 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgn1.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgn2 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgn2.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgn3 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgn3.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgn4 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgn4.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgn5 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgn5.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgp1 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgp1.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgp2 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgp2.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgp3 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgp3.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgp4 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgp4.jpg',\n",
        "    target_size=(64, 64)\n",
        ")\n",
        "\n",
        "imgp5 = image.load_img(\n",
        "    'drive/My Drive/Colab Notebooks/neural_network/nn_diploma/img/imgp5.jpg',\n",
        "    target_size=(64, 64)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7I-i5iODeqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgn1_array = image.img_to_array(imgn1)\n",
        "imgn1_array = np.expand_dims(imgn1_array, axis=0)\n",
        "\n",
        "imgn2_array = image.img_to_array(imgn2)\n",
        "imgn2_array = np.expand_dims(imgn2_array, axis=0)\n",
        "\n",
        "imgn3_array = image.img_to_array(imgn3)\n",
        "imgn3_array = np.expand_dims(imgn3_array, axis=0)\n",
        "\n",
        "imgn4_array = image.img_to_array(imgn4)\n",
        "imgn4_array = np.expand_dims(imgn4_array, axis=0)\n",
        "\n",
        "imgn5_array = image.img_to_array(imgn5)\n",
        "imgn5_array = np.expand_dims(imgn5_array, axis=0)\n",
        "\n",
        "imgp1_array = image.img_to_array(imgp1)\n",
        "imgp1_array = np.expand_dims(imgp1_array, axis=0)\n",
        "\n",
        "imgp2_array = image.img_to_array(imgp2)\n",
        "imgp2_array = np.expand_dims(imgp2_array, axis=0)\n",
        "\n",
        "imgp3_array = image.img_to_array(imgp3)\n",
        "imgp3_array = np.expand_dims(imgp3_array, axis=0)\n",
        "\n",
        "imgp4_array = image.img_to_array(imgp4)\n",
        "imgp4_array = np.expand_dims(imgp4_array, axis=0)\n",
        "\n",
        "imgp5_array = image.img_to_array(imgp5)\n",
        "imgp5_array = np.expand_dims(imgp5_array, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bfli1OEDZUU",
        "colab_type": "code",
        "outputId": "08b17015-1763-4d53-9e37-5ca4b153dbe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "plt.imshow(imgp1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f419cee3cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDB0IW/gGQwfCFr7B0IH4/8/PeIT+1FxiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVArw7GhidTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scaling features \n",
        "v_min = imgn1_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgn1_array.max(axis=(0, 1), keepdims=True)\n",
        "imgn1_array_scld = (imgn1_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgn2_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgn2_array.max(axis=(0, 1), keepdims=True)\n",
        "imgn2_array_scld = (imgn2_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgn3_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgn3_array.max(axis=(0, 1), keepdims=True)\n",
        "imgn3_array_scld = (imgn3_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgn4_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgn4_array.max(axis=(0, 1), keepdims=True)\n",
        "imgn4_array_scld = (imgn4_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgn5_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgn5_array.max(axis=(0, 1), keepdims=True)\n",
        "imgn5_array_scld = (imgn5_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgp1_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgp1_array.max(axis=(0, 1), keepdims=True)\n",
        "imgp1_array_scld = (imgp1_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgp2_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgp2_array.max(axis=(0, 1), keepdims=True)\n",
        "imgp2_array_scld = (imgp2_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgp3_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgp3_array.max(axis=(0, 1), keepdims=True)\n",
        "imgp3_array_scld = (imgp3_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgp4_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgp4_array.max(axis=(0, 1), keepdims=True)\n",
        "imgp4_array_scld = (imgp4_array - v_min)/(v_max - v_min)\n",
        "\n",
        "v_min = imgp5_array.min(axis=(0, 1), keepdims=True)\n",
        "v_max = imgp5_array.max(axis=(0, 1), keepdims=True)\n",
        "imgp5_array_scld = (imgp5_array - v_min)/(v_max - v_min)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhL3HZGiweMe",
        "colab_type": "code",
        "outputId": "9c708c0a-f7af-4641-c07b-955728286e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 02 \n",
        "# scaled x train\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization(input_shape=(64, 64, 3)))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (7, 7), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train_happy_scld,\n",
        "    y_train_happy_cat,\n",
        "    batch_size=batch_size,\n",
        "    epochs=100,\n",
        "    callbacks=check('02'),\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "result(history)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 480 samples, validate on 120 samples\n",
            "Epoch 1/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 3.9594 - acc: 0.5125\n",
            "Epoch 00001: val_acc improved from -inf to 0.45000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-01-0.45000-0.51875.hdf5\n",
            "480/480 [==============================] - 2s 4ms/sample - loss: 3.7269 - acc: 0.5188 - val_loss: 0.8663 - val_acc: 0.4500\n",
            "Epoch 2/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.9514 - acc: 0.5150\n",
            "Epoch 00002: val_acc improved from 0.45000 to 0.48333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-02-0.48333-0.51250.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.9086 - acc: 0.5125 - val_loss: 0.6916 - val_acc: 0.4833\n",
            "Epoch 3/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6960 - acc: 0.5175\n",
            "Epoch 00003: val_acc improved from 0.48333 to 0.55000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-03-0.55000-0.51667.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6984 - acc: 0.5167 - val_loss: 0.6885 - val_acc: 0.5500\n",
            "Epoch 4/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6873 - acc: 0.5325\n",
            "Epoch 00004: val_acc did not improve from 0.55000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6850 - acc: 0.5521 - val_loss: 0.6863 - val_acc: 0.5500\n",
            "Epoch 5/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6773 - acc: 0.6100\n",
            "Epoch 00005: val_acc improved from 0.55000 to 0.56667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-05-0.56667-0.59583.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6784 - acc: 0.5958 - val_loss: 0.6757 - val_acc: 0.5667\n",
            "Epoch 6/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6411 - acc: 0.6650\n",
            "Epoch 00006: val_acc improved from 0.56667 to 0.70000, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-06-0.70000-0.66042.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6438 - acc: 0.6604 - val_loss: 0.6492 - val_acc: 0.7000\n",
            "Epoch 7/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.6018 - acc: 0.6675\n",
            "Epoch 00007: val_acc did not improve from 0.70000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.6077 - acc: 0.6583 - val_loss: 0.6451 - val_acc: 0.6667\n",
            "Epoch 8/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5761 - acc: 0.7025\n",
            "Epoch 00008: val_acc did not improve from 0.70000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5788 - acc: 0.6896 - val_loss: 0.6148 - val_acc: 0.6750\n",
            "Epoch 9/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5305 - acc: 0.7250\n",
            "Epoch 00009: val_acc did not improve from 0.70000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5217 - acc: 0.7292 - val_loss: 0.6285 - val_acc: 0.6833\n",
            "Epoch 10/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.5221 - acc: 0.7325\n",
            "Epoch 00010: val_acc did not improve from 0.70000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.5152 - acc: 0.7375 - val_loss: 0.6768 - val_acc: 0.5750\n",
            "Epoch 11/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4443 - acc: 0.7700\n",
            "Epoch 00011: val_acc did not improve from 0.70000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4491 - acc: 0.7688 - val_loss: 0.6286 - val_acc: 0.6667\n",
            "Epoch 12/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4308 - acc: 0.7800\n",
            "Epoch 00012: val_acc did not improve from 0.70000\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4241 - acc: 0.7792 - val_loss: 0.7670 - val_acc: 0.4750\n",
            "Epoch 13/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.4413 - acc: 0.7775\n",
            "Epoch 00013: val_acc improved from 0.70000 to 0.70833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-13-0.70833-0.77292.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.4412 - acc: 0.7729 - val_loss: 0.6419 - val_acc: 0.7083\n",
            "Epoch 14/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3759 - acc: 0.8250\n",
            "Epoch 00014: val_acc did not improve from 0.70833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3747 - acc: 0.8313 - val_loss: 0.7159 - val_acc: 0.5250\n",
            "Epoch 15/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3746 - acc: 0.8425\n",
            "Epoch 00015: val_acc did not improve from 0.70833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3676 - acc: 0.8417 - val_loss: 0.6678 - val_acc: 0.5583\n",
            "Epoch 16/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3496 - acc: 0.8450\n",
            "Epoch 00016: val_acc did not improve from 0.70833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.3553 - acc: 0.8438 - val_loss: 0.6093 - val_acc: 0.6500\n",
            "Epoch 17/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2800 - acc: 0.8725\n",
            "Epoch 00017: val_acc did not improve from 0.70833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2891 - acc: 0.8667 - val_loss: 0.6441 - val_acc: 0.6417\n",
            "Epoch 18/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2900 - acc: 0.8475\n",
            "Epoch 00018: val_acc did not improve from 0.70833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2788 - acc: 0.8604 - val_loss: 0.5926 - val_acc: 0.6833\n",
            "Epoch 19/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.3036 - acc: 0.8675\n",
            "Epoch 00019: val_acc improved from 0.70833 to 0.71667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-19-0.71667-0.87292.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2942 - acc: 0.8729 - val_loss: 0.5317 - val_acc: 0.7167\n",
            "Epoch 20/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2642 - acc: 0.8950\n",
            "Epoch 00020: val_acc did not improve from 0.71667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2544 - acc: 0.9000 - val_loss: 0.6208 - val_acc: 0.6500\n",
            "Epoch 21/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2590 - acc: 0.8925\n",
            "Epoch 00021: val_acc did not improve from 0.71667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2514 - acc: 0.8896 - val_loss: 0.5223 - val_acc: 0.7083\n",
            "Epoch 22/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2375 - acc: 0.9150\n",
            "Epoch 00022: val_acc improved from 0.71667 to 0.74167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-22-0.74167-0.91250.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2339 - acc: 0.9125 - val_loss: 0.5035 - val_acc: 0.7417\n",
            "Epoch 23/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2338 - acc: 0.8825\n",
            "Epoch 00023: val_acc did not improve from 0.74167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2339 - acc: 0.8875 - val_loss: 0.5537 - val_acc: 0.7083\n",
            "Epoch 24/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2290 - acc: 0.9175\n",
            "Epoch 00024: val_acc did not improve from 0.74167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2285 - acc: 0.9104 - val_loss: 0.4748 - val_acc: 0.7333\n",
            "Epoch 25/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1828 - acc: 0.9225\n",
            "Epoch 00025: val_acc did not improve from 0.74167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2158 - acc: 0.9062 - val_loss: 0.6952 - val_acc: 0.6417\n",
            "Epoch 26/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.2025 - acc: 0.9200\n",
            "Epoch 00026: val_acc did not improve from 0.74167\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.2112 - acc: 0.9125 - val_loss: 0.6000 - val_acc: 0.7000\n",
            "Epoch 27/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1922 - acc: 0.9150\n",
            "Epoch 00027: val_acc improved from 0.74167 to 0.77500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-27-0.77500-0.91875.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1842 - acc: 0.9187 - val_loss: 0.4751 - val_acc: 0.7750\n",
            "Epoch 28/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1904 - acc: 0.9200\n",
            "Epoch 00028: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1894 - acc: 0.9208 - val_loss: 0.5027 - val_acc: 0.7083\n",
            "Epoch 29/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1717 - acc: 0.9450\n",
            "Epoch 00029: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1634 - acc: 0.9479 - val_loss: 0.4585 - val_acc: 0.7750\n",
            "Epoch 30/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1776 - acc: 0.9300\n",
            "Epoch 00030: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1701 - acc: 0.9333 - val_loss: 0.5058 - val_acc: 0.7250\n",
            "Epoch 31/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1498 - acc: 0.9500\n",
            "Epoch 00031: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1507 - acc: 0.9438 - val_loss: 0.5982 - val_acc: 0.7083\n",
            "Epoch 32/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1609 - acc: 0.9450\n",
            "Epoch 00032: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1551 - acc: 0.9458 - val_loss: 0.5772 - val_acc: 0.7167\n",
            "Epoch 33/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1549 - acc: 0.9350\n",
            "Epoch 00033: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1669 - acc: 0.9271 - val_loss: 0.4581 - val_acc: 0.7667\n",
            "Epoch 34/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1757 - acc: 0.9200\n",
            "Epoch 00034: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1661 - acc: 0.9292 - val_loss: 0.4029 - val_acc: 0.7667\n",
            "Epoch 35/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1512 - acc: 0.9500\n",
            "Epoch 00035: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1438 - acc: 0.9479 - val_loss: 0.4652 - val_acc: 0.7333\n",
            "Epoch 36/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1386 - acc: 0.9425\n",
            "Epoch 00036: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1748 - acc: 0.9229 - val_loss: 0.4234 - val_acc: 0.7500\n",
            "Epoch 37/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1511 - acc: 0.9500\n",
            "Epoch 00037: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1399 - acc: 0.9563 - val_loss: 0.4765 - val_acc: 0.7583\n",
            "Epoch 38/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1492 - acc: 0.9375\n",
            "Epoch 00038: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1349 - acc: 0.9438 - val_loss: 0.4655 - val_acc: 0.7417\n",
            "Epoch 39/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1470 - acc: 0.9450\n",
            "Epoch 00039: val_acc did not improve from 0.77500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1480 - acc: 0.9417 - val_loss: 0.3579 - val_acc: 0.7750\n",
            "Epoch 40/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1171 - acc: 0.9625\n",
            "Epoch 00040: val_acc improved from 0.77500 to 0.85833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-40-0.85833-0.95625.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1182 - acc: 0.9563 - val_loss: 0.2879 - val_acc: 0.8583\n",
            "Epoch 41/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1271 - acc: 0.9450\n",
            "Epoch 00041: val_acc did not improve from 0.85833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1263 - acc: 0.9500 - val_loss: 0.3161 - val_acc: 0.8500\n",
            "Epoch 42/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1510 - acc: 0.9275\n",
            "Epoch 00042: val_acc improved from 0.85833 to 0.86667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-42-0.86667-0.93333.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1396 - acc: 0.9333 - val_loss: 0.3099 - val_acc: 0.8667\n",
            "Epoch 43/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1336 - acc: 0.9450\n",
            "Epoch 00043: val_acc did not improve from 0.86667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1377 - acc: 0.9417 - val_loss: 0.3246 - val_acc: 0.8583\n",
            "Epoch 44/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1252 - acc: 0.9525\n",
            "Epoch 00044: val_acc improved from 0.86667 to 0.87500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-44-0.87500-0.95417.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1254 - acc: 0.9542 - val_loss: 0.2516 - val_acc: 0.8750\n",
            "Epoch 45/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0966 - acc: 0.9675\n",
            "Epoch 00045: val_acc did not improve from 0.87500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1255 - acc: 0.9646 - val_loss: 0.3147 - val_acc: 0.8750\n",
            "Epoch 46/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1066 - acc: 0.9600\n",
            "Epoch 00046: val_acc did not improve from 0.87500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1273 - acc: 0.9563 - val_loss: 0.2976 - val_acc: 0.8500\n",
            "Epoch 47/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1203 - acc: 0.9475\n",
            "Epoch 00047: val_acc improved from 0.87500 to 0.88333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-47-0.88333-0.94375.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1201 - acc: 0.9438 - val_loss: 0.2362 - val_acc: 0.8833\n",
            "Epoch 48/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1133 - acc: 0.9575\n",
            "Epoch 00048: val_acc improved from 0.88333 to 0.90833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-48-0.90833-0.96250.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1049 - acc: 0.9625 - val_loss: 0.1980 - val_acc: 0.9083\n",
            "Epoch 49/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0718 - acc: 0.9900\n",
            "Epoch 00049: val_acc improved from 0.90833 to 0.92500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-49-0.92500-0.97500.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0917 - acc: 0.9750 - val_loss: 0.1846 - val_acc: 0.9250\n",
            "Epoch 50/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0972 - acc: 0.9525\n",
            "Epoch 00050: val_acc did not improve from 0.92500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0885 - acc: 0.9583 - val_loss: 0.2294 - val_acc: 0.8917\n",
            "Epoch 51/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1077 - acc: 0.9575\n",
            "Epoch 00051: val_acc improved from 0.92500 to 0.94167, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-51-0.94167-0.96250.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0973 - acc: 0.9625 - val_loss: 0.1322 - val_acc: 0.9417\n",
            "Epoch 52/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0846 - acc: 0.9750\n",
            "Epoch 00052: val_acc improved from 0.94167 to 0.95833, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-52-0.95833-0.97500.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0829 - acc: 0.9750 - val_loss: 0.1281 - val_acc: 0.9583\n",
            "Epoch 53/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1147 - acc: 0.9675\n",
            "Epoch 00053: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1098 - acc: 0.9667 - val_loss: 0.2298 - val_acc: 0.9083\n",
            "Epoch 54/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1139 - acc: 0.9600\n",
            "Epoch 00054: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1175 - acc: 0.9583 - val_loss: 0.1836 - val_acc: 0.9250\n",
            "Epoch 55/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0946 - acc: 0.9625\n",
            "Epoch 00055: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0976 - acc: 0.9604 - val_loss: 0.2381 - val_acc: 0.8917\n",
            "Epoch 56/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0959 - acc: 0.9700\n",
            "Epoch 00056: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0955 - acc: 0.9667 - val_loss: 0.1929 - val_acc: 0.8833\n",
            "Epoch 57/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0726 - acc: 0.9675\n",
            "Epoch 00057: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0899 - acc: 0.9583 - val_loss: 0.2278 - val_acc: 0.8917\n",
            "Epoch 58/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0855 - acc: 0.9675\n",
            "Epoch 00058: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0882 - acc: 0.9667 - val_loss: 0.2283 - val_acc: 0.9000\n",
            "Epoch 59/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.1191 - acc: 0.9575\n",
            "Epoch 00059: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.1099 - acc: 0.9604 - val_loss: 0.1647 - val_acc: 0.9250\n",
            "Epoch 60/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0800 - acc: 0.9775\n",
            "Epoch 00060: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0830 - acc: 0.9771 - val_loss: 0.1449 - val_acc: 0.9333\n",
            "Epoch 61/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0867 - acc: 0.9700\n",
            "Epoch 00061: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0821 - acc: 0.9708 - val_loss: 0.1210 - val_acc: 0.9500\n",
            "Epoch 62/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0928 - acc: 0.9650\n",
            "Epoch 00062: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0946 - acc: 0.9583 - val_loss: 0.1109 - val_acc: 0.9583\n",
            "Epoch 63/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0863 - acc: 0.9675\n",
            "Epoch 00063: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0816 - acc: 0.9688 - val_loss: 0.1469 - val_acc: 0.9250\n",
            "Epoch 64/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0791 - acc: 0.9700\n",
            "Epoch 00064: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0846 - acc: 0.9667 - val_loss: 0.1827 - val_acc: 0.9000\n",
            "Epoch 65/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0969 - acc: 0.9625\n",
            "Epoch 00065: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0991 - acc: 0.9604 - val_loss: 0.1180 - val_acc: 0.9583\n",
            "Epoch 66/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0763 - acc: 0.9700\n",
            "Epoch 00066: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0771 - acc: 0.9667 - val_loss: 0.1181 - val_acc: 0.9500\n",
            "Epoch 67/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0475 - acc: 0.9825\n",
            "Epoch 00067: val_acc did not improve from 0.95833\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0631 - acc: 0.9750 - val_loss: 0.1221 - val_acc: 0.9500\n",
            "Epoch 68/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0716 - acc: 0.9775\n",
            "Epoch 00068: val_acc improved from 0.95833 to 0.96667, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-68-0.96667-0.96875.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0920 - acc: 0.9688 - val_loss: 0.1003 - val_acc: 0.9667\n",
            "Epoch 69/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0557 - acc: 0.9850\n",
            "Epoch 00069: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0666 - acc: 0.9708 - val_loss: 0.1248 - val_acc: 0.9417\n",
            "Epoch 70/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0754 - acc: 0.9675\n",
            "Epoch 00070: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0718 - acc: 0.9729 - val_loss: 0.1325 - val_acc: 0.9417\n",
            "Epoch 71/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0608 - acc: 0.9800\n",
            "Epoch 00071: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0648 - acc: 0.9771 - val_loss: 0.1016 - val_acc: 0.9583\n",
            "Epoch 72/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0718 - acc: 0.9725\n",
            "Epoch 00072: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0707 - acc: 0.9729 - val_loss: 0.1346 - val_acc: 0.9333\n",
            "Epoch 73/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0545 - acc: 0.9825\n",
            "Epoch 00073: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0477 - acc: 0.9854 - val_loss: 0.1142 - val_acc: 0.9417\n",
            "Epoch 74/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0633 - acc: 0.9875\n",
            "Epoch 00074: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0669 - acc: 0.9854 - val_loss: 0.1346 - val_acc: 0.9167\n",
            "Epoch 75/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0432 - acc: 0.9825\n",
            "Epoch 00075: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0573 - acc: 0.9771 - val_loss: 0.1387 - val_acc: 0.9250\n",
            "Epoch 76/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0532 - acc: 0.9825\n",
            "Epoch 00076: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0515 - acc: 0.9792 - val_loss: 0.1908 - val_acc: 0.9000\n",
            "Epoch 77/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0612 - acc: 0.9800\n",
            "Epoch 00077: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0693 - acc: 0.9771 - val_loss: 0.1346 - val_acc: 0.9250\n",
            "Epoch 78/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0360 - acc: 0.9875\n",
            "Epoch 00078: val_acc did not improve from 0.96667\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0371 - acc: 0.9854 - val_loss: 0.1106 - val_acc: 0.9583\n",
            "Epoch 79/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0711 - acc: 0.9675\n",
            "Epoch 00079: val_acc improved from 0.96667 to 0.97500, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-79-0.97500-0.97292.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0625 - acc: 0.9729 - val_loss: 0.0755 - val_acc: 0.9750\n",
            "Epoch 80/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0615 - acc: 0.9750\n",
            "Epoch 00080: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0546 - acc: 0.9792 - val_loss: 0.0791 - val_acc: 0.9750\n",
            "Epoch 81/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0581 - acc: 0.9800\n",
            "Epoch 00081: val_acc did not improve from 0.97500\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0527 - acc: 0.9833 - val_loss: 0.0922 - val_acc: 0.9750\n",
            "Epoch 82/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0906 - acc: 0.9675\n",
            "Epoch 00082: val_acc improved from 0.97500 to 0.98333, saving model to drive/My Drive/Colab Notebooks/neural_network/nn_diploma/savings/02_weights-improvement-82-0.98333-0.96667.hdf5\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0941 - acc: 0.9667 - val_loss: 0.0716 - val_acc: 0.9833\n",
            "Epoch 83/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0483 - acc: 0.9800\n",
            "Epoch 00083: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0472 - acc: 0.9812 - val_loss: 0.1065 - val_acc: 0.9583\n",
            "Epoch 84/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0651 - acc: 0.9700\n",
            "Epoch 00084: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0643 - acc: 0.9688 - val_loss: 0.1070 - val_acc: 0.9417\n",
            "Epoch 85/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0409 - acc: 0.9850\n",
            "Epoch 00085: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0446 - acc: 0.9854 - val_loss: 0.0940 - val_acc: 0.9750\n",
            "Epoch 86/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0688 - acc: 0.9775\n",
            "Epoch 00086: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0679 - acc: 0.9750 - val_loss: 0.1140 - val_acc: 0.9583\n",
            "Epoch 87/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0539 - acc: 0.9700\n",
            "Epoch 00087: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0499 - acc: 0.9729 - val_loss: 0.1577 - val_acc: 0.9417\n",
            "Epoch 88/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0596 - acc: 0.9725\n",
            "Epoch 00088: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0664 - acc: 0.9729 - val_loss: 0.0856 - val_acc: 0.9667\n",
            "Epoch 89/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0730 - acc: 0.9775\n",
            "Epoch 00089: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0670 - acc: 0.9792 - val_loss: 0.0634 - val_acc: 0.9833\n",
            "Epoch 90/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0511 - acc: 0.9900\n",
            "Epoch 00090: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0472 - acc: 0.9917 - val_loss: 0.0724 - val_acc: 0.9833\n",
            "Epoch 91/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0502 - acc: 0.9800\n",
            "Epoch 00091: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0556 - acc: 0.9771 - val_loss: 0.0733 - val_acc: 0.9667\n",
            "Epoch 92/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0420 - acc: 0.9825\n",
            "Epoch 00092: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0576 - acc: 0.9771 - val_loss: 0.0825 - val_acc: 0.9833\n",
            "Epoch 93/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0408 - acc: 0.9825\n",
            "Epoch 00093: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0572 - acc: 0.9750 - val_loss: 0.0943 - val_acc: 0.9750\n",
            "Epoch 94/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0409 - acc: 0.9850\n",
            "Epoch 00094: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0354 - acc: 0.9875 - val_loss: 0.0567 - val_acc: 0.9833\n",
            "Epoch 95/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0443 - acc: 0.9875\n",
            "Epoch 00095: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0443 - acc: 0.9875 - val_loss: 0.0595 - val_acc: 0.9750\n",
            "Epoch 96/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0580 - acc: 0.9750\n",
            "Epoch 00096: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0637 - acc: 0.9708 - val_loss: 0.1355 - val_acc: 0.9417\n",
            "Epoch 97/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0470 - acc: 0.9800\n",
            "Epoch 00097: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0445 - acc: 0.9812 - val_loss: 0.1171 - val_acc: 0.9833\n",
            "Epoch 98/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0547 - acc: 0.9800\n",
            "Epoch 00098: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0565 - acc: 0.9792 - val_loss: 0.1138 - val_acc: 0.9833\n",
            "Epoch 99/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0678 - acc: 0.9750\n",
            "Epoch 00099: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0640 - acc: 0.9771 - val_loss: 0.1162 - val_acc: 0.9833\n",
            "Epoch 100/100\n",
            "400/480 [========================>.....] - ETA: 0s - loss: 0.0506 - acc: 0.9725\n",
            "Epoch 00100: val_acc did not improve from 0.98333\n",
            "480/480 [==============================] - 1s 2ms/sample - loss: 0.0531 - acc: 0.9708 - val_loss: 0.0951 - val_acc: 0.9833\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9708333611488342,validation accuracy: 0.9833333492279053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNSyLmjnxKTL",
        "colab_type": "code",
        "outputId": "ab4d0e4c-abbd-41cc-db36-66d7f29793a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions = pred(model, x_test_happy_scld)\n",
        "accuracy_score(y_test_happy, predictions)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9733333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b00e4b5c-716e-4117-db0c-6a46f05a07c0",
        "id": "dn3m25jpG6vn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print('no smile, predicted:', pred(model, imgn1_array_scld))\n",
        "print('no smile, predicted:', pred(model, imgn2_array_scld))\n",
        "print('no smile, predicted:', pred(model, imgn3_array_scld))\n",
        "print('no smile, predicted:', pred(model, imgn4_array_scld))\n",
        "print('no smile, predicted:', pred(model, imgn5_array_scld))\n",
        "print('smile, predicted:', pred(model, imgp1_array_scld))\n",
        "print('smile, predicted:', pred(model, imgp2_array_scld))\n",
        "print('smile, predicted:', pred(model, imgp3_array_scld))\n",
        "print('smile, predicted:', pred(model, imgp4_array_scld))\n",
        "print('smile, predicted:', pred(model, imgp5_array_scld))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no smile, predicted: [0]\n",
            "no smile, predicted: [1]\n",
            "no smile, predicted: [0]\n",
            "no smile, predicted: [1]\n",
            "no smile, predicted: [0]\n",
            "smile, predicted: [1]\n",
            "smile, predicted: [1]\n",
            "smile, predicted: [1]\n",
            "smile, predicted: [1]\n",
            "smile, predicted: [0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}